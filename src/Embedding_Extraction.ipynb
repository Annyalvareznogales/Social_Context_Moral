{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "448fe123-ddae-43a7-a021-f82a7dfccb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "from codecarbon import EmissionsTracker\n",
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a1eaa0-1c70-40d7-b415-86e6a8e17e2c",
   "metadata": {},
   "source": [
    "Modelos\n",
    "- \"xlm-roberta-base\" *\n",
    "- \"roberta-base\"\n",
    "- \"meta-llama/Llama-3.2-1B\"\n",
    "- \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "- Meta‑Llama‑3‑8B\n",
    "- Unbabel/TowerBase-7B-v0.1\n",
    "- NovaSearch/stella_en_1.5B_v5\n",
    "- datificate/gpt2-small-spanish *\n",
    "- bertin-project/bertin-roberta-base-spanish *\n",
    "- bertin-project/bertin-gpt-j-6B\n",
    "- \"DeepESP/gpt2-spanish-medium\"\n",
    "- datificate/gpt2-small-spanish\n",
    "- multilingual-e5-small\n",
    "\n",
    "\n",
    "\n",
    "Modelos en el caso de hacer FT\n",
    "- LLAMA 1-B 3-B \n",
    "- DEEPSEEK R1\n",
    "  \n",
    "- gpt2-small-spanish\n",
    "- bertin-roberta-base-spanish\n",
    "\n",
    "\n",
    "Modelos en caso de no hacer FT\n",
    "- Meta‑Llama‑3B‑8B \n",
    "- bigscience/bloomz-3b\n",
    "- nvidia/multilingual-domain-classifier\n",
    "  \n",
    "- bertin-project/bertin-gpt-j-6B\n",
    "- DeepESP/gpt2-spanish-medium"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a5de87-7b7c-4675-9216-a698c04b5d14",
   "metadata": {},
   "source": [
    "# FUNCIONES EXTRACCIÓN EMBEDDING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9591cc-6534-484f-84c4-7eb10483f241",
   "metadata": {},
   "source": [
    "## LLAMA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b224bceb-cd43-4d21-9291-932d10095349",
   "metadata": {},
   "source": [
    "https://arxiv.org/html/2503.05804v1?utm_source=chatgpt.com\n",
    "\n",
    "For Meta’s Llama 3.2 1B model, the training utilized 370,000 GPU hours on H100-80GB hardware, each with a peak power consumption of 700W. This translates to approximately 932,400,000 kWh of energy consumption. The estimated location-based greenhouse gas emissions for this training amounted to 107 tons of CO₂ equivalent. However, due to Meta's commitment to net-zero emissions and the use of renewable energy, the market-based emissions were reported as 0 tons CO₂ equivalent.\n",
    "\n",
    "Regarding the normalized training energy per second, a study estimated that Llama 3.2 1B consumes about 0.003 kWh per 100 requests, which equates to approximately 1.0 kWh per 33,333 requests. Given that each request takes about 12 seconds, this implies a normalized energy consumption of approximately 0.000083 kWh per second. This is a rough estimate and may vary based on specific deployment conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d3273d45-581c-44ad-93a0-9d2ef4762a05",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon WARNING @ 07:56:24] Multiple instances of codecarbon are allowed to run at the same time.\n",
      "[codecarbon INFO @ 07:56:24] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 07:56:24] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 07:56:25] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
      " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
      "\n",
      "[codecarbon INFO @ 07:56:25] CPU Model on constant consumption mode: Intel(R) Xeon(R) CPU E5-2630 v4 @ 2.20GHz\n",
      "[codecarbon WARNING @ 07:56:25] No CPU tracking mode found. Falling back on CPU load mode.\n",
      "[codecarbon INFO @ 07:56:25] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 07:56:25] Tracking Nvidia GPU via pynvml\n",
      "[codecarbon INFO @ 07:56:25] The below tracking methods have been set up:\n",
      "                RAM Tracking Method: RAM power estimation model\n",
      "                CPU Tracking Method: cpu_load\n",
      "                GPU Tracking Method: pynvml\n",
      "            \n",
      "[codecarbon INFO @ 07:56:25] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 07:56:25]   Platform system: Linux-5.4.0-74-generic-x86_64-with-glibc2.35\n",
      "[codecarbon INFO @ 07:56:25]   Python version: 3.11.10\n",
      "[codecarbon INFO @ 07:56:25]   CodeCarbon version: 3.0.2\n",
      "[codecarbon INFO @ 07:56:25]   Available RAM : 125.807 GB\n",
      "[codecarbon INFO @ 07:56:25]   CPU count: 40 thread(s) in 2 physical CPU(s)\n",
      "[codecarbon INFO @ 07:56:25]   CPU model: Intel(R) Xeon(R) CPU E5-2630 v4 @ 2.20GHz\n",
      "[codecarbon INFO @ 07:56:25]   GPU count: 1\n",
      "[codecarbon INFO @ 07:56:25]   GPU model: 1 x NVIDIA TITAN X (Pascal) BUT only tracking these GPU ids : [0]\n",
      "[codecarbon INFO @ 07:56:28] Emissions data (if any) will be saved to file /workspace/src/emissions/emissions.csv\n",
      "[codecarbon INFO @ 07:56:29] Energy consumed for RAM : 0.000006 kWh. RAM Power : 38.0 W\n",
      "[codecarbon INFO @ 07:56:29] Delta energy consumed for CPU with cpu_load : 0.000003 kWh, power : 17.0 W\n",
      "[codecarbon INFO @ 07:56:29] Energy consumed for All CPU : 0.000003 kWh\n",
      "[codecarbon INFO @ 07:56:29] Energy consumed for all GPUs : 0.000009 kWh. Total GPU Power : 30.589584781866115 W\n",
      "[codecarbon INFO @ 07:56:29] 0.000017 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 07:56:30] Energy consumed for RAM : 0.000011 kWh. RAM Power : 38.0 W\n",
      "[codecarbon INFO @ 07:56:30] Delta energy consumed for CPU with cpu_load : 0.000003 kWh, power : 17.0 W\n",
      "[codecarbon INFO @ 07:56:30] Energy consumed for All CPU : 0.000005 kWh\n",
      "[codecarbon INFO @ 07:56:30] Energy consumed for all GPUs : 0.000018 kWh. Total GPU Power : 30.755076874522704 W\n",
      "[codecarbon INFO @ 07:56:30] 0.000034 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 07:56:31] Energy consumed for RAM : 0.000017 kWh. RAM Power : 38.0 W\n",
      "[codecarbon INFO @ 07:56:31] Delta energy consumed for CPU with cpu_load : 0.000003 kWh, power : 17.0 W\n",
      "[codecarbon INFO @ 07:56:31] Energy consumed for All CPU : 0.000008 kWh\n",
      "[codecarbon INFO @ 07:56:31] Energy consumed for all GPUs : 0.000027 kWh. Total GPU Power : 30.518971243056786 W\n",
      "[codecarbon INFO @ 07:56:31] 0.000052 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 07:56:32] Energy consumed for RAM : 0.000023 kWh. RAM Power : 38.0 W\n",
      "[codecarbon INFO @ 07:56:32] Delta energy consumed for CPU with cpu_load : 0.000003 kWh, power : 17.0 W\n",
      "[codecarbon INFO @ 07:56:32] Energy consumed for All CPU : 0.000010 kWh\n",
      "[codecarbon INFO @ 07:56:32] Energy consumed for all GPUs : 0.000036 kWh. Total GPU Power : 30.50022971916034 W\n",
      "[codecarbon INFO @ 07:56:32] 0.000069 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 07:56:33] Energy consumed for RAM : 0.000029 kWh. RAM Power : 38.0 W\n",
      "[codecarbon INFO @ 07:56:34] Delta energy consumed for CPU with cpu_load : 0.000003 kWh, power : 17.0 W\n",
      "[codecarbon INFO @ 07:56:34] Energy consumed for All CPU : 0.000013 kWh\n",
      "[codecarbon INFO @ 07:56:34] Energy consumed for all GPUs : 0.000044 kWh. Total GPU Power : 30.680544747214512 W\n",
      "[codecarbon INFO @ 07:56:34] 0.000086 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 07:56:34] Energy consumed for RAM : 0.000029 kWh. RAM Power : 38.0 W\n",
      "[codecarbon INFO @ 07:56:34] Delta energy consumed for CPU with cpu_load : 0.000000 kWh, power : 17.0 W\n",
      "[codecarbon INFO @ 07:56:34] Energy consumed for All CPU : 0.000013 kWh\n",
      "[codecarbon INFO @ 07:56:34] Energy consumed for all GPUs : 0.000053 kWh. Total GPU Power : 57.39773266578425 W\n",
      "[codecarbon INFO @ 07:56:34] 0.000094 kWh of electricity used since the beginning.\n",
      "/opt/conda/lib/python3.11/site-packages/codecarbon/output_methods/file.py:90: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, new_df], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "# LLAMA \n",
    "\n",
    "#embeddings size (2048,)\n",
    "\n",
    "# datos\n",
    "texts_df = pd.read_pickle(\"../data/OMC/final_omc_morality.pkl\").head(5)\n",
    "\n",
    "# modelo\n",
    "model_id = \"meta-llama/Llama-3.2-1B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModel.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.cuda()  \n",
    "\n",
    "def infer_and_measure(text, tracker):\n",
    "    # Tokenización\n",
    "    inputs = tokenizer([text], return_tensors=\"pt\", padding=True, truncation=True).to(model.device)\n",
    "\n",
    "    # Medición de inferencia\n",
    "    tracker.start_task(\"llama-inference\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, output_hidden_states=True, return_dict=True)\n",
    "    emissions_data = tracker.stop_task()\n",
    "    embedding = outputs.hidden_states[-1][:, 0, :].cpu().numpy()[0]\n",
    "    return embedding, emissions_data.emissions\n",
    "\n",
    "\n",
    "def pooling_infer_and_measure(text, tracker):\n",
    "    # Tokenización\n",
    "    inputs = tokenizer([text], return_tensors=\"pt\", padding=True, truncation=True).to(model.device)\n",
    "    attention_mask = inputs[\"attention_mask\"]  # shape: (1, seq_len)\n",
    "\n",
    "    # Medición de inferencia\n",
    "    tracker.start_task(\"llama-inference\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, output_hidden_states=True, return_dict=True)\n",
    "    emissions_data = tracker.stop_task()\n",
    "    # Último hidden state: (1, seq_len, hidden_dim)\n",
    "    last_hidden = outputs.hidden_states[-1]\n",
    "    # Máscara expandida\n",
    "    mask = attention_mask.unsqueeze(-1).expand(last_hidden.size()).float()\n",
    "    # Mean pooling sobre tokens no-pad\n",
    "    summed = torch.sum(last_hidden * mask, dim=1)  )\n",
    "    counts = torch.clamp(mask.sum(dim=1), min=1e-9)\n",
    "    mean_pooled = (summed / counts).squeeze(0).cpu().numpy()  \n",
    "    return mean_pooled, emissions_data.emissions\n",
    "\n",
    "\n",
    "# tracker (CPU+GPU0)\n",
    "tracker = EmissionsTracker(\n",
    "    project_name=\"llama_inference\",\n",
    "    measure_power_secs=15,\n",
    "    save_to_file=True,\n",
    "    output_dir=\"emissions\",\n",
    "    gpu_ids=[0]\n",
    ")\n",
    "\n",
    "# contar por texto\n",
    "embeddings = []\n",
    "emissions = []\n",
    "for text in texts_df[\"text\"]:\n",
    "    emb, co2 = infer_and_measure(text, tracker)\n",
    "    embeddings.append(emb)\n",
    "    emissions.append(co2)\n",
    "\n",
    "tracker.stop()\n",
    "\n",
    "#guardar\n",
    "texts_df[\"llama_embedding\"] = embeddings\n",
    "texts_df[\"inference_co2_kg\"]   = emissions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d37fffe-9d2b-4a31-9dbb-cb77755f5ed9",
   "metadata": {},
   "source": [
    "## DeepSeek "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ca9046d7-f4f0-4e42-9949-b0bc3db6a36f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon WARNING @ 07:58:15] Multiple instances of codecarbon are allowed to run at the same time.\n",
      "[codecarbon INFO @ 07:58:16] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 07:58:16] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 07:58:17] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
      " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
      "\n",
      "[codecarbon INFO @ 07:58:17] CPU Model on constant consumption mode: Intel(R) Xeon(R) CPU E5-2630 v4 @ 2.20GHz\n",
      "[codecarbon WARNING @ 07:58:17] No CPU tracking mode found. Falling back on CPU load mode.\n",
      "[codecarbon INFO @ 07:58:17] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 07:58:17] Tracking Nvidia GPU via pynvml\n",
      "[codecarbon INFO @ 07:58:17] The below tracking methods have been set up:\n",
      "                RAM Tracking Method: RAM power estimation model\n",
      "                CPU Tracking Method: cpu_load\n",
      "                GPU Tracking Method: pynvml\n",
      "            \n",
      "[codecarbon INFO @ 07:58:17] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 07:58:17]   Platform system: Linux-5.4.0-74-generic-x86_64-with-glibc2.35\n",
      "[codecarbon INFO @ 07:58:17]   Python version: 3.11.10\n",
      "[codecarbon INFO @ 07:58:17]   CodeCarbon version: 3.0.2\n",
      "[codecarbon INFO @ 07:58:17]   Available RAM : 125.807 GB\n",
      "[codecarbon INFO @ 07:58:17]   CPU count: 40 thread(s) in 2 physical CPU(s)\n",
      "[codecarbon INFO @ 07:58:17]   CPU model: Intel(R) Xeon(R) CPU E5-2630 v4 @ 2.20GHz\n",
      "[codecarbon INFO @ 07:58:17]   GPU count: 1\n",
      "[codecarbon INFO @ 07:58:17]   GPU model: 1 x NVIDIA TITAN X (Pascal) BUT only tracking these GPU ids : [0]\n",
      "[codecarbon INFO @ 07:58:20] Emissions data (if any) will be saved to file /workspace/src/emissions/emissions.csv\n",
      "[codecarbon INFO @ 07:58:20] Energy consumed for RAM : 0.000006 kWh. RAM Power : 38.0 W\n",
      "[codecarbon INFO @ 07:58:21] Delta energy consumed for CPU with cpu_load : 0.000003 kWh, power : 17.0 W\n",
      "[codecarbon INFO @ 07:58:21] Energy consumed for All CPU : 0.000003 kWh\n",
      "[codecarbon INFO @ 07:58:21] Energy consumed for all GPUs : 0.000009 kWh. Total GPU Power : 31.879898003974734 W\n",
      "[codecarbon INFO @ 07:58:21] 0.000018 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 07:58:22] Energy consumed for RAM : 0.000012 kWh. RAM Power : 38.0 W\n",
      "[codecarbon INFO @ 07:58:22] Delta energy consumed for CPU with cpu_load : 0.000003 kWh, power : 17.0 W\n",
      "[codecarbon INFO @ 07:58:22] Energy consumed for All CPU : 0.000005 kWh\n",
      "[codecarbon INFO @ 07:58:22] Energy consumed for all GPUs : 0.000019 kWh. Total GPU Power : 32.39731354639911 W\n",
      "[codecarbon INFO @ 07:58:22] 0.000036 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 07:58:23] Energy consumed for RAM : 0.000018 kWh. RAM Power : 38.0 W\n",
      "[codecarbon INFO @ 07:58:23] Delta energy consumed for CPU with cpu_load : 0.000003 kWh, power : 17.0 W\n",
      "[codecarbon INFO @ 07:58:23] Energy consumed for All CPU : 0.000008 kWh\n",
      "[codecarbon INFO @ 07:58:23] Energy consumed for all GPUs : 0.000028 kWh. Total GPU Power : 31.829239754502407 W\n",
      "[codecarbon INFO @ 07:58:23] 0.000054 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 07:58:24] Energy consumed for RAM : 0.000024 kWh. RAM Power : 38.0 W\n",
      "[codecarbon INFO @ 07:58:24] Delta energy consumed for CPU with cpu_load : 0.000003 kWh, power : 17.0 W\n",
      "[codecarbon INFO @ 07:58:24] Energy consumed for All CPU : 0.000011 kWh\n",
      "[codecarbon INFO @ 07:58:24] Energy consumed for all GPUs : 0.000038 kWh. Total GPU Power : 31.833300128253065 W\n",
      "[codecarbon INFO @ 07:58:24] 0.000072 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 07:58:25] Energy consumed for RAM : 0.000030 kWh. RAM Power : 38.0 W\n",
      "[codecarbon INFO @ 07:58:25] Delta energy consumed for CPU with cpu_load : 0.000003 kWh, power : 17.0 W\n",
      "[codecarbon INFO @ 07:58:25] Energy consumed for All CPU : 0.000013 kWh\n",
      "[codecarbon INFO @ 07:58:25] Energy consumed for all GPUs : 0.000047 kWh. Total GPU Power : 32.076258369804194 W\n",
      "[codecarbon INFO @ 07:58:25] 0.000090 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 07:58:25] Energy consumed for RAM : 0.000030 kWh. RAM Power : 38.0 W\n",
      "[codecarbon INFO @ 07:58:26] Delta energy consumed for CPU with cpu_load : 0.000000 kWh, power : 17.0 W\n",
      "[codecarbon INFO @ 07:58:26] Energy consumed for All CPU : 0.000013 kWh\n",
      "[codecarbon INFO @ 07:58:26] Energy consumed for all GPUs : 0.000055 kWh. Total GPU Power : 57.51897910250158 W\n",
      "[codecarbon INFO @ 07:58:26] 0.000098 kWh of electricity used since the beginning.\n",
      "/opt/conda/lib/python3.11/site-packages/codecarbon/output_methods/file.py:90: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, new_df], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "#DEEPSEEK\n",
    "\n",
    "#embeddings size (1536,)\n",
    "\n",
    "\n",
    "# datos\n",
    "texts_df = pd.read_pickle(\"../data/OMC/final_omc_morality.pkl\").head(5)\n",
    "texts_df= texts_df.head(5)\n",
    "# modelo\n",
    "model_id = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"  \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModel.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.cuda()  \n",
    "\n",
    "\n",
    "def infer_and_measure(text: str, tracker: EmissionsTracker):\n",
    "    # Tokenización\n",
    "    inputs = tokenizer([text],\n",
    "                       return_tensors=\"pt\",\n",
    "                       padding=True,\n",
    "                       truncation=True).to(model.device)\n",
    "\n",
    "    tracker.start_task(\"deepseek-inference\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, output_hidden_states=True, return_dict=True)\n",
    "    data = tracker.stop_task()\n",
    "\n",
    "    hidden = outputs.hidden_states[-1]\n",
    "    # Encuentra índice del último token (no-pad) para cada batch\n",
    "    seq_lens = (inputs[\"attention_mask\"].sum(dim=1) - 1).unsqueeze(-1)\n",
    "    embedding = hidden.gather(1, seq_lens.unsqueeze(-1).expand(-1, -1, hidden.size(-1)))\n",
    "    embedding = embedding.squeeze(1).cpu().numpy()\n",
    "\n",
    "    return embedding[0], data.emissions\n",
    "\n",
    "def pooling_infer_and_measure(text: str, tracker: EmissionsTracker):\n",
    "    # Tokenización\n",
    "    inputs = tokenizer([text],\n",
    "                       return_tensors=\"pt\",\n",
    "                       padding=True,\n",
    "                       truncation=True).to(model.device)\n",
    "    attention_mask = inputs[\"attention_mask\"]\n",
    "\n",
    "    # Medición\n",
    "    tracker.start_task(\"deepseek-inference\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, output_hidden_states=True, return_dict=True)\n",
    "    data = tracker.stop_task()\n",
    "\n",
    "    hidden = outputs.hidden_states[-1]  # (1, seq_len, 1536)\n",
    "\n",
    "    # Mean pooling sobre non-pad tokens\n",
    "    mask = attention_mask.unsqueeze(-1).expand(hidden.size()).float() \n",
    "    summed = torch.sum(hidden * mask, dim=1)                          \n",
    "    counts = torch.clamp(mask.sum(dim=1), min=1e-9)                   \n",
    "    embedding = (summed / counts).squeeze(0).cpu().numpy()          \n",
    "    return embedding, data.emissions\n",
    "\n",
    "# tracker (CPU+GPU0)\n",
    "tracker = EmissionsTracker(\n",
    "    project_name=\"deepseek_inference\",\n",
    "    measure_power_secs=15,\n",
    "    save_to_file=True,\n",
    "    output_dir=\"emissions\",\n",
    "    gpu_ids=[0]\n",
    ")\n",
    "\n",
    "# contar por texto\n",
    "embeddings = []\n",
    "emissions = []\n",
    "for text in texts_df[\"text\"]:\n",
    "    emb, co2 = infer_and_measure(text, tracker)\n",
    "    embeddings.append(emb)\n",
    "    emissions.append(co2)\n",
    "\n",
    "tracker.stop()\n",
    "\n",
    "#guardar\n",
    "texts_df[\"deepseek_embedding\"] = embeddings\n",
    "texts_df[\"inference_co2_kg\"]   = emissions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a71c769-ffd3-4ed6-a63c-dc248d7ed07d",
   "metadata": {},
   "source": [
    "## gpt2-small-spanish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0895a5c1-0174-4bb5-868e-d20baf7d3e4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon WARNING @ 08:27:12] Multiple instances of codecarbon are allowed to run at the same time.\n",
      "[codecarbon INFO @ 08:27:12] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 08:27:12] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 08:27:13] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
      " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
      "\n",
      "[codecarbon INFO @ 08:27:13] CPU Model on constant consumption mode: Intel(R) Xeon(R) CPU E5-2630 v4 @ 2.20GHz\n",
      "[codecarbon WARNING @ 08:27:13] No CPU tracking mode found. Falling back on CPU load mode.\n",
      "[codecarbon INFO @ 08:27:13] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 08:27:13] Tracking Nvidia GPU via pynvml\n",
      "[codecarbon INFO @ 08:27:13] The below tracking methods have been set up:\n",
      "                RAM Tracking Method: RAM power estimation model\n",
      "                CPU Tracking Method: cpu_load\n",
      "                GPU Tracking Method: pynvml\n",
      "            \n",
      "[codecarbon INFO @ 08:27:13] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 08:27:13]   Platform system: Linux-5.4.0-74-generic-x86_64-with-glibc2.35\n",
      "[codecarbon INFO @ 08:27:13]   Python version: 3.11.10\n",
      "[codecarbon INFO @ 08:27:13]   CodeCarbon version: 3.0.2\n",
      "[codecarbon INFO @ 08:27:13]   Available RAM : 125.807 GB\n",
      "[codecarbon INFO @ 08:27:13]   CPU count: 40 thread(s) in 2 physical CPU(s)\n",
      "[codecarbon INFO @ 08:27:13]   CPU model: Intel(R) Xeon(R) CPU E5-2630 v4 @ 2.20GHz\n",
      "[codecarbon INFO @ 08:27:13]   GPU count: 1\n",
      "[codecarbon INFO @ 08:27:13]   GPU model: 1 x NVIDIA TITAN X (Pascal) BUT only tracking these GPU ids : [0]\n",
      "[codecarbon INFO @ 08:27:17] Emissions data (if any) will be saved to file /workspace/src/emissions/emissions.csv\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "[codecarbon INFO @ 08:27:17] Energy consumed for RAM : 0.000006 kWh. RAM Power : 38.0 W\n",
      "[codecarbon INFO @ 08:27:18] Delta energy consumed for CPU with cpu_load : 0.000003 kWh, power : 17.0 W\n",
      "[codecarbon INFO @ 08:27:18] Energy consumed for All CPU : 0.000003 kWh\n",
      "[codecarbon INFO @ 08:27:18] Energy consumed for all GPUs : 0.000009 kWh. Total GPU Power : 29.82995480064139 W\n",
      "[codecarbon INFO @ 08:27:18] 0.000017 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 08:27:18] Energy consumed for RAM : 0.000011 kWh. RAM Power : 38.0 W\n",
      "[codecarbon INFO @ 08:27:19] Delta energy consumed for CPU with cpu_load : 0.000002 kWh, power : 17.0 W\n",
      "[codecarbon INFO @ 08:27:19] Energy consumed for All CPU : 0.000005 kWh\n",
      "[codecarbon INFO @ 08:27:19] Energy consumed for all GPUs : 0.000017 kWh. Total GPU Power : 29.369432427154273 W\n",
      "[codecarbon INFO @ 08:27:19] 0.000034 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 08:27:19] Energy consumed for RAM : 0.000017 kWh. RAM Power : 38.0 W\n",
      "[codecarbon INFO @ 08:27:20] Delta energy consumed for CPU with cpu_load : 0.000002 kWh, power : 17.0 W\n",
      "[codecarbon INFO @ 08:27:20] Energy consumed for All CPU : 0.000008 kWh\n",
      "[codecarbon INFO @ 08:27:20] Energy consumed for all GPUs : 0.000025 kWh. Total GPU Power : 29.20103404393443 W\n",
      "[codecarbon INFO @ 08:27:20] 0.000050 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 08:27:20] Energy consumed for RAM : 0.000022 kWh. RAM Power : 38.0 W\n",
      "[codecarbon INFO @ 08:27:21] Delta energy consumed for CPU with cpu_load : 0.000002 kWh, power : 17.0 W\n",
      "[codecarbon INFO @ 08:27:21] Energy consumed for All CPU : 0.000010 kWh\n",
      "[codecarbon INFO @ 08:27:21] Energy consumed for all GPUs : 0.000034 kWh. Total GPU Power : 29.024849661392427 W\n",
      "[codecarbon INFO @ 08:27:21] 0.000066 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 08:27:21] Energy consumed for RAM : 0.000028 kWh. RAM Power : 38.0 W\n",
      "[codecarbon INFO @ 08:27:22] Delta energy consumed for CPU with cpu_load : 0.000002 kWh, power : 17.0 W\n",
      "[codecarbon INFO @ 08:27:22] Energy consumed for All CPU : 0.000013 kWh\n",
      "[codecarbon INFO @ 08:27:22] Energy consumed for all GPUs : 0.000042 kWh. Total GPU Power : 29.04659055266263 W\n",
      "[codecarbon INFO @ 08:27:22] 0.000083 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 08:27:22] Energy consumed for RAM : 0.000028 kWh. RAM Power : 38.0 W\n",
      "[codecarbon INFO @ 08:27:22] Delta energy consumed for CPU with cpu_load : 0.000000 kWh, power : 17.0 W\n",
      "[codecarbon INFO @ 08:27:22] Energy consumed for All CPU : 0.000013 kWh\n",
      "[codecarbon INFO @ 08:27:22] Energy consumed for all GPUs : 0.000050 kWh. Total GPU Power : 57.410416660722404 W\n",
      "[codecarbon INFO @ 08:27:22] 0.000091 kWh of electricity used since the beginning.\n",
      "/opt/conda/lib/python3.11/site-packages/codecarbon/output_methods/file.py:90: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, new_df], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "#gpt2-small-spanish\n",
    "\n",
    "#embeddings size (768,)\n",
    "\n",
    "\n",
    "# datos\n",
    "texts_df = pd.read_pickle(\"../data/OMC/final_omc_morality.pkl\").head(5)\n",
    "texts_df= texts_df.head(5)\n",
    "# modelo\n",
    "model_id = \"datificate/gpt2-small-spanish\"  \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModel.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.cuda()  \n",
    "\n",
    "\n",
    "def infer_and_measure(text: str, tracker: EmissionsTracker):\n",
    "    # Tokenización\n",
    "    inputs = tokenizer([text],\n",
    "                       return_tensors=\"pt\",\n",
    "                       padding=True,\n",
    "                       truncation=True).to(model.device)\n",
    "\n",
    "    tracker.start_task(\"gpt2-inference\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, output_hidden_states=True, return_dict=True)\n",
    "    data = tracker.stop_task()\n",
    "\n",
    "    hidden = outputs.hidden_states[-1]\n",
    "    # Encuentra índice del último token (no-pad) para cada batch\n",
    "    seq_lens = (inputs[\"attention_mask\"].sum(dim=1) - 1).unsqueeze(-1)\n",
    "    embedding = hidden.gather(1, seq_lens.unsqueeze(-1).expand(-1, -1, hidden.size(-1)))\n",
    "    embedding = embedding.squeeze(1).cpu().numpy()\n",
    "\n",
    "    return embedding[0], data.emissions\n",
    "\n",
    "\n",
    "def pooling_infer_and_measure(text: str, tracker: EmissionsTracker):\n",
    "    # Tokenización\n",
    "    inputs = tokenizer([text],\n",
    "                       return_tensors=\"pt\",\n",
    "                       padding=True,\n",
    "                       truncation=True).to(model.device)\n",
    "    attention_mask = inputs[\"attention_mask\"]\n",
    "\n",
    "    tracker.start_task(\"gpt2-inference\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, output_hidden_states=True, return_dict=True)\n",
    "    data = tracker.stop_task()\n",
    "\n",
    "    hidden = outputs.hidden_states[-1]  \n",
    "    # Mean pooling sobre non-pad tokens\n",
    "    mask = attention_mask.unsqueeze(-1).expand(hidden.size()).float()  \n",
    "    summed = torch.sum(hidden * mask, dim=1)                           \n",
    "    counts = torch.clamp(mask.sum(dim=1), min=1e-9)                   \n",
    "    embedding = (summed / counts).squeeze(0).cpu().numpy()            \n",
    "    return embedding, data.emissions\n",
    "\n",
    "\n",
    "\n",
    "# tracker (CPU+GPU0)\n",
    "tracker = EmissionsTracker(\n",
    "    project_name=\"gpt2_inference\",\n",
    "    measure_power_secs=15,\n",
    "    save_to_file=True,\n",
    "    output_dir=\"emissions\",\n",
    "    gpu_ids=[0]\n",
    ")\n",
    "\n",
    "# contar por texto\n",
    "embeddings = []\n",
    "emissions = []\n",
    "for text in texts_df[\"text\"]:\n",
    "    emb, co2 = infer_and_measure(text, tracker)\n",
    "    embeddings.append(emb)\n",
    "    emissions.append(co2)\n",
    "\n",
    "tracker.stop()\n",
    "\n",
    "#guardar\n",
    "texts_df[\"gpt2_spanish_embedding\"] = embeddings\n",
    "texts_df[\"inference_co2_kg\"]   = emissions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9dc96f6a-f61f-418b-ac50-37daff3da9ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768,)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts_df[\"gpt2_spanish_embedding\"][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf097a9-053f-4b57-b593-284d3b070908",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a61d0452-09a8-475e-9b40-baad731dde39",
   "metadata": {},
   "source": [
    "## bertin-roberta-base-spanish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d166059a-71ab-438a-9848-55c29fa0ad6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at bertin-project/bertin-roberta-base-spanish and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "[codecarbon WARNING @ 08:27:25] Multiple instances of codecarbon are allowed to run at the same time.\n",
      "[codecarbon INFO @ 08:27:25] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 08:27:25] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 08:27:26] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
      " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
      "\n",
      "[codecarbon INFO @ 08:27:26] CPU Model on constant consumption mode: Intel(R) Xeon(R) CPU E5-2630 v4 @ 2.20GHz\n",
      "[codecarbon WARNING @ 08:27:26] No CPU tracking mode found. Falling back on CPU load mode.\n",
      "[codecarbon INFO @ 08:27:26] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 08:27:26] Tracking Nvidia GPU via pynvml\n",
      "[codecarbon INFO @ 08:27:26] The below tracking methods have been set up:\n",
      "                RAM Tracking Method: RAM power estimation model\n",
      "                CPU Tracking Method: cpu_load\n",
      "                GPU Tracking Method: pynvml\n",
      "            \n",
      "[codecarbon INFO @ 08:27:26] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 08:27:26]   Platform system: Linux-5.4.0-74-generic-x86_64-with-glibc2.35\n",
      "[codecarbon INFO @ 08:27:26]   Python version: 3.11.10\n",
      "[codecarbon INFO @ 08:27:26]   CodeCarbon version: 3.0.2\n",
      "[codecarbon INFO @ 08:27:26]   Available RAM : 125.807 GB\n",
      "[codecarbon INFO @ 08:27:26]   CPU count: 40 thread(s) in 2 physical CPU(s)\n",
      "[codecarbon INFO @ 08:27:26]   CPU model: Intel(R) Xeon(R) CPU E5-2630 v4 @ 2.20GHz\n",
      "[codecarbon INFO @ 08:27:26]   GPU count: 1\n",
      "[codecarbon INFO @ 08:27:26]   GPU model: 1 x NVIDIA TITAN X (Pascal) BUT only tracking these GPU ids : [0]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 61\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m embedding, data\u001b[38;5;241m.\u001b[39memissions\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# tracker (CPU+GPU0)\u001b[39;00m\n\u001b[0;32m---> 61\u001b[0m tracker \u001b[38;5;241m=\u001b[39m \u001b[43mEmissionsTracker\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproject_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbertin_inference\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmeasure_power_secs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_to_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43memissions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgpu_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# contar por texto\u001b[39;00m\n\u001b[1;32m     71\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/codecarbon/emissions_tracker.py:357\u001b[0m, in \u001b[0;36mBaseEmissionsTracker.__init__\u001b[0;34m(self, project_name, measure_power_secs, api_call_interval, api_endpoint, api_key, output_dir, output_file, save_to_file, save_to_api, save_to_logger, logging_logger, save_to_prometheus, save_to_logfire, prometheus_url, output_handlers, gpu_ids, emissions_endpoint, experiment_id, experiment_name, co2_signal_api_token, tracking_mode, log_level, on_csv_write, logger_preamble, force_cpu_power, force_ram_power, pue, force_mode_cpu_load, allow_multiple_runs)\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_scheduler_monitor_power \u001b[38;5;241m=\u001b[39m PeriodicScheduler(\n\u001b[1;32m    351\u001b[0m     function\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_monitor_power,\n\u001b[1;32m    352\u001b[0m     interval\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    353\u001b[0m )\n\u001b[1;32m    355\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_source \u001b[38;5;241m=\u001b[39m DataSource()\n\u001b[0;32m--> 357\u001b[0m cloud: CloudMetadata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_cloud_metadata\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cloud\u001b[38;5;241m.\u001b[39mis_on_private_infra:\n\u001b[1;32m    360\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_geo \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_geo_metadata()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/codecarbon/emissions_tracker.py:957\u001b[0m, in \u001b[0;36mEmissionsTracker._get_cloud_metadata\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    955\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_cloud_metadata\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m CloudMetadata:\n\u001b[1;32m    956\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cloud \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 957\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cloud \u001b[38;5;241m=\u001b[39m \u001b[43mCloudMetadata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_utils\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    958\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cloud\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/codecarbon/external/geography.py:40\u001b[0m, in \u001b[0;36mCloudMetadata.from_utils\u001b[0;34m(cls)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m re\u001b[38;5;241m.\u001b[39msearch(google_region_regex, zone)\u001b[38;5;241m.\u001b[39mgroup(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     34\u001b[0m extract_region_for_provider: Dict[\u001b[38;5;28mstr\u001b[39m, Callable] \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maws\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregion\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mazure\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompute\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocation\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgcp\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m x: extract_gcp_region(x[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzone\u001b[39m\u001b[38;5;124m\"\u001b[39m)),\n\u001b[1;32m     38\u001b[0m }\n\u001b[0;32m---> 40\u001b[0m cloud_metadata: Dict \u001b[38;5;241m=\u001b[39m \u001b[43mget_env_cloud_details\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cloud_metadata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m cloud_metadata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m {}:\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(provider\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, region\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/codecarbon/core/cloud.py:57\u001b[0m, in \u001b[0;36mget_env_cloud_details\u001b[0;34m(timeout)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     56\u001b[0m     params \u001b[38;5;241m=\u001b[39m CLOUD_METADATA_MAPPING[provider]\n\u001b[0;32m---> 57\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43murl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mheaders\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m     response\u001b[38;5;241m.\u001b[39mraise_for_status()\n\u001b[1;32m     61\u001b[0m     response_data \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mjson()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/requests/api.py:73\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mget\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/requests/adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    664\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    682\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/urllib3/connectionpool.py:789\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    786\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 789\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[1;32m    805\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/urllib3/connectionpool.py:495\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    492\u001b[0m \u001b[38;5;66;03m# conn.request() calls http.client.*.request, not the method in\u001b[39;00m\n\u001b[1;32m    493\u001b[0m \u001b[38;5;66;03m# urllib3.request. It also calls makefile (recv) on the socket.\u001b[39;00m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 495\u001b[0m     \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43menforce_content_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menforce_content_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    506\u001b[0m \u001b[38;5;66;03m# We are swallowing BrokenPipeError (errno.EPIPE) since the server is\u001b[39;00m\n\u001b[1;32m    507\u001b[0m \u001b[38;5;66;03m# legitimately able to close the connection after sending a valid response.\u001b[39;00m\n\u001b[1;32m    508\u001b[0m \u001b[38;5;66;03m# With this behaviour, the received response is still readable.\u001b[39;00m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBrokenPipeError\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/urllib3/connection.py:441\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[0;34m(self, method, url, body, headers, chunked, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m header, value \u001b[38;5;129;01min\u001b[39;00m headers\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    440\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mputheader(header, value)\n\u001b[0;32m--> 441\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendheaders\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;66;03m# If we're given a body we start sending that in chunks.\u001b[39;00m\n\u001b[1;32m    444\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunks \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/http/client.py:1298\u001b[0m, in \u001b[0;36mHTTPConnection.endheaders\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1296\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1297\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CannotSendHeader()\n\u001b[0;32m-> 1298\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/http/client.py:1058\u001b[0m, in \u001b[0;36mHTTPConnection._send_output\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1056\u001b[0m msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer)\n\u001b[1;32m   1057\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer[:]\n\u001b[0;32m-> 1058\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1060\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m message_body \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1061\u001b[0m \n\u001b[1;32m   1062\u001b[0m     \u001b[38;5;66;03m# create a consistent interface to message_body\u001b[39;00m\n\u001b[1;32m   1063\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(message_body, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m   1064\u001b[0m         \u001b[38;5;66;03m# Let file-like take precedence over byte-like.  This\u001b[39;00m\n\u001b[1;32m   1065\u001b[0m         \u001b[38;5;66;03m# is needed to allow the current position of mmap'ed\u001b[39;00m\n\u001b[1;32m   1066\u001b[0m         \u001b[38;5;66;03m# files to be taken into account.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/http/client.py:996\u001b[0m, in \u001b[0;36mHTTPConnection.send\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    994\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    995\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_open:\n\u001b[0;32m--> 996\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    997\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    998\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m NotConnected()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/urllib3/connection.py:279\u001b[0m, in \u001b[0;36mHTTPConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 279\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_new_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tunnel_host:\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;66;03m# If we're tunneling it means we're connected to our proxy.\u001b[39;00m\n\u001b[1;32m    282\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_connected_to_proxy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/urllib3/connection.py:199\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Establish a socket connection and set nodelay settings on it.\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \n\u001b[1;32m    196\u001b[0m \u001b[38;5;124;03m:return: New socket connection.\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 199\u001b[0m     sock \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dns_host\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43msource_address\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msource_address\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m        \u001b[49m\u001b[43msocket_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mgaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NameResolutionError(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost, \u001b[38;5;28mself\u001b[39m, e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/urllib3/util/connection.py:73\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m source_address:\n\u001b[1;32m     72\u001b[0m     sock\u001b[38;5;241m.\u001b[39mbind(source_address)\n\u001b[0;32m---> 73\u001b[0m \u001b[43msock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43msa\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n\u001b[1;32m     75\u001b[0m err \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#bertin-roberta-base-spanish\n",
    "\n",
    "#embeddings size (768,)\n",
    "\n",
    "\n",
    "# datos\n",
    "texts_df = pd.read_pickle(\"../data/OMC/final_omc_morality.pkl\").head(5)\n",
    "texts_df= texts_df.head(5)\n",
    "# modelo\n",
    "model_id = \"bertin-project/bertin-roberta-base-spanish\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModel.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.cuda()  \n",
    "\n",
    "\n",
    "def infer_and_measure(text: str, tracker: EmissionsTracker):\n",
    "    # Tokenización\n",
    "    inputs = tokenizer([text],\n",
    "                       return_tensors=\"pt\",\n",
    "                       padding=True,\n",
    "                       truncation=True).to(model.device)\n",
    "\n",
    "    tracker.start_task(\"bertin-inference\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, output_hidden_states=True, return_dict=True)\n",
    "    data = tracker.stop_task()\n",
    "\n",
    "    hidden = outputs.hidden_states[-1]\n",
    "    # Encuentra índice del último token (no-pad) para cada batch\n",
    "    seq_lens = (inputs[\"attention_mask\"].sum(dim=1) - 1).unsqueeze(-1)\n",
    "    embedding = hidden.gather(1, seq_lens.unsqueeze(-1).expand(-1, -1, hidden.size(-1)))\n",
    "    embedding = embedding.squeeze(1).cpu().numpy()\n",
    "\n",
    "    return embedding[0], data.emissions\n",
    "\n",
    "def pooling_infer_and_measure(text: str, tracker: EmissionsTracker):\n",
    "    # Tokenización\n",
    "    inputs = tokenizer([text],\n",
    "                       return_tensors=\"pt\",\n",
    "                       padding=True,\n",
    "                       truncation=True).to(model.device)\n",
    "    attention_mask = inputs[\"attention_mask\"]\n",
    "\n",
    "    tracker.start_task(\"bertin-inference\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, output_hidden_states=True, return_dict=True)\n",
    "    data = tracker.stop_task()\n",
    "\n",
    "    hidden = outputs.hidden_states[-1] \n",
    "\n",
    "    # Mean pooling sobre non-pad tokens\n",
    "    mask = attention_mask.unsqueeze(-1).expand(hidden.size()).float()\n",
    "    summed = torch.sum(hidden * mask, dim=1)                           \n",
    "    counts = torch.clamp(mask.sum(dim=1), min=1e-9)                   \n",
    "    embedding = (summed / counts).squeeze(0).cpu().numpy()            \n",
    "    return embedding, data.emissions\n",
    "\n",
    "\n",
    "# tracker (CPU+GPU0)\n",
    "tracker = EmissionsTracker(\n",
    "    project_name=\"bertin_inference\",\n",
    "    measure_power_secs=15,\n",
    "    save_to_file=True,\n",
    "    output_dir=\"emissions\",\n",
    "    gpu_ids=[0]\n",
    ")\n",
    "\n",
    "\n",
    "# contar por texto\n",
    "embeddings = []\n",
    "emissions = []\n",
    "for text in texts_df[\"text\"]:\n",
    "    emb, co2 = infer_and_measure(text, tracker)\n",
    "    embeddings.append(emb)\n",
    "    emissions.append(co2)\n",
    "\n",
    "tracker.stop()\n",
    "\n",
    "#guardar\n",
    "texts_df[\"bertin_spanish_embedding\"] = embeddings\n",
    "texts_df[\"inference_co2_kg\"]   = emissions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f7adbf3c-f020-451a-9637-328d6beea4db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [0.052701034, 0.0009660059, -0.066261984, -0.0...\n",
       "1    [0.06101517, -0.004614892, -0.05676373, -0.064...\n",
       "2    [0.027430454, -0.013715127, -0.06572105, -0.04...\n",
       "3    [0.020933025, 0.0073674778, -0.070677064, -0.0...\n",
       "4    [0.054630745, 0.0018714874, -0.08359161, -0.05...\n",
       "Name: bertin_spanish_embedding, dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts_df[\"bertin_spanish_embedding\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bec4682-a76a-4ae3-a37e-a43ff1174a34",
   "metadata": {},
   "source": [
    "# Crear Tabla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd0202f-8369-4075-a3d4-efa408ed4c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "csv_path = \"emissions/emissions_base_3a641cd1-7180-4653-af7f-31e84e4db47a.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# métricas de inferencia normalizada\n",
    "#    - duration (s) ya es \"Normalized prediction time (s)\" promedio\n",
    "norm_pred_time = df[\"duration\"].mean()\n",
    "\n",
    "# energía normalizada en kW: potencia media CPU+GPU (W) -> kW\n",
    "norm_pred_energy = ((df[\"cpu_power\"] + df[\"gpu_power\"]) / 1000).mean()\n",
    "\n",
    "\n",
    "summary = pd.DataFrame({\n",
    "    \"Model\": [\"llama\"],\n",
    "    \"Normalized training time (s)\": [None],          # buscar\n",
    "    \"Normalized prediction time (s)\": [norm_pred_time],\n",
    "    \"Normalized training energy (kW)\": [None],       # buscar\n",
    "    \"Normalized prediction energy (kW)\": [norm_pred_energy]\n",
    "})\n",
    "\n",
    "# notación científica\n",
    "summary[\"Normalized prediction time (s)\"] = summary[\"Normalized prediction time (s)\"].apply(lambda x: f\"{x:.2E}\")\n",
    "summary[\"Normalized prediction energy (kW)\"] = summary[\"Normalized prediction energy (kW)\"].apply(lambda x: f\"{x:.2E}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e5533d-b6bd-49c1-87a0-b2678eef4f1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f4016718-f9dd-4283-b83b-d69ba19445b0",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ada8b4-be3c-45c3-8d09-b662b008d7aa",
   "metadata": {},
   "source": [
    "The Wilcoxon test\n",
    "Se usa cuando se tienen dos condiciones, ambas condiciones (modelos) actúan sobre los mismos objetos (datos)\n",
    "Ambos dan dos puntuaciones y la pregunta es si hay diferencias estadísticamente significativas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "464b5254-61f0-4d66-98e9-588e2fadd0e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WilcoxonResult(statistic=np.float64(5.0), pvalue=np.float64(1.0))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "x = np.array([0.5, 0.825, 0.375, 0.5])\n",
    "y = np.array([0.525, 0.775, 0.325, 0.55])\n",
    "res = stats.wilcoxon(x, y)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a96779-b1e2-4c47-bfcb-b4c642cb17be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
