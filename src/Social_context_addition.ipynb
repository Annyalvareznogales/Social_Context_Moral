{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import torch\n",
    "import warnings\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import f1_score\n",
    "from datasets import Dataset, DatasetDict\n",
    "from typing import Optional, Union, Tuple\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "from transformers import set_seed\n",
    "from collections import Counter\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from transformers import Trainer, TrainingArguments, AutoTokenizer, AutoConfig, BertPreTrainedModel, BertModel,AutoModel,LlamaPreTrainedModel, RobertaPreTrainedModel,AutoModelForSequenceClassification,RobertaModel, BertForSequenceClassification, RobertaForSequenceClassification\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "torch.cuda.empty_cache()  \n",
    "torch.cuda.ipc_collect()  \n",
    "from typing import Callable, List, Optional, Tuple, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=42\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "torch.cuda.manual_seed_all(42)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este notebook contiene:\n",
    "- el baseline de clasificación de sentimientos \n",
    "- la adición del contexto social con modelos (svd, deepwalk, node2vec, tadw)\n",
    "- la adición haciendo uso de la concatenación con el texto original\n",
    "\n",
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CLEAN DATA\n",
    "def cleaner1(tweet):\n",
    "    # remove usernames\n",
    "    # tweet = re.sub(\"@[A-Za-z0-9]+\",\"\",tweet)\n",
    "    tweet = tweet.lower()\n",
    "    tweet = re.sub(\"^rt\", \"\", tweet)\n",
    "    tweet = re.sub(\"\\s[0-9]+\\s\", \"\", tweet)\n",
    "    # remove usernames\n",
    "    tweet = re.sub(\"@[^\\s]+\", \"\", tweet)\n",
    "    tweet = re.sub(\"at_user\", \"\", tweet)\n",
    "    # remove urls\n",
    "    tweet = re.sub(\"pic.twitter.com/[A-Za-z0-9]+\", \"\", tweet)\n",
    "    tweet = re.sub(r\"(?:\\@|http?\\://|https?\\://|www)\\S+\", \"\", tweet)\n",
    "    tweet = tweet.replace(\"url\", \"\")\n",
    "    tweet = tweet.strip()\n",
    "    tweet = \" \".join(tweet.split())\n",
    "    return tweet\n",
    "\n",
    "\n",
    "\n",
    "#BINARY LABELS SENT\n",
    "def binary_labels(df):\n",
    "    df= df.replace({'label': {'neg': 0, 'pos': 1}})\n",
    "    id2label = {0: \"NEG\", 1: \"POS\"}\n",
    "    label2id = {\"NEG\": 0, \"POS\": 1}\n",
    "    return df, id2label, label2id\n",
    "\n",
    "#MULTICLASS LABELS SENT\n",
    "def multi_labels(df):\n",
    "    df= df.replace({\"label\": {\"negative\": 0, \"positive\": 1,\"neutral\":2}})\n",
    "    id2label = {0: \"NEGATIVE\", 1: \"POSITIVE\", 2:\"NEUTRAL\"}\n",
    "    label2id = {\"NEGATIVE\": 0, \"POSITIV\": 1, \"NEUTRAL\":2}\n",
    "    return df, id2label, label2id\n",
    "\n",
    "\n",
    "#MULTICLASS LABELS MORAL\n",
    "def label_multiclass6(df):\n",
    "    df= df.replace({'label': {'care': 1, 'harm': 1,\n",
    "                                'fairness': 2,'cheating': 2,\n",
    "                                'loyalty': 3,'betrayal': 3,\n",
    "                                'authority': 4,'subversion': 4,\n",
    "                                 'purity': 5,'degradation': 5,'nonmoral': 0,'nomoral': 0\n",
    "                                }})\n",
    "    \n",
    "    id2label = {0:\"NONMORAL\", 1:\"CARE\" ,1:\"HARM\",2:\"FAIRNESS\",2:\"CHEATING\",3:\"LOYALTY\",3:\"BETRAYAL\",4:\"AUTHORITY\",4:\"SUBVERSION\",5:\"PURITY\",5:\"DEGRADATION\"}\n",
    "    label2id = {\"NONMORAL\":0, \"CARE\": 1,\"HARM\":1,\"FAIRNESS\":2,\"CHEATING\":2,\"LOYALTY\":3,\"BETRAYAL\":3,\"AUTHORITY\":4,\"SUBVERSION\":4,\"PURITY\":5,\"DEGRADATION\":5}\n",
    "\n",
    "    return df, id2label,label2id \n",
    "\n",
    "def label_multiclass11(df):\n",
    "    df= df.replace({'label': {'care': 1, 'harm': 2,\n",
    "                                'fairness': 3,'cheating': 4,\n",
    "                                'loyalty': 5,'betrayal': 6,\n",
    "                                'authority': 7,'subversion': 8,\n",
    "                                 'purity': 9,'degradation': 10,'nonmoral': 0,'no moral': 0,'nomoral': 0\n",
    "                                }})\n",
    "    \n",
    "    id2label = {0:\"NONMORAL\", 1:\"CARE\" ,2:\"HARM\",3:\"FAIRNESS\",4:\"CHEATING\",5:\"LOYALTY\",6:\"BETRAYAL\",7:\"AUTHORITY\",8:\"SUBVERSION\",9:\"PURITY\",10:\"DEGRADATION\"}\n",
    "    label2id = {\"NONMORAL\":0, \"CARE\": 1,\"HARM\":2,\"FAIRNESS\":3,\"CHEATING\":4,\"LOYALTY\":5,\"BETRAYAL\":6,\"AUTHORITY\":7,\"SUBVERSION\":8,\"PURITY\":9,\"DEGRADATION\":10}\n",
    "\n",
    "    return df, id2label,label2id \n",
    "    \n",
    "# METRICS\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    #print(predictions)\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    #print(predictions)\n",
    "    #print(labels)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='macro')\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall}\n",
    "\n",
    "\n",
    "# TOKENIZER\n",
    "def tokenize_function(examples):\n",
    "    tokenized_inputs=tokenizer(examples[\"text\"], truncation=True)\n",
    "    return tokenized_inputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BERT MODEL (TEXT EMBEDDING + USER EMBEDDING)\n",
    "from transformers import Trainer, TrainingArguments, AutoTokenizer, AutoConfig, BertPreTrainedModel, BertModel,AutoModel, AutoModelForSequenceClassification, BertForSequenceClassification\n",
    "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n",
    "class BertClassificationHead(nn.Module):\n",
    "    \"\"\"Head for sentence-level classification tasks.\"\"\"\n",
    "\n",
    "    def __init__(self, config, num_extra_dims):\n",
    "        super().__init__()\n",
    "        total_dims = config.hidden_size + num_extra_dims \n",
    "        self.dense = nn.Linear(total_dims, total_dims)\n",
    "        classifier_dropout = (\n",
    "            config.classifier_dropout if config.classifier_dropout is not None else config.hidden_dropout_prob\n",
    "        )\n",
    "        self.dropout = nn.Dropout(classifier_dropout)\n",
    "        self.out_proj = nn.Linear(total_dims, config.num_labels) \n",
    "\n",
    "\n",
    "    def forward(self, features, **kwargs):\n",
    "        x = self.dropout(features)\n",
    "        x = self.dense(x)\n",
    "        x = torch.tanh(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.out_proj(x)\n",
    "        return x\n",
    "class CustomBertSequenceClassification(BertPreTrainedModel):\n",
    "    def __init__(self, config, num_extra_dims):\n",
    "        # Initialize the base model (BERT)\n",
    "        super().__init__(config)\n",
    "        self.config = config\n",
    "        total_dims = config.hidden_size + num_extra_dims \n",
    "        self.bert = BertModel(config)\n",
    "        \n",
    "        #classifier\n",
    "        self.num_labels = config.num_labels\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        #self.classifier = nn.Linear(total_dims, config.num_labels)\n",
    "        self.classifier = BertClassificationHead(config, num_extra_dims)\n",
    "\n",
    "        self.config = config\n",
    "        self.num_extra_dims = num_extra_dims\n",
    "        self.post_init()\n",
    "\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.LongTensor] = None,\n",
    "        attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        extra_data: Optional[torch.FloatTensor] = None,\n",
    "        token_type_ids: Optional[torch.LongTensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        head_mask: Optional[torch.FloatTensor] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple, SequenceClassifierOutput]:\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        \n",
    "        # Get output from the BERT model (transformer)\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        \n",
    "        sequence_output = outputs.pooler_output\n",
    "        sequence_output = self.dropout(sequence_output)\n",
    "     \n",
    "        if extra_data is not None:\n",
    "            output = torch.cat((sequence_output, extra_data), dim=-1)\n",
    "        else:\n",
    "            output = sequence_output\n",
    "\n",
    "        logits = self.classifier(output)\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            if self.config.problem_type is None:\n",
    "                if self.num_labels == 1:\n",
    "                    self.config.problem_type = \"regression\"\n",
    "                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n",
    "                    self.config.problem_type = \"single_label_classification\"\n",
    "                else:\n",
    "                    self.config.problem_type = \"multi_label_classification\"\n",
    "\n",
    "            if self.config.problem_type == \"regression\":\n",
    "                loss_fct = MSELoss()\n",
    "                if self.num_labels == 1:\n",
    "                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n",
    "                else:\n",
    "                    loss = loss_fct(logits, labels)\n",
    "            elif self.config.problem_type == \"single_label_classification\":\n",
    "                loss_fct = CrossEntropyLoss()\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            elif self.config.problem_type == \"multi_label_classification\":\n",
    "                loss_fct = BCEWithLogitsLoss()\n",
    "                loss = loss_fct(logits, labels)\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )\n",
    "        \n",
    "    @classmethod\n",
    "    def from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs):\n",
    "        \"\"\"\n",
    "        Override `from_pretrained` to handle `num_extra_dims` when loading a pre-trained model.\n",
    "        \"\"\"\n",
    "        num_extra_dims = kwargs.pop(\"num_extra_dims\", 0)  \n",
    "        config = kwargs.pop(\"config\", None)\n",
    "\n",
    "        if config is None:\n",
    "            config = AutoConfig.from_pretrained(pretrained_model_name_or_path, **kwargs)\n",
    "\n",
    "        config.num_extra_dims = num_extra_dims  \n",
    "\n",
    "        return super().from_pretrained(\n",
    "            pretrained_model_name_or_path,\n",
    "            *model_args,\n",
    "            config=config,\n",
    "            num_extra_dims=num_extra_dims,  \n",
    "            **kwargs\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ROBERTA MODEL (TEXT EMBEDDING + USER EMBEDDING)\n",
    "from transformers import Trainer, TrainingArguments, AutoTokenizer, AutoConfig, AutoModel,RobertaPreTrainedModel,AutoModelForSequenceClassification,RobertaModel,RobertaForSequenceClassification\n",
    "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n",
    "\n",
    "class RobertaClassificationHead(nn.Module):\n",
    "    \"\"\"Head for sentence-level classification tasks.\"\"\"\n",
    "\n",
    "    def __init__(self, config, num_extra_dims):\n",
    "        super().__init__()\n",
    "        total_dims = config.hidden_size + num_extra_dims  \n",
    "        self.dense = nn.Linear(total_dims, total_dims)\n",
    "        classifier_dropout = (\n",
    "            config.classifier_dropout if config.classifier_dropout is not None else config.hidden_dropout_prob\n",
    "        )\n",
    "        self.dropout = nn.Dropout(classifier_dropout)\n",
    "        self.out_proj = nn.Linear(total_dims, config.num_labels) \n",
    "\n",
    "\n",
    "    def forward(self, features, **kwargs):\n",
    "        x = self.dropout(features)\n",
    "        x = self.dense(x)\n",
    "        x = torch.tanh(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.out_proj(x)\n",
    "        return x\n",
    "\n",
    "class CustomRobertaForSequenceClassification(RobertaPreTrainedModel):\n",
    "    def __init__(self, config, num_extra_dims):\n",
    "        super().__init__(config)\n",
    "        self.config = config\n",
    "        self.num_extra_dims = num_extra_dims \n",
    "        total_dims = config.hidden_size + num_extra_dims \n",
    "        \n",
    "        # classifier \n",
    "        self.num_labels = config.num_labels\n",
    "\n",
    "        self.roberta = RobertaModel(config, add_pooling_layer=False)\n",
    "        self.classifier = RobertaClassificationHead(config, num_extra_dims)\n",
    "\n",
    "        self.post_init()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.LongTensor] = None,\n",
    "        attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        extra_data: Optional[torch.FloatTensor] = None,  \n",
    "        token_type_ids: Optional[torch.LongTensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        head_mask: Optional[torch.FloatTensor] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple[torch.Tensor], SequenceClassifierOutput]:\n",
    "        r\"\"\"\n",
    "        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n",
    "            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n",
    "            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n",
    "            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        outputs = self.roberta(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        sequence_output = outputs[0]\n",
    "        cls_output = sequence_output[:, 0, :] \n",
    "\n",
    "        if extra_data is not None:\n",
    "            cls_output = torch.cat((cls_output, extra_data), dim=-1)\n",
    "\n",
    "        logits = self.classifier(cls_output)\n",
    "            \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # move labels to correct device to enable model parallelism\n",
    "            labels = labels.to(logits.device)\n",
    "            if self.config.problem_type is None:\n",
    "                if self.num_labels == 1:\n",
    "                    self.config.problem_type = \"regression\"\n",
    "                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n",
    "                    self.config.problem_type = \"single_label_classification\"\n",
    "                else:\n",
    "                    self.config.problem_type = \"multi_label_classification\"\n",
    "\n",
    "            if self.config.problem_type == \"regression\":\n",
    "                loss_fct = MSELoss()\n",
    "                if self.num_labels == 1:\n",
    "                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n",
    "                else:\n",
    "                    loss = loss_fct(logits, labels)\n",
    "            elif self.config.problem_type == \"single_label_classification\":\n",
    "                loss_fct = CrossEntropyLoss()\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            elif self.config.problem_type == \"multi_label_classification\":\n",
    "                loss_fct = BCEWithLogitsLoss()\n",
    "                loss = loss_fct(logits, labels)\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs):\n",
    "        \"\"\"\n",
    "        Override `from_pretrained` to handle `num_extra_dims` when loading a pre-trained model.\n",
    "        \"\"\"\n",
    "        num_extra_dims = kwargs.pop(\"num_extra_dims\", 0)  \n",
    "        config = kwargs.pop(\"config\", None)\n",
    "\n",
    "        if config is None:\n",
    "            config = AutoConfig.from_pretrained(pretrained_model_name_or_path, **kwargs)\n",
    "\n",
    "        config.num_extra_dims = num_extra_dims  \n",
    "\n",
    "        return super().from_pretrained(\n",
    "            pretrained_model_name_or_path,\n",
    "            *model_args,\n",
    "            config=config,\n",
    "            num_extra_dims=num_extra_dims,  \n",
    "            **kwargs\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LLAMA MODEL (TEXT EMBEDDING + USER EMBEDDING)\n",
    "from typing import Optional, Tuple, List, Union\n",
    "from transformers import LlamaPreTrainedModel, LlamaModel, AutoConfig\n",
    "from transformers.modeling_outputs import SequenceClassifierOutputWithPast\n",
    "\n",
    "class CustomLlamaForSequenceClassification(LlamaPreTrainedModel):\n",
    "    def __init__(self, config, num_extra_dims):\n",
    "        super().__init__(config)\n",
    "        self.config = config\n",
    "        self.num_labels = 6\n",
    "        self.num_extra_dims = num_extra_dims \n",
    "        total_dims = config.hidden_size + num_extra_dims \n",
    "        self.config.pad_token_id = self.config.eos_token_id  \n",
    "\n",
    "        self.model = LlamaModel(config)\n",
    "        self.score = nn.Linear(config.hidden_size + num_extra_dims, self.num_labels, bias=False)\n",
    "\n",
    "        self.post_init()\n",
    "\n",
    "    def get_input_embeddings(self):\n",
    "        return self.model.embed_tokens\n",
    "\n",
    "    def set_input_embeddings(self, value):\n",
    "        self.model.embed_tokens = value\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.LongTensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        extra_data: Optional[torch.FloatTensor] = None,  \n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple, SequenceClassifierOutputWithPast]:\n",
    "        r\"\"\"\n",
    "        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n",
    "            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n",
    "            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n",
    "            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        transformer_outputs = self.model(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        \n",
    "        hidden_states = transformer_outputs[0]\n",
    "        if extra_data is not None:\n",
    "            extra_data = extra_data.unsqueeze(1)  # Reshape to (batch_size, 1, num_extra_dims)\n",
    "            extra_data = extra_data.expand(-1, hidden_states.shape[1], -1)  # Expand to (batch_size, sequence_length, num_extra_dims)\n",
    "            hidden_states = torch.cat((hidden_states, extra_data), dim=-1)\n",
    "            pooled_representation = hidden_states[:, -1, :]# Shape: (batch_size, sequence_length, hidden_size + num_extra_dims)\n",
    "            logits = self.score(pooled_representation.to(self.score.weight.dtype))\n",
    "            #print(f\"Logits shape: {logits.shape}\")\n",
    "\n",
    "        else:\n",
    "            logits = self.score(hidden_states)\n",
    "            #print(f\"Logits shape: {logits.shape}\")\n",
    "\n",
    "        if input_ids is not None:\n",
    "            batch_size = input_ids.shape[0]\n",
    "        else:\n",
    "            batch_size = inputs_embeds.shape[0]\n",
    "        \n",
    "        if self.config.pad_token_id is None and batch_size != 1:\n",
    "            raise ValueError(\"Cannot handle batch sizes > 1 if no padding token is defined.\")\n",
    "        \n",
    "        # Determine the last non-pad token position (for padding handling)\n",
    "        if self.config.pad_token_id is None:\n",
    "            last_non_pad_token = -1\n",
    "        elif input_ids is not None:\n",
    "            non_pad_mask = (input_ids != self.config.pad_token_id).to(logits.device, torch.int32)\n",
    "            token_indices = torch.arange(input_ids.shape[-1], device=logits.device)\n",
    "            last_non_pad_token = (token_indices * non_pad_mask).argmax(-1)\n",
    "        else:\n",
    "            last_non_pad_token = -1\n",
    "        #print(f\"Last non-pad token indices: {last_non_pad_token}\")\n",
    "        \n",
    "        if extra_data is None:\n",
    "            pooled_logits = logits[torch.arange(batch_size, device=logits.device), last_non_pad_token]\n",
    "        else:\n",
    "            pooled_logits = logits    \n",
    "        #print(f\"Pooled logits shape before checking: {pooled_logits.shape}\")\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = self.loss_function(logits=logits, labels=labels, pooled_logits=pooled_logits, config=self.config)\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (pooled_logits,) + transformer_outputs[1:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return SequenceClassifierOutputWithPast(\n",
    "            loss=loss,\n",
    "            logits=pooled_logits,\n",
    "            past_key_values=transformer_outputs.past_key_values,\n",
    "            hidden_states=transformer_outputs.hidden_states,\n",
    "            attentions=transformer_outputs.attentions,\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs):\n",
    "        \"\"\"\n",
    "        Override `from_pretrained` to handle `num_extra_dims` when loading a pre-trained model.\n",
    "        \"\"\"\n",
    "        num_extra_dims = kwargs.pop(\"num_extra_dims\", 0)  \n",
    "        config = kwargs.pop(\"config\", None)\n",
    "\n",
    "        if config is None:\n",
    "            config = AutoConfig.from_pretrained(pretrained_model_name_or_path, **kwargs)\n",
    "\n",
    "        config.num_extra_dims = num_extra_dims  \n",
    "\n",
    "        return super().from_pretrained(\n",
    "            pretrained_model_name_or_path,\n",
    "            *model_args,\n",
    "            config=config,\n",
    "            num_extra_dims=num_extra_dims,  \n",
    "            **kwargs\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEEPSEEPK Model\n",
    "from typing import Optional, Tuple, List, Union\n",
    "from transformers import DeepseekV3PreTrainedModel, DeepseekV3Model, DeepseekV3Config\n",
    "from transformers.modeling_outputs import SequenceClassifierOutputWithPast\n",
    "from transformers.utils import (\n",
    "    add_start_docstrings,\n",
    "    add_start_docstrings_to_model_forward,\n",
    "    is_flash_attn_2_available,\n",
    "    is_flash_attn_greater_or_equal_2_10,\n",
    "    logging,\n",
    "    replace_return_docstrings,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "class CustomDeepseekV3ForSequenceClassification(DeepseekV3PreTrainedModel):\n",
    "    def __init__(self, config,num_extra_dims):\n",
    "        super().__init__(config)\n",
    "        self.config = config\n",
    "        self.num_labels = 2\n",
    "        self.num_extra_dims = num_extra_dims \n",
    "        total_dims = config.hidden_size + num_extra_dims\n",
    "        self.config.pad_token_id = self.config.eos_token_id  \n",
    "        \n",
    "        self.model = DeepseekV3Model(config)\n",
    "        self.score = nn.Linear(total_dims, self.num_labels, bias=False)\n",
    "\n",
    "        self.post_init()\n",
    "\n",
    "    def get_input_embeddings(self):\n",
    "        return self.model.embed_tokens\n",
    "\n",
    "    def set_input_embeddings(self, value):\n",
    "        self.model.embed_tokens = value\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        extra_data: Optional[torch.FloatTensor] = None,  \n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple, SequenceClassifierOutputWithPast]:\n",
    "        r\"\"\"\n",
    "        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n",
    "            Labels for computing the sequence classification/regression loss. Indices should be in `[0, transformers.,\n",
    "            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n",
    "            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
    "        \"\"\"\n",
    "        return_dict = (\n",
    "            return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        )\n",
    "\n",
    "        transformer_outputs = self.model(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        hidden_states = transformer_outputs[0]\n",
    "        \n",
    "        if extra_data is not None:\n",
    "            extra_data = extra_data.unsqueeze(1)  \n",
    "            extra_data = extra_data.expand(-1, hidden_states.shape[1], -1)  \n",
    "            hidden_states = torch.cat((hidden_states, extra_data), dim=-1)\n",
    "            pooled_representation = hidden_states[:, -1, :]\n",
    "            logits = self.score(pooled_representation.to(self.score.weight.dtype))\n",
    "            #print(f\"Logits shape: {logits.shape}\")\n",
    "\n",
    "        else:\n",
    "            logits = self.score(hidden_states)\n",
    "            #print(f\"Logits shape: {logits.shape}\")\n",
    "\n",
    "        if input_ids is not None:\n",
    "            batch_size = input_ids.shape[0]\n",
    "        else:\n",
    "            batch_size = inputs_embeds.shape[0]\n",
    "\n",
    "        if self.config.pad_token_id is None and batch_size != 1:\n",
    "            raise ValueError(\n",
    "                \"Cannot handle batch sizes > 1 if no padding token is defined.\"\n",
    "            )\n",
    "        if self.config.pad_token_id is None:\n",
    "            sequence_lengths = -1\n",
    "        else:\n",
    "            if input_ids is not None:\n",
    "                sequence_lengths = (\n",
    "                    torch.eq(input_ids, self.config.pad_token_id).int().argmax(-1) - 1\n",
    "                ).to(logits.device)\n",
    "            else:\n",
    "                sequence_lengths = -1\n",
    "        if extra_data is None:\n",
    "            pooled_logits = logits[\n",
    "                torch.arange(batch_size, device=logits.device), sequence_lengths\n",
    "            ]\n",
    "        else:\n",
    "            pooled_logits = logits    \n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            labels = labels.to(logits.device)\n",
    "            if self.config.problem_type is None:\n",
    "                if self.num_labels == 1:\n",
    "                    self.config.problem_type = \"regression\"\n",
    "                elif self.num_labels > 1 and (\n",
    "                    labels.dtype == torch.long or labels.dtype == torch.int\n",
    "                ):\n",
    "                    self.config.problem_type = \"single_label_classification\"\n",
    "                else:\n",
    "                    self.config.problem_type = \"multi_label_classification\"\n",
    "\n",
    "            if self.config.problem_type == \"regression\":\n",
    "                loss_fct = MSELoss()\n",
    "                if self.num_labels == 1:\n",
    "                    loss = loss_fct(pooled_logits.squeeze(), labels.squeeze())\n",
    "                else:\n",
    "                    loss = loss_fct(pooled_logits, labels)\n",
    "            elif self.config.problem_type == \"single_label_classification\":\n",
    "                loss_fct = CrossEntropyLoss()\n",
    "                loss = loss_fct(\n",
    "                    pooled_logits.view(-1, self.num_labels), labels.view(-1)\n",
    "                )\n",
    "            elif self.config.problem_type == \"multi_label_classification\":\n",
    "                loss_fct = BCEWithLogitsLoss()\n",
    "                loss = loss_fct(pooled_logits, labels)\n",
    "        if not return_dict:\n",
    "            output = (pooled_logits,) + transformer_outputs[1:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return SequenceClassifierOutputWithPast(\n",
    "            loss=loss,\n",
    "            logits=pooled_logits,\n",
    "            past_key_values=transformer_outputs.past_key_values,\n",
    "            hidden_states=transformer_outputs.hidden_states,\n",
    "            attentions=transformer_outputs.attentions,\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(\n",
    "        cls,\n",
    "        pretrained_model_name_or_path: str,\n",
    "        *model_args,\n",
    "        **kwargs\n",
    "    ):\n",
    "        num_extra_dims = kwargs.pop(\"num_extra_dims\", 0)\n",
    "        config = kwargs.pop(\"config\", None)\n",
    "\n",
    "        if config is None:\n",
    "            config = DeepseekV3Config.from_pretrained(pretrained_model_name_or_path, **kwargs)\n",
    "\n",
    "        config.num_extra_dims = num_extra_dims\n",
    "\n",
    "        return super().from_pretrained(\n",
    "            pretrained_model_name_or_path,\n",
    "            *model_args,\n",
    "            config=config,\n",
    "            num_extra_dims=num_extra_dims,\n",
    "            **kwargs,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#QWEN2 \n",
    "from typing import Optional, Tuple, List, Union\n",
    "#from transformers.models.deepseek_v3 import *\n",
    "from transformers import Qwen2PreTrainedModel, Qwen2Model, Qwen2Config\n",
    "from transformers.modeling_outputs import SequenceClassifierOutputWithPast\n",
    "from transformers.cache_utils import Cache\n",
    "class CustomQwen2ForSequenceClassification(Qwen2PreTrainedModel):\n",
    "    def __init__(self, config,num_extra_dims):\n",
    "        super().__init__(config)\n",
    "        self.config = config\n",
    "        self.num_labels = config.num_labels\n",
    "        self.num_extra_dims = num_extra_dims\n",
    "        total_dims = config.hidden_size + num_extra_dims\n",
    "        self.config.pad_token_id = self.config.eos_token_id  \n",
    "        \n",
    "        self.model = Qwen2Model(config)\n",
    "        self.score = nn.Linear(total_dims,self.num_labels, bias=False)\n",
    "\n",
    "        self.post_init()\n",
    "\n",
    "    def get_input_embeddings(self):\n",
    "        return self.model.embed_tokens\n",
    "\n",
    "    def set_input_embeddings(self, value):\n",
    "        self.model.embed_tokens = value\n",
    "\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.LongTensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        extra_data: Optional[torch.FloatTensor] = None,  \n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_values: Optional[Cache] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "    ) -> SequenceClassifierOutputWithPast:\n",
    "        r\"\"\"\n",
    "        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n",
    "            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n",
    "            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n",
    "            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
    "        \"\"\"\n",
    "\n",
    "        transformer_outputs: BaseModelOutputWithPast = self.model(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "        )\n",
    "        hidden_states = transformer_outputs.last_hidden_state\n",
    "        \n",
    "        if extra_data is not None:\n",
    "            extra_data = extra_data.unsqueeze(1)  \n",
    "            extra_data = extra_data.expand(-1, hidden_states.shape[1], -1)  \n",
    "            hidden_states = torch.cat((hidden_states, extra_data), dim=-1)\n",
    "            pooled_representation = hidden_states[:, -1, :]\n",
    "            logits = self.score(pooled_representation.to(self.score.weight.dtype))\n",
    "            #print(f\"Logits shape: {logits.shape}\")\n",
    "\n",
    "        else:\n",
    "            logits = self.score(hidden_states)\n",
    "            #print(f\"Logits shape: {logits.shape}\")\n",
    "\n",
    "            \n",
    "        if input_ids is not None:\n",
    "            batch_size = input_ids.shape[0]\n",
    "        else:\n",
    "            batch_size = inputs_embeds.shape[0]\n",
    "\n",
    "        if self.config.pad_token_id is None and batch_size != 1:\n",
    "            raise ValueError(\"Cannot handle batch sizes > 1 if no padding token is defined.\")\n",
    "        if self.config.pad_token_id is None:\n",
    "            last_non_pad_token = -1\n",
    "        elif input_ids is not None:\n",
    "            # To handle both left- and right- padding, we take the rightmost token that is not equal to pad_token_id\n",
    "            non_pad_mask = (input_ids != self.config.pad_token_id).to(logits.device, torch.int32)\n",
    "            token_indices = torch.arange(input_ids.shape[-1], device=logits.device, dtype=torch.int32)\n",
    "            last_non_pad_token = (token_indices * non_pad_mask).argmax(-1)\n",
    "        else:\n",
    "            last_non_pad_token = -1\n",
    "            logger.warning_once(\n",
    "                f\"{self.__class__.__name__} will not detect padding tokens in `inputs_embeds`. Results may be \"\n",
    "                \"unexpected if using padding tokens in conjunction with `inputs_embeds.`\"\n",
    "            )\n",
    "\n",
    "        if extra_data is None:\n",
    "            pooled_logits = logits[torch.arange(batch_size, device=logits.device), last_non_pad_token]\n",
    "        else:\n",
    "            pooled_logits = logits\n",
    "            \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = self.loss_function(logits=logits, labels=labels, pooled_logits=pooled_logits, config=self.config)\n",
    "\n",
    "        return SequenceClassifierOutputWithPast(\n",
    "            loss=loss,\n",
    "            logits=pooled_logits,\n",
    "            past_key_values=transformer_outputs.past_key_values,\n",
    "            hidden_states=transformer_outputs.hidden_states,\n",
    "            attentions=transformer_outputs.attentions,\n",
    "        )\n",
    "\n",
    "        \n",
    "    @classmethod\n",
    "    def from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs):\n",
    "        \"\"\"\n",
    "        Override `from_pretrained` to handle `num_extra_dims` when loading a pre-trained model.\n",
    "        \"\"\"\n",
    "        num_extra_dims = kwargs.pop(\"num_extra_dims\", 0) \n",
    "        config = kwargs.pop(\"config\", None)\n",
    "\n",
    "        if config is None:\n",
    "            config = AutoConfig.from_pretrained(pretrained_model_name_or_path, **kwargs)\n",
    "\n",
    "        config.num_extra_dims = num_extra_dims  \n",
    "\n",
    "        return super().from_pretrained(\n",
    "            pretrained_model_name_or_path,\n",
    "            *model_args,\n",
    "            config=config,\n",
    "            num_extra_dims=num_extra_dims,              **kwargs\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Fine Tuning (TEXTO + CS) Bert Roberta**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nonmoral' 'authority' 'loyalty' 'purity' 'care' 'fairness']\n",
      "['nonmoral' 'authority' 'loyalty' 'purity' 'care' 'fairness']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet.id</th>\n",
       "      <th>user.id</th>\n",
       "      <th>dataset.author.name</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>moralbert</th>\n",
       "      <th>roberta_mmp</th>\n",
       "      <th>roberta_mm</th>\n",
       "      <th>liwc_mfd</th>\n",
       "      <th>persp_bert</th>\n",
       "      <th>moral_label</th>\n",
       "      <th>moral_label_polarity</th>\n",
       "      <th>svd</th>\n",
       "      <th>deepwalk</th>\n",
       "      <th>node2vec</th>\n",
       "      <th>tadw</th>\n",
       "      <th>moral_label2</th>\n",
       "      <th>extra_data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>936469851</td>\n",
       "      <td>10323542</td>\n",
       "      <td>drgilpin</td>\n",
       "      <td>nonmoral</td>\n",
       "      <td>watching by myself #tweetdebate not drinking :...</td>\n",
       "      <td>nonmoral</td>\n",
       "      <td>nomoral</td>\n",
       "      <td>no moral</td>\n",
       "      <td>no moral</td>\n",
       "      <td>nonmoral</td>\n",
       "      <td>nonmoral</td>\n",
       "      <td>no moral</td>\n",
       "      <td>[-0.9321853169454714, -0.014290048491863475, -...</td>\n",
       "      <td>[-0.43476266, 0.4637962, 1.0409873, -0.2093216...</td>\n",
       "      <td>[-0.27371866, -0.19295375, -0.37779337, 0.6663...</td>\n",
       "      <td>[-0.15936547082592128, 0.23132371423577064, -0...</td>\n",
       "      <td>nonmoral</td>\n",
       "      <td>[-0.43476266, 0.4637962, 1.0409873, -0.2093216...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>936470432</td>\n",
       "      <td>11752272</td>\n",
       "      <td>starweaver</td>\n",
       "      <td>nonmoral</td>\n",
       "      <td>yeah, slime was actually my second choice, can...</td>\n",
       "      <td>nonmoral</td>\n",
       "      <td>nomoral</td>\n",
       "      <td>no moral</td>\n",
       "      <td>sanctity.vice</td>\n",
       "      <td>loyalty</td>\n",
       "      <td>nonmoral</td>\n",
       "      <td>no moral</td>\n",
       "      <td>[-2.3407145936409695, 0.29875694807026504, 1.1...</td>\n",
       "      <td>[-2.061748, 0.2598792, -0.2871443, -1.006268, ...</td>\n",
       "      <td>[0.077606544, 0.21957941, -0.21372578, 0.02413...</td>\n",
       "      <td>[-0.13507492718699363, 0.22315517862073722, -0...</td>\n",
       "      <td>nonmoral</td>\n",
       "      <td>[-2.061748, 0.2598792, -0.2871443, -1.006268, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>936472030</td>\n",
       "      <td>716543</td>\n",
       "      <td>kyeung808</td>\n",
       "      <td>nonmoral</td>\n",
       "      <td>preparing to have a heart attack #tweetdebate</td>\n",
       "      <td>degradation</td>\n",
       "      <td>nomoral</td>\n",
       "      <td>no moral</td>\n",
       "      <td>care.vice</td>\n",
       "      <td>nonmoral</td>\n",
       "      <td>nonmoral</td>\n",
       "      <td>degradation</td>\n",
       "      <td>[-0.4696246816045738, 0.011188949802830955, -0...</td>\n",
       "      <td>[-0.06279127, 0.038863122, 0.12867297, 0.29538...</td>\n",
       "      <td>[0.43654078, 0.22683652, 0.2081876, -0.0945255...</td>\n",
       "      <td>[-0.1729719604261917, 0.19232023246154403, -0....</td>\n",
       "      <td>nonmoral</td>\n",
       "      <td>[-0.06279127, 0.038863122, 0.12867297, 0.29538...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>936472042</td>\n",
       "      <td>14759482</td>\n",
       "      <td>rebot</td>\n",
       "      <td>nonmoral</td>\n",
       "      <td>no debate moderators under 50, sorry #tweetdebate</td>\n",
       "      <td>nonmoral</td>\n",
       "      <td>nomoral</td>\n",
       "      <td>no moral</td>\n",
       "      <td>no moral</td>\n",
       "      <td>nonmoral</td>\n",
       "      <td>nonmoral</td>\n",
       "      <td>no moral</td>\n",
       "      <td>[-0.4748503994385164, 0.16479432293878057, -0....</td>\n",
       "      <td>[1.3948498, 0.08554719, -0.14505437, -1.072701...</td>\n",
       "      <td>[0.31162417, 0.17204364, 0.14859162, 0.2996835...</td>\n",
       "      <td>[-0.09415871666324986, 0.22696396302001293, -0...</td>\n",
       "      <td>nonmoral</td>\n",
       "      <td>[1.3948498, 0.08554719, -0.14505437, -1.072701...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>936472907</td>\n",
       "      <td>6035262</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>nonmoral</td>\n",
       "      <td>now staring at black screen on grrrrrrrrrrrrrr...</td>\n",
       "      <td>nonmoral</td>\n",
       "      <td>nomoral</td>\n",
       "      <td>no moral</td>\n",
       "      <td>no moral</td>\n",
       "      <td>fairness</td>\n",
       "      <td>nonmoral</td>\n",
       "      <td>no moral</td>\n",
       "      <td>[-0.9144128643032162, 0.09508063320665167, -0....</td>\n",
       "      <td>[-0.35507488, -0.33618665, 0.93690664, 2.07505...</td>\n",
       "      <td>[-0.15241532, 0.32681182, 0.1377003, 0.2165719...</td>\n",
       "      <td>[-0.13134790549379227, 0.23772991422683537, -0...</td>\n",
       "      <td>nonmoral</td>\n",
       "      <td>[-0.35507488, -0.33618665, 0.93690664, 2.07505...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1762</th>\n",
       "      <td>936727686</td>\n",
       "      <td>5752932</td>\n",
       "      <td>crysharris</td>\n",
       "      <td>nonmoral</td>\n",
       "      <td>you missed the bit where mccain talked over ob...</td>\n",
       "      <td>nonmoral</td>\n",
       "      <td>nomoral</td>\n",
       "      <td>no moral</td>\n",
       "      <td>fairness.vice</td>\n",
       "      <td>authority</td>\n",
       "      <td>nonmoral</td>\n",
       "      <td>no moral</td>\n",
       "      <td>[-0.17597257154484572, -0.0780247591016859, 0....</td>\n",
       "      <td>[0.94359154, -2.4664073, -0.352156, 0.9408136,...</td>\n",
       "      <td>[-0.2640397, -0.0024188492, -0.2750953, 0.8507...</td>\n",
       "      <td>[-0.19121226784674555, 0.12374432476479204, -0...</td>\n",
       "      <td>nonmoral</td>\n",
       "      <td>[0.94359154, -2.4664073, -0.352156, 0.9408136,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1763</th>\n",
       "      <td>936731119</td>\n",
       "      <td>812825</td>\n",
       "      <td>MandianaJones</td>\n",
       "      <td>nonmoral</td>\n",
       "      <td>has hashtagged the debate #debate08. same tag ...</td>\n",
       "      <td>nonmoral</td>\n",
       "      <td>nomoral</td>\n",
       "      <td>no moral</td>\n",
       "      <td>no moral</td>\n",
       "      <td>fairness</td>\n",
       "      <td>nonmoral</td>\n",
       "      <td>no moral</td>\n",
       "      <td>[-2.225933705819569, 0.53964464069966, -0.7008...</td>\n",
       "      <td>[-0.3015819, -0.7410528, 0.46232414, 0.0280605...</td>\n",
       "      <td>[0.23174001, -0.27921534, 0.2237174, 0.6746251...</td>\n",
       "      <td>[-0.1838567057770515, 0.2297698874173019, -0.2...</td>\n",
       "      <td>nonmoral</td>\n",
       "      <td>[-0.3015819, -0.7410528, 0.46232414, 0.0280605...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1764</th>\n",
       "      <td>936732650</td>\n",
       "      <td>666913</td>\n",
       "      <td>Autumm</td>\n",
       "      <td>nonmoral</td>\n",
       "      <td>#debate08 fox says mccain won the debate. msnb...</td>\n",
       "      <td>nonmoral</td>\n",
       "      <td>nomoral</td>\n",
       "      <td>no moral</td>\n",
       "      <td>no moral</td>\n",
       "      <td>nonmoral</td>\n",
       "      <td>nonmoral</td>\n",
       "      <td>no moral</td>\n",
       "      <td>[-1.030078339024446, 0.22364630412766384, -0.9...</td>\n",
       "      <td>[-0.013444448, -1.1800328, -0.1991519, 2.45609...</td>\n",
       "      <td>[0.20520344, 0.29482052, -0.14640543, -0.16537...</td>\n",
       "      <td>[-0.18213415458960597, 0.22508570453273133, -0...</td>\n",
       "      <td>nonmoral</td>\n",
       "      <td>[-0.013444448, -1.1800328, -0.1991519, 2.45609...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1765</th>\n",
       "      <td>936733111</td>\n",
       "      <td>7769402</td>\n",
       "      <td>Laurie2</td>\n",
       "      <td>nonmoral</td>\n",
       "      <td>#debate08 did your favorite candidate \"beat\" t...</td>\n",
       "      <td>nonmoral</td>\n",
       "      <td>cheating</td>\n",
       "      <td>no moral</td>\n",
       "      <td>no moral</td>\n",
       "      <td>fairness</td>\n",
       "      <td>nonmoral</td>\n",
       "      <td>no moral</td>\n",
       "      <td>[-0.10846916464087902, 0.02237691547738228, -0...</td>\n",
       "      <td>[0.86933273, 1.3993437, -1.373814, -0.7129098,...</td>\n",
       "      <td>[0.1895242, 0.26178938, -0.05714034, 0.1517531...</td>\n",
       "      <td>[-0.020512648254196127, 0.23753842475034861, -...</td>\n",
       "      <td>nonmoral</td>\n",
       "      <td>[0.86933273, 1.3993437, -1.373814, -0.7129098,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1766</th>\n",
       "      <td>936734021</td>\n",
       "      <td>6424612</td>\n",
       "      <td>Nicolas Ward</td>\n",
       "      <td>nonmoral</td>\n",
       "      <td>#momocrats #current seriously, as a woman, you...</td>\n",
       "      <td>nonmoral</td>\n",
       "      <td>nomoral</td>\n",
       "      <td>no moral</td>\n",
       "      <td>no moral</td>\n",
       "      <td>loyalty</td>\n",
       "      <td>nonmoral</td>\n",
       "      <td>no moral</td>\n",
       "      <td>[-0.9013662538366514, 0.018131769414906233, -0...</td>\n",
       "      <td>[-1.3005122, -0.65894294, -0.8006101, 0.455483...</td>\n",
       "      <td>[-0.45010868, 0.07405205, -0.35903597, -0.1332...</td>\n",
       "      <td>[-0.13600820748917195, 0.2566080603314177, -0....</td>\n",
       "      <td>nonmoral</td>\n",
       "      <td>[-1.3005122, -0.65894294, -0.8006101, 0.455483...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1767 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       tweet.id   user.id dataset.author.name     label  \\\n",
       "0     936469851  10323542            drgilpin  nonmoral   \n",
       "1     936470432  11752272          starweaver  nonmoral   \n",
       "2     936472030    716543           kyeung808  nonmoral   \n",
       "3     936472042  14759482               rebot  nonmoral   \n",
       "4     936472907   6035262              Karoli  nonmoral   \n",
       "...         ...       ...                 ...       ...   \n",
       "1762  936727686   5752932          crysharris  nonmoral   \n",
       "1763  936731119    812825       MandianaJones  nonmoral   \n",
       "1764  936732650    666913              Autumm  nonmoral   \n",
       "1765  936733111   7769402             Laurie2  nonmoral   \n",
       "1766  936734021   6424612        Nicolas Ward  nonmoral   \n",
       "\n",
       "                                                   text    moralbert  \\\n",
       "0     watching by myself #tweetdebate not drinking :...     nonmoral   \n",
       "1     yeah, slime was actually my second choice, can...     nonmoral   \n",
       "2         preparing to have a heart attack #tweetdebate  degradation   \n",
       "3     no debate moderators under 50, sorry #tweetdebate     nonmoral   \n",
       "4     now staring at black screen on grrrrrrrrrrrrrr...     nonmoral   \n",
       "...                                                 ...          ...   \n",
       "1762  you missed the bit where mccain talked over ob...     nonmoral   \n",
       "1763  has hashtagged the debate #debate08. same tag ...     nonmoral   \n",
       "1764  #debate08 fox says mccain won the debate. msnb...     nonmoral   \n",
       "1765  #debate08 did your favorite candidate \"beat\" t...     nonmoral   \n",
       "1766  #momocrats #current seriously, as a woman, you...     nonmoral   \n",
       "\n",
       "     roberta_mmp roberta_mm       liwc_mfd persp_bert moral_label  \\\n",
       "0        nomoral   no moral       no moral   nonmoral    nonmoral   \n",
       "1        nomoral   no moral  sanctity.vice    loyalty    nonmoral   \n",
       "2        nomoral   no moral      care.vice   nonmoral    nonmoral   \n",
       "3        nomoral   no moral       no moral   nonmoral    nonmoral   \n",
       "4        nomoral   no moral       no moral   fairness    nonmoral   \n",
       "...          ...        ...            ...        ...         ...   \n",
       "1762     nomoral   no moral  fairness.vice  authority    nonmoral   \n",
       "1763     nomoral   no moral       no moral   fairness    nonmoral   \n",
       "1764     nomoral   no moral       no moral   nonmoral    nonmoral   \n",
       "1765    cheating   no moral       no moral   fairness    nonmoral   \n",
       "1766     nomoral   no moral       no moral    loyalty    nonmoral   \n",
       "\n",
       "     moral_label_polarity                                                svd  \\\n",
       "0                no moral  [-0.9321853169454714, -0.014290048491863475, -...   \n",
       "1                no moral  [-2.3407145936409695, 0.29875694807026504, 1.1...   \n",
       "2             degradation  [-0.4696246816045738, 0.011188949802830955, -0...   \n",
       "3                no moral  [-0.4748503994385164, 0.16479432293878057, -0....   \n",
       "4                no moral  [-0.9144128643032162, 0.09508063320665167, -0....   \n",
       "...                   ...                                                ...   \n",
       "1762             no moral  [-0.17597257154484572, -0.0780247591016859, 0....   \n",
       "1763             no moral  [-2.225933705819569, 0.53964464069966, -0.7008...   \n",
       "1764             no moral  [-1.030078339024446, 0.22364630412766384, -0.9...   \n",
       "1765             no moral  [-0.10846916464087902, 0.02237691547738228, -0...   \n",
       "1766             no moral  [-0.9013662538366514, 0.018131769414906233, -0...   \n",
       "\n",
       "                                               deepwalk  \\\n",
       "0     [-0.43476266, 0.4637962, 1.0409873, -0.2093216...   \n",
       "1     [-2.061748, 0.2598792, -0.2871443, -1.006268, ...   \n",
       "2     [-0.06279127, 0.038863122, 0.12867297, 0.29538...   \n",
       "3     [1.3948498, 0.08554719, -0.14505437, -1.072701...   \n",
       "4     [-0.35507488, -0.33618665, 0.93690664, 2.07505...   \n",
       "...                                                 ...   \n",
       "1762  [0.94359154, -2.4664073, -0.352156, 0.9408136,...   \n",
       "1763  [-0.3015819, -0.7410528, 0.46232414, 0.0280605...   \n",
       "1764  [-0.013444448, -1.1800328, -0.1991519, 2.45609...   \n",
       "1765  [0.86933273, 1.3993437, -1.373814, -0.7129098,...   \n",
       "1766  [-1.3005122, -0.65894294, -0.8006101, 0.455483...   \n",
       "\n",
       "                                               node2vec  \\\n",
       "0     [-0.27371866, -0.19295375, -0.37779337, 0.6663...   \n",
       "1     [0.077606544, 0.21957941, -0.21372578, 0.02413...   \n",
       "2     [0.43654078, 0.22683652, 0.2081876, -0.0945255...   \n",
       "3     [0.31162417, 0.17204364, 0.14859162, 0.2996835...   \n",
       "4     [-0.15241532, 0.32681182, 0.1377003, 0.2165719...   \n",
       "...                                                 ...   \n",
       "1762  [-0.2640397, -0.0024188492, -0.2750953, 0.8507...   \n",
       "1763  [0.23174001, -0.27921534, 0.2237174, 0.6746251...   \n",
       "1764  [0.20520344, 0.29482052, -0.14640543, -0.16537...   \n",
       "1765  [0.1895242, 0.26178938, -0.05714034, 0.1517531...   \n",
       "1766  [-0.45010868, 0.07405205, -0.35903597, -0.1332...   \n",
       "\n",
       "                                                   tadw moral_label2  \\\n",
       "0     [-0.15936547082592128, 0.23132371423577064, -0...     nonmoral   \n",
       "1     [-0.13507492718699363, 0.22315517862073722, -0...     nonmoral   \n",
       "2     [-0.1729719604261917, 0.19232023246154403, -0....     nonmoral   \n",
       "3     [-0.09415871666324986, 0.22696396302001293, -0...     nonmoral   \n",
       "4     [-0.13134790549379227, 0.23772991422683537, -0...     nonmoral   \n",
       "...                                                 ...          ...   \n",
       "1762  [-0.19121226784674555, 0.12374432476479204, -0...     nonmoral   \n",
       "1763  [-0.1838567057770515, 0.2297698874173019, -0.2...     nonmoral   \n",
       "1764  [-0.18213415458960597, 0.22508570453273133, -0...     nonmoral   \n",
       "1765  [-0.020512648254196127, 0.23753842475034861, -...     nonmoral   \n",
       "1766  [-0.13600820748917195, 0.2566080603314177, -0....     nonmoral   \n",
       "\n",
       "                                             extra_data  \n",
       "0     [-0.43476266, 0.4637962, 1.0409873, -0.2093216...  \n",
       "1     [-2.061748, 0.2598792, -0.2871443, -1.006268, ...  \n",
       "2     [-0.06279127, 0.038863122, 0.12867297, 0.29538...  \n",
       "3     [1.3948498, 0.08554719, -0.14505437, -1.072701...  \n",
       "4     [-0.35507488, -0.33618665, 0.93690664, 2.07505...  \n",
       "...                                                 ...  \n",
       "1762  [0.94359154, -2.4664073, -0.352156, 0.9408136,...  \n",
       "1763  [-0.3015819, -0.7410528, 0.46232414, 0.0280605...  \n",
       "1764  [-0.013444448, -1.1800328, -0.1991519, 2.45609...  \n",
       "1765  [0.86933273, 1.3993437, -1.373814, -0.7129098,...  \n",
       "1766  [-1.3005122, -0.65894294, -0.8006101, 0.455483...  \n",
       "\n",
       "[1767 rows x 18 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### CONTEXTO SOCIAL DATASET 1 (POZZI)\n",
    "\n",
    "model_name='roberta-base'\n",
    "extra_dims=32\n",
    "\n",
    "tweets = pd.read_pickle(\"../data/OMC/final_omc_morality.pkl\")\n",
    "tweets['text']=tweets['text'].map(cleaner1)\n",
    "tweets['extra_data']=tweets['deepwalk']\n",
    "tweets['label']=tweets['moral_label2']\n",
    "\n",
    "\n",
    "print(tweets.label.unique())\n",
    "tweets.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 4, 3, 5, 1, 2])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#args\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = '/model/',\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size = 16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy = \"epoch\",\n",
    "    push_to_hub=False,\n",
    "    save_strategy='no',\n",
    "    seed=42)\n",
    "\n",
    "\n",
    "#data\n",
    "#df, id2label, label2id = binary_labels(tweets)\n",
    "\n",
    "#df, id2label, label2id = multi_labels(tweets)\n",
    "\n",
    "df, id2label, label2id = label_multiclass6(tweets)\n",
    "\n",
    "#label_5_rows = df[df['label'] == 5]\n",
    "#index_data = list(label_5_rows.index[0:2])\n",
    "#selected_rows = df.loc[index_data]\n",
    "#df = df.drop(index_data)\n",
    "#selected_rows\n",
    "df.label.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    299\n",
       "4     22\n",
       "1     18\n",
       "2      9\n",
       "3      5\n",
       "5      1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "#test_df = pd.concat([test_df, selected_rows]).reset_index(drop=True)\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42)\n",
    "datasets = {'train': Dataset.from_pandas(train_df), 'val': Dataset.from_pandas(val_df), 'test': Dataset.from_pandas(test_df)}\n",
    "datasets = DatasetDict(datasets)\n",
    "test_df.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'roberta-base'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name=\"roberta-base\"\n",
    "model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c8c44ef80d24d79b463fd8d10ed72ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1130 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1402a93871b2484dba0ecc6b0c99b982",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/283 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e017de5ea64544809ca8f541b339e3ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/354 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of CustomRobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='710' max='710' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [710/710 01:12, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.567150</td>\n",
       "      <td>0.865724</td>\n",
       "      <td>0.154672</td>\n",
       "      <td>0.144287</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.529777</td>\n",
       "      <td>0.865724</td>\n",
       "      <td>0.154672</td>\n",
       "      <td>0.144287</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.539804</td>\n",
       "      <td>0.840989</td>\n",
       "      <td>0.275146</td>\n",
       "      <td>0.278696</td>\n",
       "      <td>0.277675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.586189</td>\n",
       "      <td>0.844523</td>\n",
       "      <td>0.248262</td>\n",
       "      <td>0.335481</td>\n",
       "      <td>0.234941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.604537</td>\n",
       "      <td>0.851590</td>\n",
       "      <td>0.251147</td>\n",
       "      <td>0.294452</td>\n",
       "      <td>0.236302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.669093</td>\n",
       "      <td>0.844523</td>\n",
       "      <td>0.326771</td>\n",
       "      <td>0.448663</td>\n",
       "      <td>0.327451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.702249</td>\n",
       "      <td>0.826855</td>\n",
       "      <td>0.360269</td>\n",
       "      <td>0.404391</td>\n",
       "      <td>0.365036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.446800</td>\n",
       "      <td>0.717164</td>\n",
       "      <td>0.823322</td>\n",
       "      <td>0.285883</td>\n",
       "      <td>0.305792</td>\n",
       "      <td>0.279955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.446800</td>\n",
       "      <td>0.736695</td>\n",
       "      <td>0.837456</td>\n",
       "      <td>0.383174</td>\n",
       "      <td>0.376711</td>\n",
       "      <td>0.392911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.446800</td>\n",
       "      <td>0.716722</td>\n",
       "      <td>0.844523</td>\n",
       "      <td>0.385614</td>\n",
       "      <td>0.379123</td>\n",
       "      <td>0.394272</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load configuration\n",
    "if model_name == \"bert-base-uncased\":\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased', truncation=True)\n",
    "    tokenized_datasets = datasets.map(tokenize_function, batched=True)\n",
    "\n",
    "    config = AutoConfig.from_pretrained('bert-base-uncased', \n",
    "        num_labels=2, \n",
    "        id2label=id2label, \n",
    "        label2id=label2id\n",
    "    )\n",
    "    \n",
    "    new_model = CustomBertSequenceClassification.from_pretrained(\n",
    "        'bert-base-uncased',  \n",
    "        config=config,\n",
    "        num_extra_dims=extra_dims)\n",
    "\n",
    "\n",
    "elif model_name == \"roberta-base\":\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"FacebookAI/roberta-base\", truncation=True)\n",
    "    tokenized_datasets = datasets.map(tokenize_function, batched=True)\n",
    "\n",
    "    config = AutoConfig.from_pretrained(\n",
    "        \"FacebookAI/roberta-base\",\n",
    "        num_labels=6,\n",
    "        id2label=id2label,\n",
    "        label2id=label2id\n",
    "    )\n",
    "\n",
    "    new_model = CustomRobertaForSequenceClassification.from_pretrained(\n",
    "        \"FacebookAI/roberta-base\",\n",
    "        config=config,\n",
    "        num_extra_dims=extra_dims\n",
    "    )\n",
    "trainer = Trainer(\n",
    "    model=new_model,           \n",
    "    args=training_args,              \n",
    "    train_dataset=tokenized_datasets['train'],  \n",
    "    eval_dataset=tokenized_datasets['val'],     \n",
    "    tokenizer=tokenizer,             \n",
    "    compute_metrics=compute_metrics  \n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "#pred\n",
    "predictions = trainer.predict(tokenized_datasets['test'])\n",
    "predicted_class_ids = predictions.predictions.argmax(axis=1)\n",
    "actual_labels = tokenized_datasets['test']['label']\n",
    "results = classification_report(actual_labels, predicted_class_ids, digits=5, output_dict=True)\n",
    "\n",
    "results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete. Results for experiment baseline added to ../data/MIND/F1_results/bert-base-sentiment2\n"
     ]
    }
   ],
   "source": [
    "#save results\n",
    "import json\n",
    "results_file = '../data/MIND/F1_results/bert-base-sentiment2'\n",
    "experiment = \"baseline\"\n",
    "dataset_name= 'pozzi'\n",
    "\n",
    "if os.path.exists(results_file):\n",
    "    with open(results_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "else:\n",
    "    data = {}\n",
    "\n",
    "new_experiment = {\n",
    "    'experiment': experiment,\n",
    "    'dataset': dataset_name,\n",
    "    'results': results\n",
    "}\n",
    "\n",
    "if 'experiments' not in data:\n",
    "    data['experiments'] = []\n",
    "\n",
    "data['experiments'].append(new_experiment)\n",
    "\n",
    "with open(results_file, 'w') as f:\n",
    "    json.dump(data, f, indent=2)\n",
    "\n",
    "print(f\"Training complete. Results for experiment {experiment} added to {results_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save results\n",
    "\n",
    "results_file = '../data/OMC/F1_results/roberta-base-moral-polarity'\n",
    "experiment='tadw'\n",
    "\n",
    "with open(results_file, \"a\") as f:\n",
    "    f.write(f\"\\nExperimento: {experiment}\\n\")\n",
    "    f.write(f\"\\nDataset OMC: 1\\n\")\n",
    "    f.write(json.dumps(results, indent=2))\n",
    "\n",
    "print(\"Training complete. Results saved in\", results_file)\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Llama Fine Tuning (TEXTO + CS)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import peft\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    TrainerCallback, TrainerState, TrainerControl, \n",
    "    AutoModelForSequenceClassification,\n",
    "    LlamaForSequenceClassification,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "import torch\n",
    "from peft import LoraConfig, TaskType, PeftModel\n",
    "from transformers.trainer_utils import PREFIX_CHECKPOINT_DIR\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "assert torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nonmoral' 'authority' 'loyalty' 'purity' 'care' 'fairness']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "label\n",
       "nonmoral     1499\n",
       "authority      97\n",
       "care           69\n",
       "fairness       52\n",
       "loyalty        39\n",
       "purity         11\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#CONTEXTO SOCIAL DATASET1 (POZZI)\n",
    "model_name = \"meta-llama/Llama-3.2-1B\"\n",
    "extra_dims=80\n",
    "\n",
    "tweets = pd.read_pickle(\"../data/OMC/final_omc_morality.pkl\")\n",
    "tweets['text']=tweets['text'].map(cleaner1)\n",
    "tweets['extra_data']=tweets['tadw']\n",
    "tweets['label']=tweets['moral_label2']\n",
    "\n",
    "\n",
    "print(tweets.label.unique())\n",
    "tweets.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    299\n",
       "4     22\n",
       "1     18\n",
       "2      9\n",
       "3      5\n",
       "5      1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#args\n",
    "truncation = True\n",
    "max_length = 2000\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = \"out/\",\n",
    "    learning_rate=2e-4,\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size = 8,\n",
    "    per_device_eval_batch_size = 8,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy = \"epoch\",\n",
    "    push_to_hub=False,\n",
    "    save_strategy='epoch',\n",
    "    #save_safetensors=True,\n",
    "    #load_best_model_at_end = True,\n",
    "    #report_to=\"none\",\n",
    ")\n",
    "\n",
    "#data\n",
    "#df, id2label, label2id = binary_labels(tweets)\n",
    "df, id2label, label2id= label_multiclass6(tweets)\n",
    "\n",
    "#df, id2label, label2id = multi_labels(tweets)\n",
    "#label_5_rows = df[df['label'] == 5]\n",
    "#index_data = list(label_5_rows.index[0:2])\n",
    "#selected_rows = df.loc[index_data]\n",
    "#df = df.drop(index_data)\n",
    "#selected_rows\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42)\n",
    "#test_df = pd.concat([test_df, selected_rows]).reset_index(drop=True)\n",
    "\n",
    "datasets = {'train': Dataset.from_pandas(train_df), 'val': Dataset.from_pandas(val_df), 'test': Dataset.from_pandas(test_df)}\n",
    "datasets = DatasetDict(datasets)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "#selected_rows\n",
    "test_df.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of CustomLlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Model Name\n",
    "model_name = \"meta-llama/Llama-3.2-1B\"\n",
    "# Quantization Config\n",
    "quantization_config = BitsAndBytesConfig(load_in_8bit=True, bnb_4bit_compute_dtype=torch.float16)\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    r=64,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_CLS\",\n",
    ")\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = \"[PAD]\"\n",
    "\n",
    "config = AutoConfig.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=6,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "new_model = CustomLlamaForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    config=config,\n",
    "    quantization_config=quantization_config,\n",
    "    low_cpu_mem_usage=True,\n",
    "    num_extra_dims=extra_dims,  \n",
    ")\n",
    "new_model.config.pad_token_id = new_model.config.eos_token_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0875d6f53f38478382915553c6c21096",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1130 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d4dc1ccf2284ddb8ac0ffa490ee86a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/283 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0a6c3d89d5d4c3e98a56a81145eebbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/354 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1420' max='1420' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1420/1420 08:18, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.632324</td>\n",
       "      <td>0.865724</td>\n",
       "      <td>0.154672</td>\n",
       "      <td>0.144287</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.621094</td>\n",
       "      <td>0.865724</td>\n",
       "      <td>0.154672</td>\n",
       "      <td>0.144287</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.589355</td>\n",
       "      <td>0.865724</td>\n",
       "      <td>0.154672</td>\n",
       "      <td>0.144287</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.680400</td>\n",
       "      <td>0.551758</td>\n",
       "      <td>0.865724</td>\n",
       "      <td>0.182404</td>\n",
       "      <td>0.311388</td>\n",
       "      <td>0.181138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.680400</td>\n",
       "      <td>0.618164</td>\n",
       "      <td>0.855124</td>\n",
       "      <td>0.179242</td>\n",
       "      <td>0.227818</td>\n",
       "      <td>0.179097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.680400</td>\n",
       "      <td>0.576172</td>\n",
       "      <td>0.816254</td>\n",
       "      <td>0.250366</td>\n",
       "      <td>0.306909</td>\n",
       "      <td>0.235181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.680400</td>\n",
       "      <td>0.607910</td>\n",
       "      <td>0.862191</td>\n",
       "      <td>0.202153</td>\n",
       "      <td>0.256719</td>\n",
       "      <td>0.194929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.349800</td>\n",
       "      <td>0.627441</td>\n",
       "      <td>0.855124</td>\n",
       "      <td>0.202006</td>\n",
       "      <td>0.270985</td>\n",
       "      <td>0.193568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.349800</td>\n",
       "      <td>0.657715</td>\n",
       "      <td>0.848057</td>\n",
       "      <td>0.199481</td>\n",
       "      <td>0.243056</td>\n",
       "      <td>0.192208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.349800</td>\n",
       "      <td>0.683594</td>\n",
       "      <td>0.851590</td>\n",
       "      <td>0.219587</td>\n",
       "      <td>0.280754</td>\n",
       "      <td>0.207359</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#train\n",
    "tokenizer.pad_token_id = new_model.config.pad_token_id\n",
    "tokenized_datasets = datasets.map(tokenize_function, batched=True)\n",
    "new_model.add_adapter(peft_config, adapter_name=\"adapter_1\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=new_model,           \n",
    "    args=training_args,              \n",
    "    train_dataset=tokenized_datasets['train'],  \n",
    "    eval_dataset=tokenized_datasets['val'],     \n",
    "    tokenizer=tokenizer,             \n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pred\n",
    "predictions = trainer.predict(tokenized_datasets['test'])\n",
    "predicted_class_ids = predictions.predictions.argmax(axis=1)\n",
    "actual_labels = tokenized_datasets['test']['label']\n",
    "results = classification_report(actual_labels, predicted_class_ids, digits=5, output_dict=True)\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save results\n",
    "import json\n",
    "results_file = '../data/MIND/F1_results/llama-3.2-1b-morality'\n",
    "experiment = \"baseline\"\n",
    "dataset_name= 'hcr'\n",
    "\n",
    "if os.path.exists(results_file):\n",
    "    with open(results_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "else:\n",
    "    data = {}\n",
    "\n",
    "new_experiment = {\n",
    "    'experiment': experiment,\n",
    "    'dataset': dataset_name,\n",
    "    'results': results\n",
    "}\n",
    "\n",
    "if 'experiments' not in data:\n",
    "    data['experiments'] = []\n",
    "\n",
    "# Añadimos el nuevo experimento\n",
    "data['experiments'].append(new_experiment)\n",
    "\n",
    "with open(results_file, 'w') as f:\n",
    "    json.dump(data, f, indent=2)\n",
    "\n",
    "print(f\"Training complete. Results for experiment {experiment} added to {results_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save results\n",
    "\n",
    "import json\n",
    "results_file = '../data/HCR/F1_results/llama-3.2-1b-moral'\n",
    "experiment= \"baseline\"\n",
    "\n",
    "with open(results_file, \"a\") as f:\n",
    "    f.write(f\"\\nExperimento: {experiment}\\n\")\n",
    "    f.write(f\"\\nDataset HCR: \\n\")\n",
    "    f.write(json.dumps(results, indent=2))\n",
    "\n",
    "print(\"Training complete. Results saved in\", results_file)\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeepSeek Fine Tunning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import peft\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    TrainerCallback, TrainerState, TrainerControl, \n",
    "    AutoModelForSequenceClassification,\n",
    "    LlamaForSequenceClassification,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "import torch\n",
    "from peft import LoraConfig, TaskType, PeftModel\n",
    "from transformers.trainer_utils import PREFIX_CHECKPOINT_DIR\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "assert torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qwen2\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig\n",
    "\n",
    "config = AutoConfig.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\")\n",
    "print(config.model_type)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CONTEXTO SOCIAL DATASET1 (POZZI)\n",
    "model_name= \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "extra_dims=0\n",
    "tweets = pd.read_pickle(\"../data/MIND/final_omc_morality.pkl\")\n",
    "tweets['text']=tweets['text'].map(cleaner1)\n",
    "tweets['extra_data']=tweets['tadw']\n",
    "tweets['label']=tweets['moral_label']\n",
    "\n",
    "\n",
    "print(tweets.label.unique())\n",
    "tweets.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#args\n",
    "truncation = True\n",
    "max_length = 2000\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = \"out/\",\n",
    "    learning_rate=2e-4,\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size = 8,\n",
    "    per_device_eval_batch_size = 8,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy = \"epoch\",\n",
    "    push_to_hub=False,\n",
    "    save_strategy='epoch',\n",
    "    #save_safetensors=True,\n",
    "    #load_best_model_at_end = True,\n",
    "    #report_to=\"none\",\n",
    ")\n",
    "\n",
    "#data\n",
    "#df, id2label, label2id = binary_labels(tweets)\n",
    "\n",
    "df, id2label, label2id= label_multiclass6(tweets)\n",
    "\n",
    "#df, id2label, label2id = multi_labels(tweets)\n",
    "#label_5_rows = df[df['label'] == 5]\n",
    "#index_data = list(label_5_rows.index[0:2])\n",
    "#selected_rows = df.loc[index_data]\n",
    "#df = df.drop(index_data)\n",
    "#selected_rows\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42)\n",
    "#test_df = pd.concat([test_df, selected_rows]).reset_index(drop=True)\n",
    "\n",
    "datasets = {'train': Dataset.from_pandas(train_df), 'val': Dataset.from_pandas(val_df), 'test': Dataset.from_pandas(test_df)}\n",
    "datasets = DatasetDict(datasets)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "#selected_rows\n",
    "test_df.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Name\n",
    "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "# Quantization Config\n",
    "quantization_config = BitsAndBytesConfig(load_in_8bit=True, bnb_4bit_compute_dtype=torch.float16)\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    r=64,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_CLS\",\n",
    ")\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = \"[PAD]\"\n",
    "\n",
    "config = AutoConfig.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=6,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "new_model = CustomQwen2ForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    config=config,\n",
    "    quantization_config=quantization_config,\n",
    "    low_cpu_mem_usage=True,\n",
    "    num_extra_dims=extra_dims,  \n",
    ")\n",
    "new_model.config.pad_token_id = new_model.config.eos_token_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train\n",
    "tokenizer.pad_token_id = new_model.config.pad_token_id\n",
    "tokenized_datasets = datasets.map(tokenize_function, batched=True)\n",
    "new_model.add_adapter(peft_config, adapter_name=\"adapter_1\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=new_model,           \n",
    "    args=training_args,              \n",
    "    train_dataset=tokenized_datasets['train'],  \n",
    "    eval_dataset=tokenized_datasets['val'],     \n",
    "    tokenizer=tokenizer,             \n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pred\n",
    "predictions = trainer.predict(tokenized_datasets['test'])\n",
    "predicted_class_ids = predictions.predictions.argmax(axis=1)\n",
    "actual_labels = tokenized_datasets['test']['label']\n",
    "results = classification_report(actual_labels, predicted_class_ids, digits=5, output_dict=True)\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save results\n",
    "import json\n",
    "results_file = '../data/OMC/F1_results/DeepSeek-Qwen-morality'\n",
    "experiment= \"tadw\"\n",
    "\n",
    "with open(results_file, \"a\") as f:\n",
    "    f.write(f\"\\nExperimento: {experiment}\\n\")\n",
    "    f.write(f\"\\nDataset OMC: \\n\")\n",
    "    f.write(json.dumps(results, indent=2))\n",
    "\n",
    "print(\"Training complete. Results saved in\", results_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
