{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81ca94dd-dc3b-494e-95c8-051f32803af4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import warnings\n",
    "import torch\n",
    "import random\n",
    "import optuna\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import networkx as nx\n",
    "from sklearn.metrics import f1_score\n",
    "from datasets import Dataset, DatasetDict\n",
    "from typing import Optional, Union, Tuple\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "from transformers import set_seed\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from transformers import Trainer, TrainingArguments, AutoTokenizer, AutoConfig, AutoModel, BertPreTrainedModel, RobertaPreTrainedModel, RobertaModel, BertModel,AutoModelForSequenceClassification, BertForSequenceClassification, RobertaForSequenceClassification\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import torch\n",
    "\n",
    "torch.cuda.empty_cache()  # Libera memoria en caché\n",
    "torch.cuda.ipc_collect()  # Recoge memoria inaccesible\n",
    "# Clear memory in Python\n",
    "import gc\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee81ea0c-d6e8-4e7c-a24e-eee38660cd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import peft\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    # https://github.com/huggingface/peft/issues/96#issuecomment-1460080427\n",
    "    TrainerCallback, TrainerState, TrainerControl, \n",
    "    AutoModelForSequenceClassification,\n",
    "    LlamaForSequenceClassification,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "import torch\n",
    "from peft import LoraConfig, TaskType, PeftModel\n",
    "from transformers.trainer_utils import PREFIX_CHECKPOINT_DIR\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "assert torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ffb569e-c086-45f6-a2f0-1b8ac568f4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=42\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "torch.cuda.manual_seed_all(42)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e87f4d6-cf30-49a5-922a-6d742f0d53ee",
   "metadata": {},
   "source": [
    "Este notebook contiene:\n",
    "- la implementación de optuna para buscar los hiperparámetros del clasificador\n",
    "- optuna usa optimizacion bayesiana (usa un modelo probabilistico para buscar valores que tengan mas probabilidad de mejorar el modelo basándose en resultados de haber aplicado valores anteriores)\n",
    "- Usa el método de optimización bayesiana TPE (Tree-structured Parzen Estimator),  que divide experimentos previos en grupos que dan buenos y malos resultados, no modela función de búsqueda, crea dos distribuciones de probabilidad y busca en la región donde haya mas probabilidad de mejorar el rendimiento del modelo\n",
    "- optuna equilibra exploración/explotación"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4cd8e0-2679-420e-ae68-b5abc9a1ce9f",
   "metadata": {},
   "source": [
    "Conceptos:\n",
    "- trial: es un intento de optuna por encontrar buena combinación de hiperparámetros\n",
    "- en cada trial genera combinación de hiperparámetros(n cadas, n neuronas, dropout), entrena y evalúa\n",
    "\n",
    "- experimento: conjunto de trials o intentos ejecutados durante la optimización (proceso completo de encontrar le mejor combinación de hiperparámetros)\n",
    "- en el experimento se gestioan los trials, se realiza el seguimiento de las evaluaciones y se selecciona la mejor combinación de hiperparámetros\n",
    "\n",
    "- 15 trials, combinaciones independientes\n",
    "- 10 experimentos, 10 mejores combinaciones de esas 15 trials en cada uno\n",
    "- al final de cada experimento se queda con el que se haya obtenido mayor f1 score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba9eadbb-f457-47d2-bf36-cfe8f7698340",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaner1(tweet):\n",
    "    # remove usernames\n",
    "    # tweet = re.sub(\"@[A-Za-z0-9]+\",\"\",tweet)\n",
    "    tweet = tweet.lower()\n",
    "    tweet = re.sub(\"^rt\", \"\", tweet)\n",
    "    tweet = re.sub(\"\\s[0-9]+\\s\", \"\", tweet)\n",
    "\n",
    "    # remove usernames\n",
    "    tweet = re.sub(\"@[^\\s]+\", \"\", tweet)\n",
    "    tweet = re.sub(\"at_user\", \"\", tweet)\n",
    "\n",
    "\n",
    "    # remove urls\n",
    "    tweet = re.sub(\"pic.twitter.com/[A-Za-z0-9]+\", \"\", tweet)\n",
    "    tweet = re.sub(r\"(?:\\@|http?\\://|https?\\://|www)\\S+\", \"\", tweet)\n",
    "    tweet = tweet.replace(\"url\", \"\")\n",
    "\n",
    "    tweet = tweet.strip()\n",
    "    tweet = \" \".join(tweet.split())\n",
    "\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa36a6a2-b812-4c31-b034-8ab96d1fdb45",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b494f0-b859-44f2-9eec-bcfab6e8beeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model names: \"meta-llama/Llama-3.2-1B\", \"bert-base-uncased\" , \"roberta-base\"\n",
    "\n",
    "model_name = \"roberta-base\"\n",
    "\n",
    "#BINARY LABELS\n",
    "def binary_labels(df):\n",
    "    df= df.replace({'label': {'neg': 0, 'pos': 1}})\n",
    "    id2label = {0: \"NEG\", 1: \"POS\"}\n",
    "    label2id = {\"NEG\": 0, \"POS\": 1}\n",
    "    return df, id2label, label2id\n",
    "\n",
    "# METRICS\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='macro')\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall}\n",
    "\n",
    "# TOKENIZER\n",
    "def tokenize_function(examples):\n",
    "    tokenized_inputs=tokenizer(examples[\"text\"], truncation=True)\n",
    "    return tokenized_inputs\n",
    "\n",
    "\n",
    "#https://discuss.huggingface.co/t/combine-bertforsequenceclassificaion-with-additional-features/6523/2\n",
    "#MODELS\n",
    "\n",
    "\n",
    "#dimensión del contexto social\n",
    "\n",
    "tweets = pd.read_pickle('../models/svd_df.pkl')\n",
    "#tweets\n",
    "#tweets = pd.read_pickle('../models/deepwalk_df.pkl')\n",
    "#tweets\n",
    "#tweets = pd.read_pickle('../models/node2vec_df.pkl')\n",
    "#tweets\n",
    "#tweets = pd.read_pickle('../models/tadw_df.pkl')\n",
    "\n",
    "tweets['text']=tweets['text'].map(cleaner1)\n",
    "extra_dims = 32\n",
    "\n",
    "tweets.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b56828e-1562-43de-9d48-1e5d14d66fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n",
    "\n",
    "class CustomBertSequenceClassification(BertPreTrainedModel):\n",
    "    def __init__(self, config, num_extra_dims,trial):\n",
    "        # Initialize the base model (BERT)\n",
    "        super().__init__(config)\n",
    "        self.config = config\n",
    "        total_dims = config.hidden_size + num_extra_dims \n",
    "        self.bert = BertModel(config)\n",
    "        \n",
    "        #classifier\n",
    "        self.num_labels = config.num_labels\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        #self.classifier = nn.Linear(total_dims, config.num_labels)\n",
    "        self.config = config\n",
    "        self.num_extra_dims = num_extra_dims\n",
    "\n",
    "        layers = []\n",
    "        #numero de capas que se van a definir, optuna elige entre 1 y 3\n",
    "        num_layers = trial.suggest_int(\"num_layers\", 1, 3) \n",
    "        in_features = total_dims\n",
    "        print(in_features)\n",
    "        for i in range(num_layers):\n",
    "            #numero de neuronas que se van a definir en cada capa, elige un número entre 32 y 256\n",
    "            #log true permite que buscar tener más probabilidad de elegir valores pequeños \n",
    "            out_features = trial.suggest_int(f\"n_units_l{i}\", 32, 256, log=True) if trial else 128\n",
    "            #print(out_features)\n",
    "            layers.append(nn.Linear(in_features, out_features))\n",
    "            # valor de droppout elegido, elegirá valor decimal entre 0.1 y 0.5\n",
    "            layers.append(nn.Dropout(trial.suggest_float(f\"dropout_l{i}\", 0.1, 0.5) if trial else 0.2))\n",
    "            in_features = out_features\n",
    "            #print(in_features, out_features)\n",
    "        \n",
    "        layers.append(nn.Linear(in_features, config.num_labels))  \n",
    "        self.classifier = nn.Sequential(*layers)  \n",
    "        self.post_init()\n",
    "    \n",
    "\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.LongTensor] = None,\n",
    "        attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        extra_data: Optional[torch.FloatTensor] = None,\n",
    "        token_type_ids: Optional[torch.LongTensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        head_mask: Optional[torch.FloatTensor] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple, SequenceClassifierOutput]:\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        \n",
    "        # Get output from the BERT model (transformer)\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        \n",
    "        sequence_output = outputs.pooler_output\n",
    "        sequence_output = self.dropout(sequence_output)\n",
    "     \n",
    "        if extra_data is not None:\n",
    "            output = torch.cat((sequence_output, extra_data), dim=-1)\n",
    "        else:\n",
    "            output = sequence_output\n",
    "\n",
    "        logits = self.classifier(output)\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            if self.config.problem_type is None:\n",
    "                if self.num_labels == 1:\n",
    "                    self.config.problem_type = \"regression\"\n",
    "                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n",
    "                    self.config.problem_type = \"single_label_classification\"\n",
    "                else:\n",
    "                    self.config.problem_type = \"multi_label_classification\"\n",
    "\n",
    "            if self.config.problem_type == \"regression\":\n",
    "                loss_fct = MSELoss()\n",
    "                if self.num_labels == 1:\n",
    "                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n",
    "                else:\n",
    "                    loss = loss_fct(logits, labels)\n",
    "            elif self.config.problem_type == \"single_label_classification\":\n",
    "                loss_fct = CrossEntropyLoss()\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            elif self.config.problem_type == \"multi_label_classification\":\n",
    "                loss_fct = BCEWithLogitsLoss()\n",
    "                loss = loss_fct(logits, labels)\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )\n",
    "        \n",
    "    @classmethod\n",
    "    def from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs):\n",
    "        \"\"\"\n",
    "        Override `from_pretrained` to handle `num_extra_dims` when loading a pre-trained model.\n",
    "        \"\"\"\n",
    "        num_extra_dims = kwargs.pop(\"num_extra_dims\", 0)  # Extract num_extra_dims\n",
    "        config = kwargs.pop(\"config\", None)\n",
    "\n",
    "        if config is None:\n",
    "            config = AutoConfig.from_pretrained(pretrained_model_name_or_path, **kwargs)\n",
    "\n",
    "        config.num_extra_dims = num_extra_dims  # Inject extra_dims into config\n",
    "\n",
    "        return super().from_pretrained(\n",
    "            pretrained_model_name_or_path,\n",
    "            *model_args,\n",
    "            config=config,\n",
    "            num_extra_dims=num_extra_dims,  # Pass it explicitly\n",
    "            **kwargs\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c78186-74f8-45ec-ab73-c6b3d81ba0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n",
    "\n",
    "class RobertaClassificationHead(nn.Module):\n",
    "    \"\"\"Head for sentence-level classification tasks.\"\"\"\n",
    "\n",
    "    def __init__(self, config, num_extra_dims,trial):\n",
    "        super().__init__()\n",
    "        total_dims = config.hidden_size + num_extra_dims  # Correctly add extra dimensions\n",
    "        classifier_dropout = (config.classifier_dropout if config.classifier_dropout is not None else config.hidden_dropout_prob)\n",
    "        self.dense = nn.Linear(total_dims, total_dims)\n",
    "        self.dropout = nn.Dropout(classifier_dropout)\n",
    "\n",
    "\n",
    "        layers = []\n",
    "        #numero de capas que se van a definir, optuna elige entre 1 y 3\n",
    "        num_layers = trial.suggest_int(\"num_layers\", 1, 3) \n",
    "        in_features = total_dims\n",
    "        print(in_features)\n",
    "        for i in range(num_layers):\n",
    "            #numero de neuronas que se van a definir en cada capa, elige un número entre 32 y 256\n",
    "            #log true permite que buscar tener más probabilidad de elegir valores pequeños \n",
    "            out_features = trial.suggest_int(f\"n_units_l{i}\", 32, 256, log=True) if trial else 128\n",
    "            #print(out_features)\n",
    "            layers.append(nn.Linear(in_features, out_features))\n",
    "            # valor de droppout elegido, elegirá valor decimal entre 0.1 y 0.5\n",
    "            layers.append(nn.ReLU())  # Se agregó activación ReLU para mejorar aprendizaje\n",
    "\n",
    "            layers.append(nn.Dropout(trial.suggest_float(f\"dropout_l{i}\", 0.1, 0.5) if trial else 0.2))\n",
    "            in_features = out_features\n",
    "            #print(in_features, out_features)\n",
    "        \n",
    "        layers.append(nn.Linear(in_features, config.num_labels))  \n",
    "        #self.classifier = nn.Sequential(*layers)  \n",
    "\n",
    "        \n",
    "        #classifier_dropout = (config.classifier_dropout if config.classifier_dropout is not None else config.hidden_dropout_prob)\n",
    "        #self.dropout = nn.Dropout(classifier_dropout)\n",
    "        self.out_proj = nn.Sequential(*layers) # Match total_dims\n",
    "\n",
    "\n",
    "    def forward(self, features, **kwargs):\n",
    "        x = self.dropout(features)\n",
    "        x = self.dense(x)\n",
    "        x = torch.tanh(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.out_proj(x)\n",
    "        return x\n",
    "\n",
    "class CustomRobertaForSequenceClassification(RobertaPreTrainedModel):\n",
    "    def __init__(self, config, num_extra_dims,trial):\n",
    "        super().__init__(config)\n",
    "        self.config = config\n",
    "        self.num_extra_dims = num_extra_dims \n",
    "        total_dims = config.hidden_size + num_extra_dims \n",
    "        \n",
    "        # classifier \n",
    "        self.num_labels = config.num_labels\n",
    "\n",
    "        self.roberta = RobertaModel(config, add_pooling_layer=False)\n",
    "        self.classifier = RobertaClassificationHead(config, num_extra_dims,trial)\n",
    "\n",
    "        self.post_init()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.LongTensor] = None,\n",
    "        attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        extra_data: Optional[torch.FloatTensor] = None,  \n",
    "        token_type_ids: Optional[torch.LongTensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        head_mask: Optional[torch.FloatTensor] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple[torch.Tensor], SequenceClassifierOutput]:\n",
    "        r\"\"\"\n",
    "        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n",
    "            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n",
    "            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n",
    "            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        outputs = self.roberta(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        sequence_output = outputs[0]\n",
    "        cls_output = sequence_output[:, 0, :] \n",
    "\n",
    "        if extra_data is not None:\n",
    "            cls_output = torch.cat((cls_output, extra_data), dim=-1)\n",
    "\n",
    "        logits = self.classifier(cls_output)\n",
    "            \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # move labels to correct device to enable model parallelism\n",
    "            labels = labels.to(logits.device)\n",
    "            if self.config.problem_type is None:\n",
    "                if self.num_labels == 1:\n",
    "                    self.config.problem_type = \"regression\"\n",
    "                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n",
    "                    self.config.problem_type = \"single_label_classification\"\n",
    "                else:\n",
    "                    self.config.problem_type = \"multi_label_classification\"\n",
    "\n",
    "            if self.config.problem_type == \"regression\":\n",
    "                loss_fct = MSELoss()\n",
    "                if self.num_labels == 1:\n",
    "                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n",
    "                else:\n",
    "                    loss = loss_fct(logits, labels)\n",
    "            elif self.config.problem_type == \"single_label_classification\":\n",
    "                loss_fct = CrossEntropyLoss()\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            elif self.config.problem_type == \"multi_label_classification\":\n",
    "                loss_fct = BCEWithLogitsLoss()\n",
    "                loss = loss_fct(logits, labels)\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs):\n",
    "        \"\"\"\n",
    "        Override `from_pretrained` to handle `num_extra_dims` when loading a pre-trained model.\n",
    "        \"\"\"\n",
    "        num_extra_dims = kwargs.pop(\"num_extra_dims\", 0)  # Extract num_extra_dims\n",
    "        config = kwargs.pop(\"config\", None)\n",
    "\n",
    "        if config is None:\n",
    "            config = AutoConfig.from_pretrained(pretrained_model_name_or_path, **kwargs)\n",
    "\n",
    "        config.num_extra_dims = num_extra_dims  # Inject extra_dims into config\n",
    "\n",
    "        return super().from_pretrained(\n",
    "            pretrained_model_name_or_path,\n",
    "            *model_args,\n",
    "            config=config,\n",
    "            num_extra_dims=num_extra_dims,  # Pass it explicitly\n",
    "            **kwargs\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0ee2ef-87ee-4835-9ccb-238872dd359d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Optional, Tuple, List, Union\n",
    "from transformers import LlamaPreTrainedModel, LlamaModel, AutoConfig\n",
    "from transformers.modeling_outputs import SequenceClassifierOutputWithPast\n",
    "from transformers.utils import logging\n",
    "\n",
    "class CustomLlamaForSequenceClassification(LlamaPreTrainedModel):\n",
    "    def __init__(self, config, num_extra_dims,trial):\n",
    "        super().__init__(config)\n",
    "        self.config = config\n",
    "        self.num_labels = 2\n",
    "        self.num_extra_dims = num_extra_dims\n",
    "        total_dims = config.hidden_size + num_extra_dims\n",
    "        self.config.pad_token_id = self.config.eos_token_id  # Asegurar que el padding sea el token de fin de secuencia\n",
    "\n",
    "        self.model = LlamaModel(config)\n",
    "\n",
    "        # Construcción de la capa de clasificación con Optuna\n",
    "        layers = []\n",
    "        num_layers = trial.suggest_int(\"num_layers\", 1, 3) if trial else 2  # Default: 2 capas si no hay trial\n",
    "        in_features = total_dims\n",
    "\n",
    "        for i in range(num_layers):\n",
    "            out_features = trial.suggest_int(f\"n_units_l{i}\", 32, 256, log=True) if trial else 128\n",
    "            layers.append(nn.Linear(in_features, out_features))\n",
    "            #layers.append(nn.ReLU())  # Se agregó activación ReLU para mejorar aprendizaje\n",
    "            layers.append(nn.Dropout(trial.suggest_float(f\"dropout_l{i}\", 0.1, 0.5) if trial else 0.2))\n",
    "            in_features = out_features\n",
    "\n",
    "        layers.append(nn.Linear(in_features, self.num_labels))\n",
    "        self.score = nn.Sequential(*layers)\n",
    "\n",
    "        self.post_init()\n",
    "\n",
    "    def get_input_embeddings(self):\n",
    "        return self.model.embed_tokens\n",
    "\n",
    "    def set_input_embeddings(self, value):\n",
    "        self.model.embed_tokens = value\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.LongTensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        extra_data: Optional[torch.FloatTensor] = None,  \n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple, SequenceClassifierOutputWithPast]:\n",
    "        r\"\"\"\n",
    "        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n",
    "            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n",
    "            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n",
    "            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        transformer_outputs = self.model(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        \n",
    "        hidden_states = transformer_outputs[0]\n",
    "        if extra_data is not None:\n",
    "            extra_data = extra_data.unsqueeze(1)  # Reshape to (batch_size, 1, num_extra_dims)\n",
    "            extra_data = extra_data.expand(-1, hidden_states.shape[1], -1)  # Expand to (batch_size, sequence_length, num_extra_dims)\n",
    "            hidden_states = torch.cat((hidden_states, extra_data), dim=-1)\n",
    "            pooled_representation = hidden_states[:, -1, :]# Shape: (batch_size, sequence_length, hidden_size + num_extra_dims)\n",
    "            logits = self.score(pooled_representation.to(self.score.weight.dtype))\n",
    "            #print(f\"Logits shape: {logits.shape}\")\n",
    "\n",
    "        else:\n",
    "            logits = self.score(hidden_states)\n",
    "            print(f\"Logits shape: {logits.shape}\")\n",
    "\n",
    "        if input_ids is not None:\n",
    "            batch_size = input_ids.shape[0]\n",
    "        else:\n",
    "            batch_size = inputs_embeds.shape[0]\n",
    "        \n",
    "        if self.config.pad_token_id is None and batch_size != 1:\n",
    "            raise ValueError(\"Cannot handle batch sizes > 1 if no padding token is defined.\")\n",
    "        \n",
    "        # Determine the last non-pad token position (for padding handling)\n",
    "        if self.config.pad_token_id is None:\n",
    "            last_non_pad_token = -1\n",
    "        elif input_ids is not None:\n",
    "            non_pad_mask = (input_ids != self.config.pad_token_id).to(logits.device, torch.int32)\n",
    "            token_indices = torch.arange(input_ids.shape[-1], device=logits.device)\n",
    "            last_non_pad_token = (token_indices * non_pad_mask).argmax(-1)\n",
    "        else:\n",
    "            last_non_pad_token = -1\n",
    "        #print(f\"Last non-pad token indices: {last_non_pad_token}\")\n",
    "        \n",
    "        if extra_data is None:\n",
    "            pooled_logits = logits[torch.arange(batch_size, device=logits.device), last_non_pad_token]\n",
    "        else:\n",
    "            pooled_logits = logits    \n",
    "        #print(f\"Pooled logits shape before checking: {pooled_logits.shape}\")\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = self.loss_function(logits=logits, labels=labels, pooled_logits=pooled_logits, config=self.config)\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (pooled_logits,) + transformer_outputs[1:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return SequenceClassifierOutputWithPast(\n",
    "            loss=loss,\n",
    "            logits=pooled_logits,\n",
    "            past_key_values=transformer_outputs.past_key_values,\n",
    "            hidden_states=transformer_outputs.hidden_states,\n",
    "            attentions=transformer_outputs.attentions,\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs):\n",
    "        \"\"\"\n",
    "        Override `from_pretrained` to handle `num_extra_dims` when loading a pre-trained model.\n",
    "        \"\"\"\n",
    "        num_extra_dims = kwargs.pop(\"num_extra_dims\", 0)  # Extract num_extra_dims\n",
    "        config = kwargs.pop(\"config\", None)\n",
    "\n",
    "        if config is None:\n",
    "            config = AutoConfig.from_pretrained(pretrained_model_name_or_path, **kwargs)\n",
    "\n",
    "        config.num_extra_dims = num_extra_dims  # Inject extra_dims into config\n",
    "\n",
    "        return super().from_pretrained(\n",
    "            pretrained_model_name_or_path,\n",
    "            *model_args,\n",
    "            config=config,\n",
    "            num_extra_dims=num_extra_dims,  # Pass it explicitly\n",
    "            **kwargs\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d86a558-fb6a-4737-af54-4377adf4cad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data\n",
    "\n",
    "df, id2label, label2id = binary_labels(tweets)\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42)\n",
    "datasets = {'train': Dataset.from_pandas(train_df), 'val': Dataset.from_pandas(val_df), 'test': Dataset.from_pandas(test_df)}\n",
    "\n",
    "\n",
    "datasets = DatasetDict(datasets)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d85fd3-0ed5-4fee-a5a3-4dcb5f46cbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8436e78-4d9f-4ba4-9ae3-cbbe1678027d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#model names: \"meta-llama/Llama-3.2-1B\", \"bert-base-uncased\" , \"roberta-base\"\n",
    "\n",
    "def objective(trial):\n",
    "    if model_name == \"meta-llama/Llama-3.2-1B\":\n",
    "        truncation = True\n",
    "        max_length = 2000\n",
    "        tokenized_datasets = datasets.map(tokenize_function, batched=True)\n",
    "        \n",
    "\n",
    "        # Quantization Config\n",
    "        quantization_config = BitsAndBytesConfig(load_in_8bit=True, bnb_4bit_compute_dtype=torch.float16)\n",
    "        \n",
    "        peft_config = LoraConfig(\n",
    "            lora_alpha=16,\n",
    "            lora_dropout=0.1,\n",
    "            r=64,\n",
    "            bias=\"none\",\n",
    "            task_type=\"SEQ_CLS\",\n",
    "        )\n",
    "        \n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        tokenizer.pad_token = \"[PAD]\"\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        config = AutoConfig.from_pretrained(\n",
    "            model_name,\n",
    "            num_labels=2,\n",
    "            id2label=id2label,\n",
    "            label2id=label2id\n",
    "        )\n",
    "        \n",
    "        # Load Custom Model with Quantization\n",
    "        model = CustomLlamaForSequenceClassification.from_pretrained(\n",
    "            model_name,\n",
    "            config=config,\n",
    "            quantization_config=quantization_config,\n",
    "            low_cpu_mem_usage=True,\n",
    "            num_extra_dims=extra_dims,  trial=trial\n",
    "        ).to(torch.float32)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        training_args = TrainingArguments(\n",
    "            output_dir = \"out/\",\n",
    "            learning_rate=2e-4,\n",
    "            num_train_epochs=10,\n",
    "            per_device_train_batch_size = 1,\n",
    "            per_device_eval_batch_size = 1,\n",
    "            weight_decay=0.01,\n",
    "            eval_strategy = \"epoch\",\n",
    "            push_to_hub=False,\n",
    "            save_strategy='no',\n",
    "            #save_safetensors=True,\n",
    "            #load_best_model_at_end = True,\n",
    "            #report_to=\"none\",\n",
    "        )\n",
    "\n",
    "        \n",
    "    elif model_name == \"bert-base-uncased\":\n",
    "\n",
    "        tokenized_datasets = datasets.map(tokenize_function, batched=True)\n",
    "    \n",
    "        config = AutoConfig.from_pretrained('bert-base-uncased', num_labels=2, \n",
    "            id2label=id2label, \n",
    "            label2id=label2id\n",
    "        )\n",
    "        \n",
    "        model = CustomBertSequenceClassification.from_pretrained(\n",
    "            'bert-base-uncased',  \n",
    "            config=config,\n",
    "            num_extra_dims=extra_dims, trial=trial)\n",
    "    \n",
    "        training_args = TrainingArguments(\n",
    "            output_dir = '/model/',\n",
    "            learning_rate=2e-5,\n",
    "            num_train_epochs=10,\n",
    "            per_device_train_batch_size = 16,\n",
    "            per_device_eval_batch_size=16,\n",
    "            weight_decay=0.01,\n",
    "            eval_strategy = \"epoch\",\n",
    "            push_to_hub=False,\n",
    "            save_strategy='no',\n",
    "            seed=42)\n",
    "\n",
    "    \n",
    "        \n",
    "    elif model_name == \"roberta-base\":\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"FacebookAI/roberta-base\")\n",
    "        tokenized_datasets = datasets.map(tokenize_function, batched=True)\n",
    "    \n",
    "        config = AutoConfig.from_pretrained(\n",
    "            \"FacebookAI/roberta-base\",\n",
    "            num_labels=2,\n",
    "            id2label=id2label,\n",
    "            label2id=label2id\n",
    "        )\n",
    "    \n",
    "        model = CustomRobertaForSequenceClassification.from_pretrained(\n",
    "            \"FacebookAI/roberta-base\",\n",
    "            config=config,\n",
    "            num_extra_dims=extra_dims, trial=trial\n",
    "        )\n",
    "                \n",
    "        training_args = TrainingArguments(\n",
    "            output_dir = '/model/',\n",
    "            learning_rate=2e-5,\n",
    "            num_train_epochs=10,\n",
    "            per_device_train_batch_size = 16,\n",
    "            per_device_eval_batch_size=16,\n",
    "            weight_decay=0.01,\n",
    "            eval_strategy = \"epoch\",\n",
    "            push_to_hub=False,\n",
    "            save_strategy='no',\n",
    "            seed=42)\n",
    "        \n",
    "\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,           \n",
    "        args=training_args,              \n",
    "        train_dataset=tokenized_datasets['train'],  \n",
    "        eval_dataset=tokenized_datasets['val'],     \n",
    "        tokenizer=tokenizer,             \n",
    "        compute_metrics=compute_metrics  \n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    #para evaluar el estudio se usan los datos de evaluación se usa el valor de f1 score\n",
    "    eval_result = trainer.evaluate()\n",
    "    return eval_result[\"eval_f1\"]  \n",
    "    \n",
    "# se indica que se quiere maximizar el f1 score\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=50)\n",
    "print(\"Best trial:\", study.best_trial)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27f7ae7-714e-4af7-9c37-f495d97bc3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = study.best_trial\n",
    "\n",
    "print(\"Mejores hiperparámetros encontrados:\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e26fc93-22cc-4631-aae5-c82ac96c68c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a9b188c7-90e7-41c5-9dac-3f1bb3965171",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38a14576-c980-41ac-88c8-6fac8ba136ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model names: \"meta-llama/Llama-3.2-1B\", \"bert-base-uncased\" , \"roberta-base\"\n",
    "model_name = \"roberta-base\"\n",
    "\n",
    "\n",
    "def binary_labels(df):\n",
    "    df= df.replace({'label': {'neg': 0, 'pos': 1}})\n",
    "    id2label = {0: \"NEG\", 1: \"POS\"}\n",
    "    label2id = {\"NEG\": 0, \"POS\": 1}\n",
    "    return df, id2label, label2id\n",
    "\n",
    "\n",
    "#tweets = pd.read_pickle('../models/svd_df.pkl')\n",
    "#tweets\n",
    "#tweets = pd.read_pickle('../models/deepwalk_df.pkl')\n",
    "#tweets\n",
    "#tweets = pd.read_pickle('../models/node2vec_df.pkl')\n",
    "#tweets\n",
    "tweets = pd.read_pickle('../models/tadw_df.pkl')\n",
    "tweets['text']=tweets['text'].map(cleaner1)\n",
    "extra_dims = 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd731f71-8830-4eab-84ae-e3b13766c947",
   "metadata": {},
   "outputs": [],
   "source": [
    "# METRICS\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='macro')\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall}\n",
    "\n",
    "# TOKENIZER\n",
    "def tokenize_function(examples):\n",
    "    tokenized_inputs=tokenizer(examples[\"text\"], truncation=True)\n",
    "    return tokenized_inputs\n",
    "\n",
    "\n",
    "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n",
    "\n",
    "class CustomBertSequenceClassification(BertPreTrainedModel):\n",
    "    def __init__(self, config, num_extra_dims):\n",
    "        # Initialize the base model (BERT)\n",
    "        super().__init__(config)\n",
    "        self.config = config\n",
    "        total_dims = config.hidden_size + num_extra_dims \n",
    "        self.bert = BertModel(config)\n",
    "        \n",
    "        #classifier\n",
    "        self.num_labels = config.num_labels\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        '''\n",
    "        # Aplicar los mejores hiperparámetros encontrados\n",
    "        self.num_layers = 1\n",
    "        self.dropout_prob0 = 0.4308857828688666\n",
    "        self.n_units_l0 = 221\n",
    "        \n",
    "\n",
    "        layers = []\n",
    "        in_features = total_dims\n",
    "        layers.append(nn.Linear(in_features, self.n_units_l0))\n",
    "        layers.append(nn.Dropout(self.dropout_prob0))\n",
    "        layers.append(nn.Linear(self.n_units_l0, config.num_labels))  \n",
    "        self.classifier = nn.Sequential(*layers)\n",
    "        self.post_init()\n",
    "\n",
    "        '''\n",
    "        # Aplicar los mejores hiperparámetros encontrados\n",
    "        self.num_layers = 3\n",
    "        self.dropout_prob0 = 0.23176856907840251\n",
    "        self.n_units_l0 = 54\n",
    "        self.dropout_prob1 = 0.18476776697400715\n",
    "        self.n_units_l1 = 85\n",
    "        self.dropout_prob2 = 0.382516054666877\n",
    "        self.n_units_l2 = 122\n",
    "\n",
    "        # Define layers\n",
    "        layers = []\n",
    "        in_features = total_dims\n",
    "        layers.append(nn.Linear(in_features, self.n_units_l0))\n",
    "        layers.append(nn.Dropout(self.dropout_prob0))\n",
    "        layers.append(nn.Linear(self.n_units_l0, self.n_units_l1)) \n",
    "        layers.append(nn.Dropout(self.dropout_prob1))\n",
    "        layers.append(nn.Linear(self.n_units_l1, config.num_labels))\n",
    "        #layers.append(nn.Dropout(self.dropout_prob2))\n",
    "        #layers.append(nn.Linear(self.n_units_l2, config.num_labels))\n",
    "        self.classifier = nn.Sequential(*layers)\n",
    "        self.post_init()\n",
    "        \n",
    "\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.LongTensor] = None,\n",
    "        attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        extra_data: Optional[torch.FloatTensor] = None,\n",
    "        token_type_ids: Optional[torch.LongTensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        head_mask: Optional[torch.FloatTensor] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple, SequenceClassifierOutput]:\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        \n",
    "        # Get output from the BERT model (transformer)\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        \n",
    "        sequence_output = outputs.pooler_output\n",
    "        sequence_output = self.dropout(sequence_output)\n",
    "     \n",
    "        if extra_data is not None:\n",
    "            output = torch.cat((sequence_output, extra_data), dim=-1)\n",
    "        else:\n",
    "            output = sequence_output\n",
    "\n",
    "        logits = self.classifier(output)\n",
    "\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            if self.config.problem_type is None:\n",
    "                if self.num_labels == 1:\n",
    "                    self.config.problem_type = \"regression\"\n",
    "                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n",
    "                    self.config.problem_type = \"single_label_classification\"\n",
    "                else:\n",
    "                    self.config.problem_type = \"multi_label_classification\"\n",
    "\n",
    "            if self.config.problem_type == \"regression\":\n",
    "                loss_fct = MSELoss()\n",
    "                if self.num_labels == 1:\n",
    "                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n",
    "                else:\n",
    "                    loss = loss_fct(logits, labels)\n",
    "            elif self.config.problem_type == \"single_label_classification\":\n",
    "                loss_fct = CrossEntropyLoss()\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            elif self.config.problem_type == \"multi_label_classification\":\n",
    "                loss_fct = BCEWithLogitsLoss()\n",
    "                loss = loss_fct(logits, labels)\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )\n",
    "        \n",
    "    @classmethod\n",
    "    def from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs):\n",
    "        \"\"\"\n",
    "        Override `from_pretrained` to handle `num_extra_dims` when loading a pre-trained model.\n",
    "        \"\"\"\n",
    "        num_extra_dims = kwargs.pop(\"num_extra_dims\", 0)  # Extract num_extra_dims\n",
    "        config = kwargs.pop(\"config\", None)\n",
    "\n",
    "        if config is None:\n",
    "            config = AutoConfig.from_pretrained(pretrained_model_name_or_path, **kwargs)\n",
    "\n",
    "        config.num_extra_dims = num_extra_dims  # Inject extra_dims into config\n",
    "\n",
    "        return super().from_pretrained(\n",
    "            pretrained_model_name_or_path,\n",
    "            *model_args,\n",
    "            config=config,\n",
    "            num_extra_dims=num_extra_dims,  # Pass it explicitly\n",
    "            **kwargs\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "927a1570-2ccb-40a7-b8be-a3c004e682e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n",
    "\n",
    "class RobertaClassificationHead(nn.Module):\n",
    "    \"\"\"Head for sentence-level classification tasks.\"\"\"\n",
    "\n",
    "    def __init__(self, config, num_extra_dims):\n",
    "        super().__init__()\n",
    "        total_dims = config.hidden_size + num_extra_dims  # Correctly add extra dimensions\n",
    "        classifier_dropout = (config.classifier_dropout if config.classifier_dropout is not None else config.hidden_dropout_prob)\n",
    "        self.dense = nn.Linear(total_dims, total_dims)\n",
    "        self.dropout = nn.Dropout(classifier_dropout)\n",
    "\n",
    "\n",
    "        self.num_layers = 3\n",
    "        self.dropout_prob0 = 0.2902443815942271\n",
    "        self.n_units_l0 = 203\n",
    "        self.dropout_prob1 = 0.21294012560852782\n",
    "        self.n_units_l1 = 134\n",
    "        self.dropout_prob2 = 0.2101716211711512\n",
    "        self.n_units_l2 = 239\n",
    "\n",
    "        # Define layers\n",
    "        layers = []\n",
    "        in_features = total_dims\n",
    "        layers.append(nn.Linear(in_features, self.n_units_l0))\n",
    "        layers.append(nn.Dropout(self.dropout_prob0))\n",
    "        layers.append(nn.Linear(self.n_units_l0, self.n_units_l1)) \n",
    "        layers.append(nn.Dropout(self.dropout_prob1))\n",
    "        layers.append(nn.Linear(self.n_units_l1, self.n_units_l2))\n",
    "        layers.append(nn.Dropout(self.dropout_prob2))\n",
    "        layers.append(nn.Linear(self.n_units_l2, config.num_labels))\n",
    "        #self.classifier = nn.Sequential(*layers)  \n",
    "\n",
    "        \n",
    "        #classifier_dropout = (config.classifier_dropout if config.classifier_dropout is not None else config.hidden_dropout_prob)\n",
    "        #self.dropout = nn.Dropout(classifier_dropout)\n",
    "        self.out_proj = nn.Sequential(*layers) # Match total_dims\n",
    "\n",
    "\n",
    "    def forward(self, features, **kwargs):\n",
    "        x = self.dropout(features)\n",
    "        x = self.dense(x)\n",
    "        x = torch.tanh(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.out_proj(x)\n",
    "        return x\n",
    "\n",
    "class CustomRobertaForSequenceClassification(RobertaPreTrainedModel):\n",
    "    def __init__(self, config, num_extra_dims):\n",
    "        super().__init__(config)\n",
    "        self.config = config\n",
    "        self.num_extra_dims = num_extra_dims \n",
    "        total_dims = config.hidden_size + num_extra_dims \n",
    "        \n",
    "        # classifier \n",
    "        self.num_labels = config.num_labels\n",
    "\n",
    "        self.roberta = RobertaModel(config, add_pooling_layer=False)\n",
    "        self.classifier = RobertaClassificationHead(config, num_extra_dims)\n",
    "\n",
    "        self.post_init()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.LongTensor] = None,\n",
    "        attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        extra_data: Optional[torch.FloatTensor] = None,  \n",
    "        token_type_ids: Optional[torch.LongTensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        head_mask: Optional[torch.FloatTensor] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple[torch.Tensor], SequenceClassifierOutput]:\n",
    "        r\"\"\"\n",
    "        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n",
    "            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n",
    "            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n",
    "            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        outputs = self.roberta(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        sequence_output = outputs[0]\n",
    "        cls_output = sequence_output[:, 0, :] \n",
    "\n",
    "        if extra_data is not None:\n",
    "            cls_output = torch.cat((cls_output, extra_data), dim=-1)\n",
    "\n",
    "        logits = self.classifier(cls_output)\n",
    "            \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # move labels to correct device to enable model parallelism\n",
    "            labels = labels.to(logits.device)\n",
    "            if self.config.problem_type is None:\n",
    "                if self.num_labels == 1:\n",
    "                    self.config.problem_type = \"regression\"\n",
    "                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n",
    "                    self.config.problem_type = \"single_label_classification\"\n",
    "                else:\n",
    "                    self.config.problem_type = \"multi_label_classification\"\n",
    "\n",
    "            if self.config.problem_type == \"regression\":\n",
    "                loss_fct = MSELoss()\n",
    "                if self.num_labels == 1:\n",
    "                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n",
    "                else:\n",
    "                    loss = loss_fct(logits, labels)\n",
    "            elif self.config.problem_type == \"single_label_classification\":\n",
    "                loss_fct = CrossEntropyLoss()\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            elif self.config.problem_type == \"multi_label_classification\":\n",
    "                loss_fct = BCEWithLogitsLoss()\n",
    "                loss = loss_fct(logits, labels)\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs):\n",
    "        \"\"\"\n",
    "        Override `from_pretrained` to handle `num_extra_dims` when loading a pre-trained model.\n",
    "        \"\"\"\n",
    "        num_extra_dims = kwargs.pop(\"num_extra_dims\", 0)  # Extract num_extra_dims\n",
    "        config = kwargs.pop(\"config\", None)\n",
    "\n",
    "        if config is None:\n",
    "            config = AutoConfig.from_pretrained(pretrained_model_name_or_path, **kwargs)\n",
    "\n",
    "        config.num_extra_dims = num_extra_dims  # Inject extra_dims into config\n",
    "\n",
    "        return super().from_pretrained(\n",
    "            pretrained_model_name_or_path,\n",
    "            *model_args,\n",
    "            config=config,\n",
    "            num_extra_dims=num_extra_dims,  # Pass it explicitly\n",
    "            **kwargs\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7838ccd4-b09a-4e65-9407-8a69c569b868",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data\n",
    "\n",
    "df, id2label, label2id = binary_labels(tweets)\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42)\n",
    "datasets = {'train': Dataset.from_pandas(train_df), 'val': Dataset.from_pandas(val_df), 'test': Dataset.from_pandas(test_df)}\n",
    "\n",
    "\n",
    "datasets = DatasetDict(datasets)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model_name='roberta-base'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38c78a73-0d0c-4614-8c8d-13072ae8bf81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8b1c9f2b4684e2eaf3dacab07236d4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/101 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a98adcb40563409ab85a0bd55e2f97b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/26 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1d9098642b74440bf8c9b67c53de435",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/32 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of CustomRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.0.bias', 'classifier.out_proj.0.weight', 'classifier.out_proj.2.bias', 'classifier.out_proj.2.weight', 'classifier.out_proj.4.bias', 'classifier.out_proj.4.weight', 'classifier.out_proj.6.bias', 'classifier.out_proj.6.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='70' max='70' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [70/70 00:07, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.693127</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.693104</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.693062</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.692874</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.691817</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.692261</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.690629</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.691562</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.691639</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.691594</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=70, training_loss=0.6867175510951451, metrics={'train_runtime': 7.6218, 'train_samples_per_second': 132.515, 'train_steps_per_second': 9.184, 'total_flos': 19897866696240.0, 'train_loss': 0.6867175510951451, 'epoch': 10.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "if model_name==\"meta-llama/Llama-3.2-1B\":\n",
    "    truncation = True\n",
    "    max_length = 2000\n",
    "\n",
    "    quantization_config = BitsAndBytesConfig(load_in_8bit=True, bnb_4bit_compute_dtype=torch.float16)\n",
    "    \n",
    "    peft_config = LoraConfig(\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.1,\n",
    "        r=64,\n",
    "        bias=\"none\",\n",
    "        task_type=\"SEQ_CLS\",\n",
    "    )\n",
    "        \n",
    "    new_model = CustomLlamaForSequenceClassification(\n",
    "        model_name=model_name,\n",
    "        quantization_config=quantization_config,\n",
    "        peft_config=peft_config,\n",
    "        num_extra_dims=extra_dims,\n",
    "        num_labels=2)\n",
    "    \n",
    "    tokenizer.pad_token = \"[PAD]\"\n",
    "    tokenizer.pad_token_id = new_model.config.pad_token_id\n",
    "    tokenized_datasets = datasets.map(tokenize_function, batched=True)\n",
    "    \n",
    "    new_model.config.pad_token_id = new_model.config.eos_token_id\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir = \"out/\",\n",
    "        learning_rate=2e-4,\n",
    "        num_train_epochs=10,\n",
    "        per_device_train_batch_size = 1,\n",
    "        per_device_eval_batch_size = 1,\n",
    "        weight_decay=0.01,\n",
    "        eval_strategy = \"epoch\",\n",
    "        push_to_hub=False,\n",
    "        save_strategy='no',\n",
    "        #save_safetensors=True,\n",
    "        #load_best_model_at_end = True,\n",
    "        #report_to=\"none\",\n",
    "    )\n",
    "\n",
    "elif model_name == \"bert-base-uncased\":\n",
    "    tokenized_datasets = datasets.map(tokenize_function, batched=True)\n",
    "\n",
    "    config = AutoConfig.from_pretrained('bert-base-uncased', num_labels=2, \n",
    "        id2label=id2label, \n",
    "        label2id=label2id\n",
    "    )\n",
    "    \n",
    "    new_model = CustomBertSequenceClassification.from_pretrained(\n",
    "        'bert-base-uncased',  \n",
    "        config=config,\n",
    "        num_extra_dims=extra_dims) \n",
    "    #args\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir = '/model/',\n",
    "        learning_rate=2e-5,\n",
    "        num_train_epochs=10,\n",
    "        per_device_train_batch_size = 16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        weight_decay=0.01,\n",
    "        eval_strategy = \"epoch\",\n",
    "        push_to_hub=False,\n",
    "        save_strategy='no',\n",
    "        seed=42)\n",
    "    \n",
    "    \n",
    "\n",
    "elif model_name ==\"roberta-base\":\n",
    "    tokenized_datasets = datasets.map(tokenize_function, batched=True)\n",
    "\n",
    "\n",
    "    config = AutoConfig.from_pretrained('roberta-base', num_labels=2, \n",
    "        id2label=id2label, \n",
    "        label2id=label2id\n",
    "    )\n",
    "    \n",
    "    new_model = CustomRobertaForSequenceClassification.from_pretrained(\n",
    "        'roberta-base',  \n",
    "        config=config,\n",
    "        num_extra_dims=extra_dims\n",
    "    )\n",
    "    #args\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir = '/model/',\n",
    "        learning_rate=2e-5,\n",
    "        num_train_epochs=10,\n",
    "        per_device_train_batch_size = 16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        weight_decay=0.01,\n",
    "        eval_strategy = \"epoch\",\n",
    "        push_to_hub=False,\n",
    "        save_strategy='no',\n",
    "        seed=42)\n",
    "\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "        model=new_model,           \n",
    "        args=training_args,              \n",
    "        train_dataset=tokenized_datasets['train'],  \n",
    "        eval_dataset=tokenized_datasets['val'],     \n",
    "        tokenizer=tokenizer,             \n",
    "        compute_metrics=compute_metrics  \n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c97650fe-0ff2-4a51-add5-5c998b919e16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'0': {'precision': 0.53125,\n",
       "  'recall': 1.0,\n",
       "  'f1-score': 0.6938775510204082,\n",
       "  'support': 17.0},\n",
       " '1': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 15.0},\n",
       " 'accuracy': 0.53125,\n",
       " 'macro avg': {'precision': 0.265625,\n",
       "  'recall': 0.5,\n",
       "  'f1-score': 0.3469387755102041,\n",
       "  'support': 32.0},\n",
       " 'weighted avg': {'precision': 0.2822265625,\n",
       "  'recall': 0.53125,\n",
       "  'f1-score': 0.36862244897959184,\n",
       "  'support': 32.0}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "predictions = trainer.predict(tokenized_datasets['test'])\n",
    "predicted_class_ids = predictions.predictions.argmax(axis=1)\n",
    "actual_labels = tokenized_datasets['test']['label']\n",
    "results = classification_report(actual_labels, predicted_class_ids, digits=5, output_dict=True) \n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2580ec-2927-4593-be6a-6063c4d81207",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c98a886-4752-4948-bb84-703cfc4741e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomRobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=848, out_features=848, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Sequential(\n",
       "      (0): Linear(in_features=848, out_features=203, bias=True)\n",
       "      (1): Dropout(p=0.2902443815942271, inplace=False)\n",
       "      (2): Linear(in_features=203, out_features=134, bias=True)\n",
       "      (3): Dropout(p=0.21294012560852782, inplace=False)\n",
       "      (4): Linear(in_features=134, out_features=239, bias=True)\n",
       "      (5): Dropout(p=0.2101716211711512, inplace=False)\n",
       "      (6): Linear(in_features=239, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
