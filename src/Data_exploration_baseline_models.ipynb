{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e9a70c-da06-4f84-99d4-a4cbcd39c473",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from transformers import Trainer, TrainingArguments, AutoTokenizer, AutoConfig, BertPreTrainedModel, BertModel,AutoModel,LlamaPreTrainedModel, RobertaPreTrainedModel,AutoModelForSequenceClassification,RobertaModel, BertForSequenceClassification, RobertaForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset, DatasetDict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9be70f0",
   "metadata": {},
   "source": [
    "Este cuadernillo contiene:\n",
    "- la exploración de los datasets\n",
    "- la implementación de la clasificación del texto haciendo uso de la librería Hugging Face para el baseline (BERT Y LLAMA)\n",
    "- la comprobación de los mismos resultados usando la librería que con el código de adición de contexto social"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a42589-eb10-4a8b-97f4-15422dac10a3",
   "metadata": {},
   "source": [
    "# **DATASET1**\n",
    "Users from Twitter who tweeted about the topic 'Obama' during the period 8-10 May 2013;\n",
    "The last 32005 tweets for each user\n",
    "\n",
    "Cite: Pozzi, F.A., Maccagnola, D., Fersini, E., Messina, E. (2013). Enhance User-Level Sentiment Analysis on Microblogs with Approval Relations. In: Baldoni, M., Baroglio, C., Boella, G., Micalizio, R. (eds) AI*IA 2013: Advances in Artificial Intelligence. AI*IA 2013. Lecture Notes in Computer Science(), vol 8249. Springer, Cham. https://doi.org/10.1007/978-3-319-03524-6_12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e91a0a-b234-4bfb-8304-74b5b94e0936",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_pickle(\"../data/MIND/final_tweets_morality.pkl\")\n",
    "print('MIND DATASET')\n",
    "print('Moral labels')\n",
    "print(tweets['moral_label'].value_counts())\n",
    "print('------')\n",
    "print('Sentiment labels')\n",
    "print(tweets['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c053c69d-d1bf-4f56-8473-49169e6f2f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "users=pd.read_csv('../data/MIND/users.csv')\n",
    "\n",
    "\n",
    "tweets=pd.read_csv('../data/MIND/tweets.csv')\n",
    "#tweets = pd.read_csv('../data/tweets.csv', quotechar=\"'\")\n",
    "#tweets= tweets[['id','id_author','id_tweet','tweet','polarity']]\n",
    "#tweets.rename(columns={'tweet':'text', 'polarity':'label'}, inplace=True)\n",
    "\n",
    "network=pd.read_csv('../data/MIND/rt_network.csv')\n",
    "network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc52b59-61ff-415b-98e9-ba6698592b89",
   "metadata": {},
   "source": [
    "- Visualización de la red de usuarios\n",
    "- Visualización de embeddings con modelo ajustado y el modelo pre-entrenado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d779585-dd2f-45d0-aa8e-59809cd3cd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RED DE TWEETS\n",
    "user_tweets = tweets.rename(columns={'id_author': 'user', 'id_tweet': 'user_tweet'})\n",
    "retweeted_tweets = tweets.rename(columns={'id_author': 'retweetedUser', 'id_tweet': 'retweeted_tweet'})\n",
    "import pandas as pd\n",
    "\n",
    "# Crear combinaciones entre todos los tweets del user y retweetedUser\n",
    "user_tweets_expanded = pd.merge(network, user_tweets, on='user', how='inner')  # Vincular usuario y sus tweets\n",
    "all_combinations = pd.merge(user_tweets_expanded, retweeted_tweets, on='retweetedUser', how='inner')  # Vincular con retweets\n",
    "\n",
    "# Crear el dataframe de relaciones\n",
    "tweet_edges = all_combinations[['user_tweet', 'retweeted_tweet', 'weight']].rename(\n",
    "    columns={'user_tweet': 'source', 'retweeted_tweet': 'target'}\n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bd89e1-316e-4254-9a78-022eb3ddb3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#red de usuarios\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "# Crear el grafo\n",
    "#G = nx.from_pandas_edgelist(network, 'user', 'retweetedUser', edge_attr='weight')\n",
    "G = nx.from_pandas_edgelist(network, 'user', 'retweetedUser', edge_attr='weight', create_using=nx.MultiGraph())\n",
    "\n",
    "\n",
    "color_map = []\n",
    "for node in G.nodes():\n",
    "    if node in users['id_author'].values:\n",
    "        # Obtener la polaridad del usuario\n",
    "        polarity = users[users['id_author'] == node]['polarity'].values[0]\n",
    "        if polarity == 'neg':\n",
    "            color_map.append('red')  # Nodo de polaridad negativa en rojo\n",
    "        else:\n",
    "            color_map.append('blue')  # Nodo de polaridad positiva en azul\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e38574-e57a-4e5f-9aff-18bcc1ed4212",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = nx.spring_layout(G, seed=42)\n",
    "plt.figure(figsize=(10, 8))\n",
    "nx.draw(G, pos, with_labels=True, node_size=600, node_color=color_map, font_size=7, font_color='black', font_weight='bold', edge_color='gray')\n",
    "# Añadir los pesos en las aristas (solo si el peso es mayor que 1)\n",
    "for u, v, d in G.edges(data=True):\n",
    "    if d['weight'] > 1:\n",
    "        weight = d['weight']\n",
    "        \n",
    "        x1, y1 = pos[u]\n",
    "        x2, y2 = pos[v]\n",
    "        \n",
    "        x = (x1 + x2) / 2\n",
    "        y = (y1 + y2) / 2\n",
    "        plt.text(x, y, str(weight), fontsize=8, ha='center', va='center', color='red')\n",
    "\n",
    "plt.title(\"Retweet Network with User Polarities and Edge Weights\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754a8c58-351b-4ae6-bff8-633d3308409d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Número de nodos: {G.number_of_nodes()}\")\n",
    "print(f\"Número de aristas: {G.number_of_edges()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed8fece-0ae8-4712-8cbc-b3a3e397084a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usar circular_layout para una disposición circular de los nodos\n",
    "pos = nx.circular_layout(G)\n",
    "\n",
    "# Dibujar el grafo con el layout circular\n",
    "plt.figure(figsize=(10, 8))  # Ajustar el tamaño de la figura\n",
    "nx.draw(G, pos, with_labels=True, node_size=300, node_color=color_map, font_size=10, font_color='black', font_weight='bold', edge_color='gray')\n",
    "for u, v, d in G.edges(data=True):\n",
    "    if d['weight'] > 1:\n",
    "        weight = d['weight']\n",
    "        \n",
    "        x1, y1 = pos[u]\n",
    "        x2, y2 = pos[v]\n",
    "        \n",
    "        x = (x1 + x2) / 2\n",
    "        y = (y1 + y2) / 2\n",
    "        plt.text(x, y, str(weight), fontsize=8, ha='center', va='center', color='red')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acb4898-ac78-478d-a2d9-f71be9abd379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usar kamada_kawai_layout para una disposición que minimiza las distancias entre nodos conectados\n",
    "pos = nx.kamada_kawai_layout(G)\n",
    "\n",
    "# Dibujar el grafo con el layout Kamada-Kawai\n",
    "plt.figure(figsize=(10, 8))  # Ajustar el tamaño de la figura\n",
    "nx.draw(G, pos, with_labels=True, node_size=300, node_color=color_map, font_size=10, font_color='black', font_weight='bold', edge_color='gray')\n",
    "for u, v, d in G.edges(data=True):\n",
    "    if d['weight'] > 1:\n",
    "        weight = d['weight']\n",
    "        \n",
    "        x1, y1 = pos[u]\n",
    "        x2, y2 = pos[v]\n",
    "        \n",
    "        x = (x1 + x2) / 2\n",
    "        y = (y1 + y2) / 2\n",
    "        plt.text(x, y, str(weight), fontsize=8, ha='center', va='center', color='red')\n",
    "plt.title(\"Retweet Network with User Polarities (Kamada-Kawai Layout)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d946c7f9-4646-494a-b865-cbbc04ec30f2",
   "metadata": {},
   "source": [
    "## SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0e8c12-0a4c-40c5-9cd7-1918435e3409",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "users=pd.read_csv('../data/MIND/users.csv')\n",
    "users= users[[\"id_author\",'polarity']]\n",
    "\n",
    "tweets=pd.read_csv('../data/MIND/tweets.csv')\n",
    "\n",
    "network=pd.read_csv('../data/MIND/rt_network.csv')\n",
    "network= network[['id','user','retweetedUser','weight']]\n",
    "network = pd.merge(network, users, left_on='user', right_on='id_author', how='left')\n",
    "network.drop('id_author', axis=1, inplace=True)\n",
    "network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e9d221-e8c5-43ae-b085-67dc90fca4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "#scikit-network\n",
    "from sknetwork.embedding import SVD\n",
    "G = nx.from_pandas_edgelist(network, 'user', 'retweetedUser', edge_attr='weight')\n",
    "A = nx.adjacency_matrix(G)\n",
    "#Shallow Embedding (singular value decomposition)\n",
    "#Python3 implementation of svd in scikit-network’s, Scikit-network: Graph Analysis in Python, {Thomas Bonald and Nathan de Lara and Quentin Lutz and Bertrand Charpentier\n",
    "''' Aplicar a la matriz de adyacencia para aprender representaciones vectoriales (embeddings) de los nodos'''\n",
    "\n",
    "\n",
    "'''Usa la matriz de adyacencia de la red, factorización de la matriz (en 3 matrices: m ortogonal (vectores singulares izq), matriz diagonal (valores singulares) y matriz otogonal(vectores singulares der)) para conservar su estructura y propiedades\n",
    "a la vez que reduce su dimensión. '''\n",
    "\n",
    "svd = SVD(32)\n",
    "embedding = svd.fit_transform(A)\n",
    "\n",
    "\n",
    "nodes = list(G.nodes())\n",
    "df = pd.DataFrame({\n",
    "    'node': nodes,\n",
    "    'embedding': [embedding[i].tolist() for i in range(len(nodes))]})\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba441a3-eef7-4077-bd6f-66c8f6d85318",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tweets.merge(df, how='left', left_on='id_author', right_on='node')\n",
    "tweets.drop(columns=['node'], inplace=True)\n",
    "tweets.rename(columns={'embedding':'extra_data'}, inplace=True)\n",
    "tweets.head()\n",
    "\n",
    "# Save DataFrame as a pickle file\n",
    "#tweets.to_pickle('svd_df.pkl')\n",
    "\n",
    "#tweets = pd.read_pickle('../models/svd_df.pkl')\n",
    "#tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac12a55d-76bc-4d24-879c-7057a4b1791f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "users=pd.read_csv('../data/MIND/users.csv')\n",
    "users= users[[\"id_author\",'polarity']]\n",
    "\n",
    "tweets=pd.read_csv('../data/MIND/tweets.csv')\n",
    "\n",
    "network=pd.read_csv('../data/MIND/rt_network.csv')\n",
    "network= network[['id','user','retweetedUser','weight']]\n",
    "network = pd.merge(network, users, left_on='user', right_on='id_author', how='left')\n",
    "network.drop('id_author', axis=1, inplace=True)\n",
    "network\n",
    "\n",
    "G = nx.from_pandas_edgelist(network, 'user', 'retweetedUser', edge_attr='weight')\n",
    "mapping = {node: i for i, node in enumerate(G.nodes())}\n",
    "G = nx.relabel_nodes(G, mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d659375-95a2-4789-b265-d34da0847d7d",
   "metadata": {},
   "source": [
    "## DeepWalk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02dd4f1d-160e-4304-84e0-2128043a40da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "users=pd.read_csv('../data/MIND/users.csv')\n",
    "users= users[[\"id_author\",'polarity']]\n",
    "\n",
    "tweets=pd.read_csv('../data/MIND/tweets.csv')\n",
    "\n",
    "network=pd.read_csv('../data/MIND/rt_network.csv')\n",
    "network= network[['id','user','retweetedUser','weight']]\n",
    "network = pd.merge(network, users, left_on='user', right_on='id_author', how='left')\n",
    "network.drop('id_author', axis=1, inplace=True)\n",
    "network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0efd64-e0d4-4f25-8def-98574542ef2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from karateclub import DeepWalk\n",
    "import networkx as nx\n",
    "\n",
    "G = nx.from_pandas_edgelist(network, 'user', 'retweetedUser', edge_attr='weight')\n",
    "mapping = {node: i for i, node in enumerate(G.nodes())}\n",
    "G = nx.relabel_nodes(G, mapping)\n",
    "\n",
    "model = DeepWalk(dimensions=32, walk_length=30, workers=4)\n",
    "model.fit(G)\n",
    "embeddings = model.get_embedding()\n",
    "embeddings\n",
    "\n",
    "reverse_mapping = {v: k for k, v in mapping.items()}  # Reverse the mapping\n",
    "node_embeddings = {reverse_mapping[i]: embeddings[i] for i in range(len(embeddings))}\n",
    "\n",
    "embeddings_df = pd.DataFrame(list(node_embeddings.items()), columns=['user', 'extra_data'])\n",
    "tweets = tweets.merge(embeddings_df, how='left', left_on='id_author', right_on='user')\n",
    "tweets.drop(columns=['user'], inplace=True)\n",
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c688a01f-ef4a-45e4-b1c2-79e0b8f49f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save DataFrame as a pickle file\n",
    "#tweets.to_pickle('deepwalk_df.pkl')\n",
    "\n",
    "tweets = pd.read_pickle('../models/deepwalk_df.pkl')\n",
    "#tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613b92c1-a965-4a98-baf1-320e025aa7c9",
   "metadata": {},
   "source": [
    "## Node2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8266a4d3-061b-4107-a6dc-40947d4170f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "users=pd.read_csv('../data/MIND/users.csv')\n",
    "users= users[[\"id_author\",'polarity']]\n",
    "\n",
    "tweets=pd.read_csv('../data/MIND/tweets.csv')\n",
    "\n",
    "network=pd.read_csv('../data/MIND/rt_network.csv')\n",
    "network= network[['id','user','retweetedUser','weight']]\n",
    "network = pd.merge(network, users, left_on='user', right_on='id_author', how='left')\n",
    "network.drop('id_author', axis=1, inplace=True)\n",
    "network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb955d2d-6664-4b44-bf06-fe125c6fe984",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from node2vec import Node2Vec\n",
    "\n",
    "#Node2Vec\n",
    "#Python3 implementation of the node2vec algorithm Aditya Grover, Jure Leskovec and Vid Kocijan. node2vec: Scalable Feature Learning for Networks. A. Grover, J. Leskovec. ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD), 2016.\n",
    "'''node2vec es un algoritmo que genera representaciones (embeddings) de nodos en un grafo, utiliza un enfoque basado \n",
    "en caminatas aleatorias (explorando nodos vecinos) para capturar la estructura del grafo'''\n",
    "\n",
    "\n",
    "G = nx.from_pandas_edgelist(network, 'user', 'retweetedUser', edge_attr='weight')\n",
    "\n",
    "node2vec = Node2Vec(G, dimensions=32, walk_length=30, num_walks=200, workers=1)  \n",
    "model = node2vec.fit(window=10, min_count=1, batch_words=4)  \n",
    "\n",
    "#node_embedding = model.wv['716543']\n",
    "#print(\"Embedding for node 0:\", node_embedding)\n",
    "\n",
    "# Nodos similares a '716543'\n",
    "#similar_nodes = model.wv.similar_by_vector(model.wv['716543'], topn=5)\n",
    "#print(\"Most similar nodes to node 716543:\", similar_nodes)\n",
    "\n",
    "# Map node index to original node labels\n",
    "node_embeddings = {node: model.wv[str(node)] for node in list(G.nodes())}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ce7d24-aede-4352-9eb1-4c9ce687b8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(node):\n",
    "    try:\n",
    "        return model.wv[str(node)]\n",
    "    except KeyError:\n",
    "        return np.zeros(model.wv.vector_size)\n",
    "\n",
    "# Column for embeddings\n",
    "tweets['extra_data'] = tweets['id_author'].apply(get_embedding)\n",
    "\n",
    "tweets.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91237411-6bb2-42b6-ae2c-856f6c1d1767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save DataFrame as a pickle file\n",
    "#tweets.to_pickle('node2vec_df.pkl')\n",
    "\n",
    "tweets = pd.read_pickle('../models/node2vec_df.pkl')\n",
    "tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba4ebb0-d491-4510-a712-c00d79e2963b",
   "metadata": {},
   "source": [
    "## TADW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a347e78-7b26-409e-93c5-94f3d53ecb1b",
   "metadata": {},
   "source": [
    "Text Associated Deep Walk\n",
    "Enfoque que utiliza atributos textuales de los nodos (en este caso One Hot Encoding) sin hacer uso de convoluciones y usando un enfoque matricial.\n",
    "\n",
    "- Representar la estructura del grafo como una matriz M (basada en las conexiones del grafo)\n",
    "- Descomponer M en dos partes:\n",
    "\n",
    "    Una matriz W, que representa las relaciones entre los nodos según la estructura del grafo.\n",
    "    Una matriz H, que está conectada a T y captura la información textual.\n",
    "\n",
    "Resolver un problema matemático que ajusta W y H al mismo tiempo, de modo que ambas matrices trabajen juntas para combinar estructura y texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff28b56-7cbe-4c70-a40f-ca17e1bf04c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "users=pd.read_csv('../data/MIND/users.csv')\n",
    "users= users[[\"id_author\",'polarity']]\n",
    "\n",
    "tweets=pd.read_csv('../data/MIND/tweets.csv')\n",
    "\n",
    "network=pd.read_csv('../data/MIND/rt_network.csv')\n",
    "network= network[['id','user','retweetedUser','weight']]\n",
    "network = pd.merge(network, users, left_on='user', right_on='id_author', how='left')\n",
    "network.drop('id_author', axis=1, inplace=True)\n",
    "network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587facca-7778-4d9e-9b24-164552eb4f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "\n",
    "#Grafo\n",
    "G = nx.Graph()\n",
    "\n",
    "for _, row in network.iterrows():\n",
    "    G.add_edge(\n",
    "        row['user'],                # Source node\n",
    "        row['retweetedUser'],       # Target node\n",
    "        weight=row['weight'],       # Edge attribute: weight\n",
    "        polarity=row['polarity']    # Edge attribute: polarity\n",
    "    )\n",
    "\n",
    "#Añadir atributos a los nodos (polaridad)\n",
    "node_polarity = {}\n",
    "for u, v, data in G.edges(data=True):\n",
    "    if u not in node_polarity:\n",
    "        node_polarity[u] = data['polarity']\n",
    "    if v not in node_polarity:\n",
    "        node_polarity[v] = data['polarity']\n",
    "\n",
    "\n",
    "nx.set_node_attributes(G, node_polarity, name='polarity')\n",
    "\n",
    "#codificar atributos\n",
    "polarity_map = {'neg': [1, 0], 'pos': [0, 1]}\n",
    "features = {node: polarity_map[data['polarity']] for node, data in G.nodes(data=True)}\n",
    "\n",
    "#matriz de atributos\n",
    "feature_matrix = [features[node] for node in G.nodes()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ba02c3-34de-453d-9f18-3023c8867f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#modelo TADW\n",
    "from tadw import *\n",
    "tadw = TADW(graph=G, features=feature_matrix, dim=80, lamb=0.2)\n",
    "\n",
    "tadw.learn_embeddings()\n",
    "\n",
    "embeddings = tadw.get_embeddings()\n",
    "\n",
    "user_embeddings = {node: embeddings[i] for i, node in enumerate(G.nodes())}\n",
    "\n",
    "tweets['extra_data'] = tweets['id_author'].apply(lambda x: user_embeddings.get(x, None))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cecc4271-112e-4ea8-adcb-6b6a1799a38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save DataFrame as a pickle file\n",
    "tweets.to_pickle('../models/tweets_tadw_df2.pkl')\n",
    "\n",
    "tweets = pd.read_pickle('../models/tadw_df2.pkl')\n",
    "tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10360781-0788-41ca-aec1-f5f1e1403296",
   "metadata": {},
   "source": [
    "# **DATASET2**\n",
    "\n",
    "\n",
    "Tweets Health Care Reform\n",
    "Cite: Mukhija, S. Twitter Polarity Classification with Label Propagation over Lexical Links and the Follower Graph.Health Care Reform\n",
    "Cite: Mukhija, S. Twitter Polarity Classification with Label Propagation over Lexical Links and the Follower Graph.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca2cdd3-f3ea-406c-9530-2ae8833b73b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_pickle(\"../data/HCR/final_hcr_morality.pkl\")\n",
    "print('HCR DATASET')\n",
    "print('Moral labels')\n",
    "print(tweets['moral_label'].value_counts())\n",
    "print('------')\n",
    "print('Sentiment labels')\n",
    "print(tweets['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00796ab-33b4-4f14-bf86-77ce7ee40ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---1617 tweets---\n",
    "tweets = pd.read_csv(\"../data/HCR/hcr.tweets.polarity.tsv\", sep=\"\\t\")\n",
    "#-- 470 tweets con target hcr --- tweets[tweets['target']=='hcr']\n",
    "\n",
    "# --- 837 tweets etiquetados y 598 usuarios----- \n",
    "tweets=tweets[['tweet.id','user.id','author.nickname','content','sentiment','target']]\n",
    "tweets = tweets.dropna(subset=['sentiment'])\n",
    "tweets = tweets[tweets['sentiment'].isin(['positive', 'negative', 'neutral'])]\n",
    "tweets.sentiment.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe27f6c6-6d3b-4578-94c0-0484e74376b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "network = pd.read_csv(\"../data/HCR/hcr.relations.follower.inner.tsv\", sep=\"\\t\")\n",
    "\n",
    "#---- hay información sobre 1603 usuarios (más que de os que tenemos textos)\n",
    "print(len(network['from'].unique()), len(network['to'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26afdd2-67e1-4828-a9c4-97a243d1dfde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "network = pd.read_csv(\"../data/HCR/hcr.relations.follower.inner.tsv\", sep=\"\\t\")\n",
    "G = nx.from_pandas_edgelist(network, 'from', 'to')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "# Subgraph with matching users\n",
    "subG = G\n",
    "\n",
    "# Use spring layout for visualization\n",
    "pos = nx.spring_layout(subG, seed=42)\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "nx.draw_networkx_nodes(subG, pos, node_size=50, node_color='green', alpha=0.7)\n",
    "nx.draw_networkx_edges(subG, pos, alpha=0.3, edge_color='gray')\n",
    "plt.title(\"Follower Network (Dataset 2) - Users with Tweets\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e28cb0e-5273-482b-be42-e3c983c27200",
   "metadata": {},
   "source": [
    "## SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e316ae1f-cfcd-4fcf-a1b5-1345610ca59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from sknetwork.embedding import SVD\n",
    "network = pd.read_csv(\"../data/HCR/hcr.relations.follower.inner.tsv\", sep=\"\\t\")\n",
    "G = nx.from_pandas_edgelist(network, 'from', 'to')\n",
    "A = nx.adjacency_matrix(G).todense()\n",
    "\n",
    "svd = SVD(32)\n",
    "embedding = svd.fit_transform(A)\n",
    "\n",
    "nodes = list(G.nodes())\n",
    "df = pd.DataFrame({\n",
    "    'node': nodes,\n",
    "    'embedding': [embedding[i].tolist() for i in range(len(nodes))]})\n",
    "\n",
    "df.head()\n",
    "print(len(df['node'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0964bd4e-f774-4b33-bb3e-6c00688956a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tweets con etiqueta que sea positivo, negativo o neutral\n",
    "\n",
    "tweets = pd.read_csv(\"../data/HCR/hcr.tweets.polarity.tsv\", sep=\"\\t\")\n",
    "tweets=tweets[['tweet.id','user.id','author.nickname','content','sentiment','target']]\n",
    "tweets = tweets.dropna(subset=['sentiment'])\n",
    "tweets = tweets[tweets['sentiment'].isin(['positive', 'negative', 'neutral'])]\n",
    "#print(len(tweets['user.id'].unique()))\n",
    "\n",
    "#quedarnos con datos de los cuales tengamos la informacion del usuario\n",
    "df2=pd.merge(df,tweets, left_on='node', right_on='user.id', how='inner')\n",
    "df2.drop(columns=['node'], inplace=True)\n",
    "df2.rename(columns={'embedding':'extra_data'}, inplace=True)\n",
    "\n",
    "len(df2['user.id'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80a4721-0b2b-4587-afcf-dde90f6e8dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df2.to_pickle('hcr_svd_df.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b8ed8b-6fab-4c29-a038-673ec286c29d",
   "metadata": {},
   "source": [
    "## DeepWalk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574118aa-6a45-4c16-a879-2a24b831905b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from karateclub import DeepWalk\n",
    "import networkx as nx\n",
    "\n",
    "tweets = pd.read_csv(\"../data/HCR/hcr.tweets.polarity.tsv\", sep=\"\\t\")\n",
    "tweets=tweets[['tweet.id','user.id','author.nickname','content','sentiment','target']]\n",
    "tweets = tweets.dropna(subset=['sentiment'])\n",
    "tweets = tweets[tweets['sentiment'].isin(['positive', 'negative', 'neutral'])]\n",
    "#print(len(tweets['user.id'].unique()))\n",
    "\n",
    "\n",
    "network = pd.read_csv(\"../data/HCR/hcr.relations.follower.inner.tsv\", sep=\"\\t\")\n",
    "G = nx.from_pandas_edgelist(network, 'from', 'to')\n",
    "mapping = {node: i for i, node in enumerate(G.nodes())}\n",
    "G = nx.relabel_nodes(G, mapping)\n",
    "\n",
    "model = DeepWalk(dimensions=32, walk_length=30, workers=4)\n",
    "model.fit(G)\n",
    "embeddings = model.get_embedding()\n",
    "embeddings\n",
    "\n",
    "reverse_mapping = {v: k for k, v in mapping.items()} \n",
    "node_embeddings = {reverse_mapping[i]: embeddings[i] for i in range(len(embeddings))}\n",
    "len(node_embeddings.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c241dfe4-5555-4375-8c0f-807cce5111e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(list(node_embeddings.items()), columns=['user', 'extra_data'])\n",
    "tweets = tweets.merge(df, how='inner', left_on='user.id', right_on='user')\n",
    "tweets.drop(columns=['user'], inplace=True)\n",
    "print(len(tweets['user.id'].unique()))\n",
    "\n",
    "#tweets.to_pickle('hcr_deepwalk_df.pkl')\n",
    "matching_users= tweets['user.id'].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7139ae15-e42b-4893-8987-1fd5d1cc0373",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tweets.to_pickle('hcr_deepwalk_df.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06749452-557b-4d9a-b149-e08074eca9ff",
   "metadata": {},
   "source": [
    "## Node2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae8d1e2-4358-4c06-8a14-9a556b2e1709",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from node2vec import Node2Vec\n",
    "\n",
    "tweets = pd.read_csv(\"../data/HCR/hcr.tweets.polarity.tsv\", sep=\"\\t\")\n",
    "tweets=tweets[['tweet.id','user.id','author.nickname','content','sentiment','target']]\n",
    "tweets = tweets.dropna(subset=['sentiment'])\n",
    "print(len(tweets['user.id'].unique()))\n",
    "tweets = tweets[tweets['sentiment'].isin(['positive', 'negative', 'neutral'])]\n",
    "print(len(tweets['user.id'].unique()))\n",
    "\n",
    "\n",
    "network = pd.read_csv(\"../data/HCR/hcr.relations.follower.inner.tsv\", sep=\"\\t\")\n",
    "G = nx.from_pandas_edgelist(network, 'from', 'to')\n",
    "\n",
    "node2vec = Node2Vec(G, dimensions=32, walk_length=30, num_walks=200, workers=1)  \n",
    "model = node2vec.fit(window=10, min_count=1, batch_words=4)  \n",
    "node_embeddings = {node: model.wv[str(node)] for node in list(G.nodes())}\n",
    "print(len(node_embeddings.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf3e29a-615a-4708-803c-5991703ef69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def get_embedding(node):\n",
    "    try:\n",
    "        return model.wv[str(node)]\n",
    "    except KeyError:\n",
    "        return np.nan\n",
    "# Column for embeddings\n",
    "tweets['extra_data'] = tweets['user.id'].apply(get_embedding)\n",
    "tweets= tweets.dropna(subset=['extra_data'])\n",
    "\n",
    "print(len(tweets['user.id'].unique()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2e4cdb-5622-4d59-bc6f-38638e00187d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.to_pickle('hcr_node2vec_df.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da39c3a-c545-4fd8-920f-c4f0033e090b",
   "metadata": {},
   "source": [
    "## TADW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bc07c1-bc23-4031-83b3-d4b99dc1c37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#seleccionar la polaridad de cada usuario\n",
    "polarity = pd.read_csv(\"../data/HCR/hcr.user.polarity.hcr.tsv\", sep=\"\\t\")\n",
    "polarity[\"polarity\"] = polarity.iloc[:, 1:].idxmax(axis=1)\n",
    "polarity= polarity[['user.id','polarity']]\n",
    "polarity = polarity[polarity['user.id'].isin(matching_users)]\n",
    "polarity.rename(columns={'polarity': 'user_polarity'}, inplace=True)\n",
    "polarity['user_polarity'] = polarity['user_polarity'].replace({'irrelevant': 'neutral', 'unsure': 'neutral'})\n",
    "polarity['user_polarity'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf065c4e-7387-4efd-9753-87d59e75815a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#asociar a los usuarios que tenemos en la red su polaridad, buscar su polaridad en from y to\n",
    "network = pd.read_csv(\"../data/HCR/hcr.relations.follower.inner.tsv\", sep=\"\\t\")\n",
    "\n",
    "# Añadir la polaridad de 'from'\n",
    "network2 = pd.merge(network, polarity, left_on='from', right_on='user.id', how='left')\n",
    "network2.rename(columns={'user_polarity': 'from_polarity'}, inplace=True)\n",
    "network2.drop(columns=['user.id'], inplace=True)  # Eliminar la columna duplicada\n",
    "\n",
    "# Añadir la polaridad de 'to'\n",
    "network2 = pd.merge(network2, polarity, left_on='to', right_on='user.id', how='left')\n",
    "network2.rename(columns={'user_polarity': 'to_polarity'}, inplace=True)\n",
    "network2.drop(columns=['user.id'], inplace=True)  # Eliminar la columna duplicada\n",
    "\n",
    "# Unir ambas polaridades en un solo conjunto único de usuarios\n",
    "unique_tuples = pd.concat([\n",
    "    network2[['from', 'from_polarity']].rename(columns={'from': 'user.id', 'from_polarity': 'user_polarity'}),\n",
    "    network2[['to', 'to_polarity']].rename(columns={'to': 'user.id', 'to_polarity': 'user_polarity'})\n",
    "]).drop_duplicates()\n",
    "\n",
    "# Mostrar las primeras filas para verificar\n",
    "unique_tuples= unique_tuples.dropna(subset=['user_polarity'])\n",
    "unique_tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d807e4-90e8-4f9d-858d-8c5496600e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#asociar polaridad de los usuarios y añadirlo a el df tweets, si un usuario no tiene polaridad ponemos neutral\n",
    "\n",
    "tweets = pd.read_csv(\"../data/HCR/hcr.tweets.polarity.tsv\", sep=\"\\t\")\n",
    "tweets=tweets[['tweet.id','user.id','author.nickname','content','sentiment','target']]\n",
    "tweets = tweets.dropna(subset=['sentiment'])\n",
    "tweets = tweets[tweets['sentiment'].isin(['positive', 'negative', 'neutral'])]\n",
    "\n",
    "#ahora uno con la polaridad que tengo de los usuarios (algunos no tienen le pondremos neutral)\n",
    "tweets=pd.merge(tweets,unique_tuples, left_on='user.id', right_on='user.id', how='left')\n",
    "tweets['user_polarity'] = tweets['user_polarity'].fillna('neutral')\n",
    "\n",
    "print(len(tweets['user.id'].unique()), len(tweets['tweet.id'].unique()))\n",
    "tweets.sentiment.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff37c22-ea13-4c1b-b542-9cee1136afd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "G = nx.Graph()\n",
    "\n",
    "for _, row in network.iterrows():\n",
    "    G.add_edge(row['from'], row['to'])\n",
    "\n",
    "# Asignar polaridad a los nodos\n",
    "user_polarity_dict = tweets.set_index(\"user.id\")[\"user_polarity\"].to_dict()\n",
    "node_polarity = {node: user_polarity_dict.get(node, 'neutral') for node in G.nodes()}\n",
    "nx.set_node_attributes(G, node_polarity, name=\"user_polarity\")\n",
    "\n",
    "# Codificar atributos de polaridad\n",
    "polarity_map = {'negative': [1, 0, 0], 'positive': [0, 1, 0], 'neutral': [0, 0, 1]}\n",
    "features = {node: polarity_map[data['user_polarity']] for node, data in G.nodes(data=True)}\n",
    "\n",
    "# Crear matriz de atributos (ordenada según los nodos en el grafo)\n",
    "feature_matrix = [features[node] for node in G.nodes()]\n",
    "feature_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d36f97b-20d3-4882-9ffb-9a51465ba05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#modelo TADW NECESITA pip install networkx==2.7\n",
    "from tadw import *\n",
    "tadw = TADW(graph=G, features=feature_matrix, dim=80, lamb=0.2)\n",
    "\n",
    "tadw.learn_embeddings()\n",
    "\n",
    "embeddings = tadw.get_embeddings()\n",
    "\n",
    "user_embeddings = {node: embeddings[i] for i, node in enumerate(G.nodes())}\n",
    "\n",
    "tweets['extra_data'] = tweets['user.id'].apply(lambda x: user_embeddings.get(x, None))\n",
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d678a7-29b5-47ac-a72e-4575a801964f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets= tweets.dropna(subset=['extra_data'])\n",
    "\n",
    "print(len(tweets['user.id'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549f7b38-62e3-417b-85df-c49338acbabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tweets.to_pickle('hcr_tadw_df.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b873e316-bec7-4887-8f9f-3058b79f9d64",
   "metadata": {},
   "source": [
    "# **DATASET3**\n",
    "\n",
    "Tweets 2008 Presidential Debates (Obama MCain)\n",
    "\n",
    "Cite: David A. Shamma, Lyndon Kennedy, and Elizabeth F. Churchill. 2009. Tweet the debates: understanding community annotation of uncollected sources. In Proceedings of the first SIGMM workshop on Social media (WSM '09). Association for Computing Machinery, New York, NY, USA, 3–10. https://doi.org/10.1145/1631144.1631148"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b36a364-2323-4342-92be-16bdfd8ae7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_pickle(\"../data/OMC/final_omc_morality.pkl\")\n",
    "print('OMC DATASET')\n",
    "print('Moral labels')\n",
    "print(tweets['moral_label'].value_counts())\n",
    "print('------')\n",
    "print('Sentiment labels')\n",
    "print(tweets['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf4023e-66c0-45ef-b235-08526087dbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# ----- hay 2678 anotaciones de textos------\n",
    "tweets = pd.read_csv(\"../data/OMC/shamma.content.tweets.polarity.majority.tsv\", sep=\"\\t\")\n",
    "\n",
    "# ----- hay 2518 textos recuperados--------\n",
    "#hay más anotaciones que textos disponibles\n",
    "texts = pd.read_csv(\"../data/OMC/shamma.content.tweets.recovered.merged.tsv\", sep=\"\\t\")\n",
    "texts =texts[['tweet.id','user.id','content','dataset.author.name']]\n",
    "\n",
    "#hay textos no etiquetados con el majority pero si poseen los ratings (no hay etiqueta agregada por empate)\n",
    "tweets=pd.merge(texts,tweets, left_on='tweet.id', right_on='tweet.id', how='left')\n",
    "\n",
    "#se eliminan aquellos tweets que no tengan etiqueta agregada\n",
    "#--- hay 2071 tweets y 791 usuarios----ver de cuales tenemos información en la red\n",
    "tweets= tweets.dropna(subset=['majority_polarity'])\n",
    "print(len(tweets['user.id'].unique()), len(tweets['tweet.id'].unique()))\n",
    "print(tweets.majority_polarity.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42deb7df-9435-4d25-8fb0-7160c803e9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "network = pd.read_csv(\"../data/OMC/shamma.relations.user.user.follower.inner.tsv\", sep=\"\\t\")\n",
    "\n",
    "#---- hay información sobre 635 usuarios, ver de los cuales tenemos textos\n",
    "print(len(network['from'].unique()), len(network['to'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbbf4f6-6c3a-40c7-ab69-1f29df334593",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "network = pd.read_csv(\"../data/OMC/shamma.relations.user.user.follower.inner.tsv\", sep=\"\\t\")\n",
    "\n",
    "G = nx.from_pandas_edgelist(network, 'from', 'to')\n",
    "\n",
    "nodos = set(G.nodes)  \n",
    "usuarios = set(tweets['user.id'].unique())\n",
    "matching_users = nodos.intersection(usuarios)\n",
    "\n",
    "# --- información de 604 usuarios---\n",
    "filtered_tweets = tweets[tweets['user.id'].isin(matching_users)]\n",
    "\n",
    "print(f\"Total de nodos en el grafo: {len(nodos)}\")\n",
    "print(f\"Usuarios en común (matching_users): {len(matching_users)}\")\n",
    "print(f\"Filas en filtered_df: {len(filtered_tweets)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbadaa2-fcff-492d-af19-d7f89c3b81b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "subG = G.subgraph(matching_users).copy()\n",
    "\n",
    "pos = nx.fruchterman_reingold_layout(subG, seed=42)\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "\n",
    "nx.draw_networkx_edges(subG, pos, alpha=0.2, edge_color='gray', width=1)\n",
    "nx.draw_networkx_nodes(subG, pos, node_size=80, node_color='skyblue', alpha=0.7)\n",
    "\n",
    "# No etiquetas para mantener limpio\n",
    "\n",
    "plt.title(\"User network (Dataset 3) - Fruchterman-Reingold layout\")\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d361b8aa-6145-435f-a5ae-5930734b8804",
   "metadata": {},
   "source": [
    "## SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ddbed9-3484-48ef-93cf-d98205c1a92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from sknetwork.embedding import SVD\n",
    "\n",
    "network = pd.read_csv(\"../data/OMC/shamma.relations.user.user.follower.inner.tsv\", sep=\"\\t\")\n",
    "G = nx.from_pandas_edgelist(network, 'from', 'to')\n",
    "A = nx.adjacency_matrix(G).todense()\n",
    "\n",
    "svd = SVD(32)\n",
    "embedding = svd.fit_transform(A)\n",
    "\n",
    "\n",
    "nodes = list(G.nodes())\n",
    "df = pd.DataFrame({\n",
    "    'node': nodes,\n",
    "    'embedding': [embedding[i].tolist() for i in range(len(nodes))]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c87962-6b1f-4c6d-9655-4c211fc68398",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_csv(\"../data/OMC/shamma.content.tweets.polarity.majority.tsv\", sep=\"\\t\")\n",
    "\n",
    "texts = pd.read_csv(\"../data/OMC/shamma.content.tweets.recovered.merged.tsv\", sep=\"\\t\")\n",
    "texts =texts[['tweet.id','user.id','content','dataset.author.name']]\n",
    "tweets=pd.merge(texts,tweets, left_on='tweet.id', right_on='tweet.id', how='left')\n",
    "tweets= tweets.dropna(subset=['majority_polarity'])\n",
    "\n",
    "df2=pd.merge(df,tweets, left_on='node', right_on='user.id', how='inner')\n",
    "df2.drop(columns=['node'], inplace=True)\n",
    "df2.rename(columns={'embedding':'extra_data'}, inplace=True)\n",
    "len(df2['user.id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4b9c8d-6073-4cd1-bff4-8014503774f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df2 = pd.read_pickle('omc_svd_df.pkl')\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cd8860-2f70-44e7-aa9b-4374f7a666bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualizar red\n",
    "\n",
    "#fig, ax = plt.subplots(figsize=(15, 9))\n",
    "#ax.axis(\"off\")\n",
    "#plot_options = {\"node_size\": 10, \"with_labels\": False, \"width\": 0.15}\n",
    "#nx.draw_networkx(G, pos=nx.random_layout(G), ax=ax, **plot_options)\n",
    "\n",
    "#pos = nx.spring_layout(G, iterations=15, seed=1721)\n",
    "#fig, ax = plt.subplots(figsize=(15, 9))\n",
    "#ax.axis(\"off\")\n",
    "#nx.draw_networkx(G, pos=pos, ax=ax, **plot_options)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f94f65f-eca9-418c-98dd-a6cfd6bc9820",
   "metadata": {},
   "source": [
    "## Deepwalk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628369d0-258f-4b54-a7fb-c3e37bcb5a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from karateclub import DeepWalk\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "\n",
    "tweets = pd.read_csv(\"../data/OMC/shamma.content.tweets.polarity.majority.tsv\", sep=\"\\t\")\n",
    "\n",
    "texts = pd.read_csv(\"../data/OMC/shamma.content.tweets.recovered.merged.tsv\", sep=\"\\t\")\n",
    "texts =texts[['tweet.id','user.id','content','dataset.author.name']]\n",
    "tweets=pd.merge(texts,tweets, left_on='tweet.id', right_on='tweet.id', how='left')\n",
    "tweets= tweets.dropna(subset=['majority_polarity'])\n",
    "print(len(tweets['user.id'].unique()), len(tweets['tweet.id'].unique()))\n",
    "\n",
    "\n",
    "network = pd.read_csv(\"../data/OMC/shamma.relations.user.user.follower.inner.tsv\", sep=\"\\t\")\n",
    "G = nx.from_pandas_edgelist(network, 'from', 'to')\n",
    "mapping = {node: i for i, node in enumerate(G.nodes())}\n",
    "G = nx.relabel_nodes(G, mapping)\n",
    "\n",
    "model = DeepWalk(dimensions=32, walk_length=30, workers=4)\n",
    "model.fit(G)\n",
    "embeddings = model.get_embedding()\n",
    "embeddings\n",
    "\n",
    "reverse_mapping = {v: k for k, v in mapping.items()} \n",
    "node_embeddings = {reverse_mapping[i]: embeddings[i] for i in range(len(embeddings))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8229e2-2fe6-44e1-9bfe-6e4aa20bcb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(list(node_embeddings.items()), columns=['user', 'extra_data'])\n",
    "tweets = tweets.merge(df, how='inner', left_on='user.id', right_on='user')\n",
    "tweets.drop(columns=['user'], inplace=True)\n",
    "print(len(tweets['user.id'].unique()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797dbc71-488b-49f6-bc19-d339d0a83be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tweets.to_pickle('omc_deepwalk_df.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f392c2-a9db-4d3b-854f-45b3a3cd9be7",
   "metadata": {},
   "source": [
    "## Node2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357884e8-dfc2-4102-99f5-140e47efa2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from node2vec import Node2Vec\n",
    "\n",
    "tweets = pd.read_csv(\"../data/OMC/shamma.content.tweets.polarity.majority.tsv\", sep=\"\\t\")\n",
    "\n",
    "texts = pd.read_csv(\"../data/OMC/shamma.content.tweets.recovered.merged.tsv\", sep=\"\\t\")\n",
    "texts =texts[['tweet.id','user.id','content','dataset.author.name']]\n",
    "tweets=pd.merge(texts,tweets, left_on='tweet.id', right_on='tweet.id', how='left')\n",
    "tweets= tweets.dropna(subset=['majority_polarity'])\n",
    "print(len(tweets['user.id'].unique()), len(tweets['tweet.id'].unique()))\n",
    "\n",
    "network = pd.read_csv(\"../data/OMC/shamma.relations.user.user.follower.inner.tsv\", sep=\"\\t\")\n",
    "G = nx.from_pandas_edgelist(network, 'from', 'to')\n",
    "\n",
    "node2vec = Node2Vec(G, dimensions=32, walk_length=30, num_walks=200, workers=1)  \n",
    "model = node2vec.fit(window=10, min_count=1, batch_words=4)  \n",
    "node_embeddings = {node: model.wv[str(node)] for node in list(G.nodes())}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe85acb-6dc6-497f-833b-8b7b157ebe30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def get_embedding(node):\n",
    "    try:\n",
    "        return model.wv[str(node)]\n",
    "    except KeyError:\n",
    "        return np.nan\n",
    "# Column for embeddings\n",
    "tweets['extra_data'] = tweets['user.id'].apply(get_embedding)\n",
    "tweets= tweets.dropna(subset=['extra_data'])\n",
    "\n",
    "print(len(tweets['user.id'].unique()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe881bc-6e8d-4b28-abdc-f22345406481",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tweets.to_pickle('omc_node2vec_df.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018bca15-c9f9-479b-9416-b0ac964f3b8c",
   "metadata": {},
   "source": [
    "## TADW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defc40bb-0367-4c19-8843-1e379bbe574c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "polarity = pd.read_csv(\"../data/OMC/shamma.users.polarity.majority.tsv\", sep=\"\\t\")\n",
    "polarity = polarity[polarity['user.id'].isin(matching_users)]\n",
    "polarity.rename(columns={'polarity': 'user_polarity'}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202d662a-c07a-4388-b5ac-edd39a65fcd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "network = pd.read_csv(\"../data/OMC/shamma.relations.user.user.follower.inner.tsv\", sep=\"\\t\")\n",
    "\n",
    "# Añadir la polaridad de 'from'\n",
    "network2 = pd.merge(network, polarity, left_on='from', right_on='user.id', how='left')\n",
    "network2.rename(columns={'user_polarity': 'from_polarity'}, inplace=True)\n",
    "network2.drop(columns=['user.id'], inplace=True)  # Eliminar la columna duplicada\n",
    "\n",
    "# Añadir la polaridad de 'to'\n",
    "network2 = pd.merge(network2, polarity, left_on='to', right_on='user.id', how='left')\n",
    "network2.rename(columns={'user_polarity': 'to_polarity'}, inplace=True)\n",
    "network2.drop(columns=['user.id'], inplace=True)  # Eliminar la columna duplicada\n",
    "\n",
    "# Unir ambas polaridades en un solo conjunto único de usuarios\n",
    "unique_tuples = pd.concat([\n",
    "    network2[['from', 'from_polarity']].rename(columns={'from': 'user.id', 'from_polarity': 'user_polarity'}),\n",
    "    network2[['to', 'to_polarity']].rename(columns={'to': 'user.id', 'to_polarity': 'user_polarity'})\n",
    "]).drop_duplicates()\n",
    "\n",
    "# Mostrar las primeras filas para verificar\n",
    "unique_tuples= unique_tuples.dropna(subset=['user_polarity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07c2153-2b28-4bd6-89b8-d3f401a21d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecciono los textos cuyos usuarios se que tengo\n",
    "tweets = pd.read_csv(\"../data/OMC/shamma.content.tweets.polarity.majority.tsv\", sep=\"\\t\")\n",
    "texts = pd.read_csv(\"../data/OMC/shamma.content.tweets.recovered.merged.tsv\", sep=\"\\t\")\n",
    "texts =texts[['tweet.id','user.id','content','dataset.author.name']]\n",
    "tweets=pd.merge(texts,tweets, left_on='tweet.id', right_on='tweet.id', how='left')\n",
    "tweets= tweets.dropna(subset=['majority_polarity'])\n",
    "tweets = tweets[tweets['user.id'].isin(matching_users)]\n",
    "\n",
    "#ahora uno con la polaridad que tengo de los usuarios (algunos no tienen le pondremos neutral)\n",
    "tweets=pd.merge(tweets,unique_tuples, left_on='user.id', right_on='user.id', how='left')\n",
    "tweets['user_polarity'] = tweets['user_polarity'].fillna('neutral')\n",
    "print(len(tweets['user.id'].unique()), len(tweets['tweet.id'].unique()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2833fadb-9fde-411e-a7a7-ef5f32f9c453",
   "metadata": {},
   "outputs": [],
   "source": [
    "network = pd.read_csv(\"../data/OMC/shamma.relations.user.user.follower.inner.tsv\", sep=\"\\t\")\n",
    "\n",
    "G = nx.Graph()\n",
    "\n",
    "for _, row in network.iterrows():\n",
    "    G.add_edge(row['from'], row['to'])\n",
    "\n",
    "# Asignar polaridad a los nodos\n",
    "user_polarity_dict = tweets.set_index(\"user.id\")[\"user_polarity\"].to_dict()\n",
    "node_polarity = {node: user_polarity_dict.get(node, 'neutral') for node in G.nodes()}\n",
    "nx.set_node_attributes(G, node_polarity, name=\"user_polarity\")\n",
    "\n",
    "polarity_map = {'neg': [1, 0, 0], 'pos': [0, 1, 0], 'neutral': [0, 0, 1]}\n",
    "features = {node: polarity_map[data['user_polarity']] for node, data in G.nodes(data=True)}\n",
    "\n",
    "feature_matrix = [features[node] for node in G.nodes()]\n",
    "\n",
    "feature_matrix[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccf702c-a277-4894-846b-0ce37014eb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#modelo TADW\n",
    "from tadw import *\n",
    "tadw = TADW(graph=G, features=feature_matrix, dim=80, lamb=0.2)\n",
    "\n",
    "tadw.learn_embeddings()\n",
    "\n",
    "embeddings = tadw.get_embeddings()\n",
    "\n",
    "user_embeddings = {node: embeddings[i] for i, node in enumerate(G.nodes())}\n",
    "\n",
    "tweets['extra_data'] = tweets['user.id'].apply(lambda x: user_embeddings.get(x, None))\n",
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b8d44f-0425-449b-8717-82cd1dd1e3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tweets.to_pickle('omc_tadw_df.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb81d4a-9afa-4846-8185-4cbede06e43c",
   "metadata": {},
   "source": [
    "# **Gráficas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16369d7d-41a5-48d6-aad8-9897fe4e42c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set style\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Sentiment data\n",
    "sentiment_data = {\n",
    "    'Dataset': ['RT-MIND', 'RT-MIND', 'HCR', 'HCR', 'HCR', 'OMC', 'OMC'],\n",
    "    'Label': ['negative', 'positive', 'negative', 'positive', 'neutral', 'negative', 'positive'],\n",
    "    'Count': [89, 70, 371, 149, 132, 1051, 716]\n",
    "}\n",
    "df_sentiment = pd.DataFrame(sentiment_data)\n",
    "df_sentiment['Percent'] = df_sentiment.groupby('Dataset')['Count'].transform(lambda x: 100 * x / x.sum())\n",
    "\n",
    "# Morality data\n",
    "morality_data = {\n",
    "    'Dataset': ['RT-MIND']*6 + ['HCR']*6 + ['OMC']*6,\n",
    "    'Label': ['Care', 'Fairness', 'Authority', 'Loyalty', 'Purity', 'NM']*3,\n",
    "    'Count': [11, 15, 26, 9, 1, 97, 87, 73, 63, 48, 8, 370, 116, 74, 116, 79, 13, 1392]\n",
    "}\n",
    "df_morality = pd.DataFrame(morality_data)\n",
    "df_morality['Percent'] = df_morality.groupby('Dataset')['Count'].transform(lambda x: 100 * x / x.sum())\n",
    "\n",
    "# Plotting\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Normalized Sentiment Plot\n",
    "sns.barplot(data=df_sentiment, x='Dataset', y='Percent', hue='Label', ax=axes[0])\n",
    "axes[0].set_title('Normalized Sentiment Label Distribution (%)')\n",
    "axes[0].set_ylabel('Percentage')\n",
    "axes[0].legend(title='Sentiment')\n",
    "\n",
    "# Normalized Morality Plot\n",
    "sns.barplot(data=df_morality, x='Dataset', y='Percent', hue='Label', ax=axes[1])\n",
    "axes[1].set_title('Normalized Moral Foundation Label Distribution (%)')\n",
    "axes[1].set_ylabel('Percentage')\n",
    "axes[1].legend(title='Moral Label')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('normalized_sentiment_morality.pdf', bbox_inches='tight')\n",
    "plt.savefig('class_distribution.png', bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ef5a8e-bbe1-4391-ae62-798832ecab54",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_morality_pct = df_morality.copy()\n",
    "df_morality_pct['Percent'] = df_morality_pct.groupby('Dataset')['Count'].transform(lambda x: 100 * x / x.sum())\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.barplot(data=df_morality_pct, x='Dataset', y='Percent', hue='Label')\n",
    "plt.title('Normalized Moral Foundation Label Distribution (%)')\n",
    "plt.legend(title='Moral Label')\n",
    "plt.ylabel('Percentage')\n",
    "plt.tight_layout()\n",
    "plt.savefig('morality_distribution_normalized.pdf', bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76948ef-a51a-4aaf-8510-06efda8d7cd6",
   "metadata": {},
   "source": [
    "## GRAFOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b692be50-1497-491b-a43e-110340f49023",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axs = plt.subplots(3, 1, figsize=(12, 36))\n",
    "\n",
    "# Dataset 1\n",
    "users = pd.read_csv('../data/MIND/users.csv')\n",
    "users = users[[\"id_author\", 'polarity']]\n",
    "network1 = pd.read_csv('../data/MIND/rt_network.csv')\n",
    "network1 = network1[['id', 'user', 'retweetedUser', 'weight']]\n",
    "network1 = pd.merge(network1, users, left_on='user', right_on='id_author', how='left')\n",
    "network1.drop('id_author', axis=1, inplace=True)\n",
    "\n",
    "G1 = nx.from_pandas_edgelist(network1, 'user', 'retweetedUser', edge_attr='weight')\n",
    "pos1 = nx.spring_layout(G1, seed=42)\n",
    "nx.draw_networkx_nodes(G1, pos1, node_size=20, node_color='blue', alpha=0.7, ax=axs[0])\n",
    "nx.draw_networkx_edges(G1, pos1, alpha=0.3, edge_color='gray', ax=axs[0])\n",
    "axs[0].set_title(\"Retweet Network (Dataset 1)\")\n",
    "axs[0].axis('off')\n",
    "\n",
    "# Dataset 2\n",
    "network2 = pd.read_csv(\"../data/HCR/hcr.relations.follower.inner.tsv\", sep=\"\\t\")\n",
    "G2 = nx.from_pandas_edgelist(network2, 'from', 'to')\n",
    "pos2 = nx.spring_layout(G2, seed=42)\n",
    "nx.draw_networkx_nodes(G2, pos2, node_size=20, node_color='green', alpha=0.7, ax=axs[1])\n",
    "nx.draw_networkx_edges(G2, pos2, alpha=0.3, edge_color='gray', ax=axs[1])\n",
    "axs[1].set_title(\"Follower Network (Dataset 2)\")\n",
    "axs[1].axis('off')\n",
    "\n",
    "# Dataset 3\n",
    "network3 = pd.read_csv(\"../data/OMC/shamma.relations.user.user.follower.inner.tsv\", sep=\"\\t\")\n",
    "G3 = nx.from_pandas_edgelist(network3, 'from', 'to')\n",
    "pos3 = nx.spring_layout(G3, seed=42)\n",
    "nx.draw_networkx_nodes(G3, pos3, node_size=20, node_color='red', alpha=0.7, ax=axs[2])\n",
    "nx.draw_networkx_edges(G3, pos3, alpha=0.3, edge_color='gray', ax=axs[2])\n",
    "axs[2].set_title(\"Follower Network (Dataset 3)\")\n",
    "axs[2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('user_graphs.pdf', bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605b8519-bde3-42e8-b6b7-27dba21c85e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axs = plt.subplots(3, 1, figsize=(8, 18))  # más compacto\n",
    "\n",
    "# Dataset 1\n",
    "users = pd.read_csv('../data/MIND/users.csv')\n",
    "users = users[[\"id_author\", 'polarity']]\n",
    "network1 = pd.read_csv('../data/MIND/rt_network.csv')\n",
    "network1 = network1[['id', 'user', 'retweetedUser', 'weight']]\n",
    "network1 = pd.merge(network1, users, left_on='user', right_on='id_author', how='left')\n",
    "network1.drop('id_author', axis=1, inplace=True)\n",
    "\n",
    "G1 = nx.from_pandas_edgelist(network1, 'user', 'retweetedUser', edge_attr='weight')\n",
    "pos1 = nx.spring_layout(G1, seed=42)\n",
    "nx.draw_networkx_nodes(G1, pos1, node_size=10, node_color='blue', alpha=0.7, ax=axs[0])\n",
    "nx.draw_networkx_edges(G1, pos1, alpha=0.2, edge_color='gray', ax=axs[0])\n",
    "axs[0].set_title(\"Retweet Network (Dataset 1)\")\n",
    "axs[0].axis('off')\n",
    "\n",
    "# Dataset 2\n",
    "network2 = pd.read_csv(\"../data/HCR/hcr.relations.follower.inner.tsv\", sep=\"\\t\")\n",
    "G2 = nx.from_pandas_edgelist(network2, 'from', 'to')\n",
    "pos2 = nx.spring_layout(G2, seed=42)\n",
    "nx.draw_networkx_nodes(G2, pos2, node_size=10, node_color='green', alpha=0.7, ax=axs[1])\n",
    "nx.draw_networkx_edges(G2, pos2, alpha=0.2, edge_color='gray', ax=axs[1])\n",
    "axs[1].set_title(\"Follower Network (Dataset 2)\")\n",
    "axs[1].axis('off')\n",
    "\n",
    "# Dataset 3\n",
    "network3 = pd.read_csv(\"../data/OMC/shamma.relations.user.user.follower.inner.tsv\", sep=\"\\t\")\n",
    "G3 = nx.from_pandas_edgelist(network3, 'from', 'to')\n",
    "pos3 = nx.spring_layout(G3, seed=42)\n",
    "nx.draw_networkx_nodes(G3, pos3, node_size=10, node_color='red', alpha=0.7, ax=axs[2])\n",
    "nx.draw_networkx_edges(G3, pos3, alpha=0.2, edge_color='gray', ax=axs[2])\n",
    "axs[2].set_title(\"Follower Network (Dataset 3)\")\n",
    "axs[2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('user_graphs.pdf', bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78a3808-f66c-4055-93c0-df33ddd1e34c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# **Baselines HUGGING FACE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eabc48e-3847-4cbd-b7e8-74e62c24b0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import warnings\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import networkx as nx\n",
    "#import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import f1_score\n",
    "from datasets import Dataset, DatasetDict\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from transformers import Trainer, TrainingArguments, BertForSequenceClassification, RobertaForSequenceClassification, AutoTokenizer,AutoModelForSequenceClassification, AutoModel, RobertaForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca818368-9440-4ef4-b723-75cb771a81d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=42\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "torch.cuda.manual_seed_all(42)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215533f4-6f76-4d5f-88fa-65a085e0ae93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CLEAN DATA\n",
    "\n",
    "def cleaner1(tweet):\n",
    "    # remove usernames\n",
    "    # tweet = re.sub(\"@[A-Za-z0-9]+\",\"\",tweet)\n",
    "    tweet = tweet.lower()\n",
    "    tweet = re.sub(\"^rt\", \"\", tweet)\n",
    "    tweet = re.sub(\"\\s[0-9]+\\s\", \"\", tweet)\n",
    "    # remove usernames\n",
    "    tweet = re.sub(\"@[^\\s]+\", \"\", tweet)\n",
    "    tweet = re.sub(\"at_user\", \"\", tweet)\n",
    "    # remove urls\n",
    "    tweet = re.sub(\"pic.twitter.com/[A-Za-z0-9]+\", \"\", tweet)\n",
    "    tweet = re.sub(r\"(?:\\@|http?\\://|https?\\://|www)\\S+\", \"\", tweet)\n",
    "    tweet = tweet.replace(\"url\", \"\")\n",
    "    tweet = tweet.strip()\n",
    "    tweet = \" \".join(tweet.split())\n",
    "    return tweet\n",
    "\n",
    "\n",
    "    \n",
    "#BINARY LABELS SENT\n",
    "def binary_labels(df):\n",
    "    df['label'] = df['label'].map({'neg': 0, 'pos': 1}).astype('Int64')\n",
    "    id2label = {0: \"NEG\", 1: \"POS\"}\n",
    "    label2id = {\"NEG\": 0, \"POS\": 1}\n",
    "    return df, id2label, label2id\n",
    "\n",
    "def multi_labels(df):\n",
    "    df= df.replace({\"label\": {\"negative\": 0, \"positive\": 1,\"neutral\":2}})\n",
    "    id2label = {0: \"NEGATIVE\", 1: \"POSITIVE\", 2:\"NEUTRAL\"}\n",
    "    label2id = {\"NEGATIVE\": 0, \"POSITIV\": 1, \"NEUTRAL\":2}\n",
    "    return df, id2label, label2id\n",
    "    \n",
    "#MULTICLASS LABELS MORAL\n",
    "def label_multiclass6(df):\n",
    "    df= df.replace({'label': {'care': 1, 'harm': 1,\n",
    "                                'fairness': 2,'cheating': 2,\n",
    "                                'loyalty': 3,'betrayal': 3,\n",
    "                                'authority': 4,'subversion': 4,\n",
    "                                 'purity': 5,'degradation': 5,'nonmoral': 0,'nomoral': 0\n",
    "                                }})\n",
    "    \n",
    "    id2label = {0:\"NONMORAL\", 1:\"CARE\" ,1:\"HARM\",2:\"FAIRNESS\",2:\"CHEATING\",3:\"LOYALTY\",3:\"BETRAYAL\",4:\"AUTHORITY\",4:\"SUBVERSION\",5:\"PURITY\",5:\"DEGRADATION\"}\n",
    "    label2id = {\"NONMORAL\":0, \"CARE\": 1,\"HARM\":1,\"FAIRNESS\":2,\"CHEATING\":2,\"LOYALTY\":3,\"BETRAYAL\":3,\"AUTHORITY\":4,\"SUBVERSION\":4,\"PURITY\":5,\"DEGRADATION\":5}\n",
    "\n",
    "    return df, id2label,label2id \n",
    "\n",
    "    \n",
    "# METRICS\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='macro')\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall}\n",
    "\n",
    "# TOKENIZER\n",
    "def tokenize_function(examples):\n",
    "    tokenized_inputs=tokenizer(examples[\"text\"], truncation=True)\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99685b60-e81a-404f-a181-027cb3dea656",
   "metadata": {},
   "source": [
    "## BERT/ROBERTA BASELINES HUGGING FACE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de065922-8deb-4ab6-8b27-75fd823e2dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_pickle(\"../data/OMC/final_omc_morality.pkl\")\n",
    "tweets['text']=tweets['text'].map(cleaner1)\n",
    "tweets['label']=tweets['moral_label']\n",
    "\n",
    "tweets.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3ebe1b-b291-481f-be6f-c841e481e856",
   "metadata": {},
   "outputs": [],
   "source": [
    "#classic text classiication\n",
    "#df, id2label, label2id = binary_labels(tweets)\n",
    "#df, id2label, label2id = multi_labels(tweets)\n",
    "df, id2label, label2id = label_multiclass6(tweets)\n",
    "\n",
    "#label_5_rows = df[df['label'] == 5]\n",
    "#index_data = list(label_5_rows.index[0:2])\n",
    "#selected_rows = df.loc[index_data]\n",
    "#df = df.drop(index_data)\n",
    "#selected_rows\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = '/model/',\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size = 16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy = \"epoch\",\n",
    "    push_to_hub=False,\n",
    "    save_strategy='no',\n",
    "seed=42)\n",
    "\n",
    "#-----split data-----\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "#test_df = pd.concat([test_df, selected_rows]).reset_index(drop=True)\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42)\n",
    "datasets = {'train': Dataset.from_pandas(train_df), 'val': Dataset.from_pandas(val_df), 'test': Dataset.from_pandas(test_df)}\n",
    "datasets = DatasetDict(datasets)\n",
    "test_df.label.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e6516e-8214-47a9-88bb-a8c9a4e717dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----model--------\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"FacebookAI/roberta-base\", truncation=True)\n",
    "tokenized_datasets = datasets.map(tokenize_function, batched=True)\n",
    "\n",
    "\n",
    "model = RobertaForSequenceClassification.from_pretrained(\n",
    "    \"FacebookAI/roberta-base\", \n",
    "    num_labels=6, \n",
    "    id2label=id2label, \n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "\n",
    "model =  BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\", \n",
    "    num_labels=6, \n",
    "    id2label=id2label, \n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "# trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"val\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ea9f57-a2e9-4ef1-98b7-a5b14cfbb304",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train and test\n",
    "try:\n",
    "    trainer.train()\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "    \n",
    "predictions = trainer.predict(tokenized_datasets[\"test\"])\n",
    "predicted_class_ids = predictions.predictions.argmax(axis=1)\n",
    "actual_labels = tokenized_datasets[\"test\"][\"label\"]\n",
    "results = classification_report(actual_labels, predicted_class_ids, digits=5, output_dict=True)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c519e3b-9868-4402-a761-54fadc333e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_file = '../data/OMC/F1_results/roberta-base-moral'\n",
    "experiment= \"baseline\"\n",
    "\n",
    "with open(results_file, \"a\") as f:\n",
    "    f.write(f\"\\nExperimento: {experiment}\\n\")\n",
    "    f.write(f\"\\nDataset OMC: \\n\")\n",
    "    f.write(json.dumps(results, indent=2))\n",
    "\n",
    "print(\"Training complete. Results saved in\", results_file)\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54303db1-8ee8-4e04-acf5-d6583f103c3f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Llama + LoRA BASELINE HUGGING FACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27449a7-c5c1-462c-b026-e3fcf9a44b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_pickle(\"../data/OMC/final_omc_morality.pkl\")\n",
    "tweets['text']=tweets['text'].map(cleaner1)\n",
    "tweets['label']=tweets['moral_label']\n",
    "tweets.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9738430f-8b2d-4c91-b1d2-b02e57993fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import peft\n",
    "import pandas as pd\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    # https://github.com/huggingface/peft/issues/96#issuecomment-1460080427\n",
    "    TrainerCallback, TrainerState, TrainerControl, \n",
    "    LlamaForSequenceClassification,\n",
    "    LlamaForSequenceClassification,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "import torch\n",
    "from peft import LoraConfig, TaskType, PeftModel\n",
    "from transformers.trainer_utils import PREFIX_CHECKPOINT_DIR\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "assert torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14aed2d-0c76-463e-89f3-7fa2f221ce6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "#access_token = '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'  # REPLACE WITH ACCESS TOKEN\n",
    "#login(access_token)\n",
    "\n",
    "# Initialize HuggingFace model\n",
    "#from huggingface_hub import login\n",
    "#access_token = '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'  # REPLACE WITH ACCESS TOKEN\n",
    "#login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8da10e-452f-4f70-93b4-91ac53527561",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"meta-llama/Llama-3.2-1B\"\n",
    "#--------- data -----------\n",
    "df, id2label, label2id= label_multiclass6(tweets)\n",
    "#label_5_rows = df[df['label'] == 5]\n",
    "#index_data = list(label_5_rows.index[0:2])\n",
    "#selected_rows = df.loc[index_data]\n",
    "#df = df.drop(index_data)\n",
    "#selected_rows\n",
    "\n",
    "#df, id2label, label2id= multi_labels(tweets)\n",
    "#df, id2label, label2id = binary_labels(tweets)\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42)\n",
    "#test_df = pd.concat([test_df, selected_rows]).reset_index(drop=True)\n",
    "datasets = {'train': Dataset.from_pandas(train_df), 'val': Dataset.from_pandas(val_df), 'test': Dataset.from_pandas(test_df)}\n",
    "\n",
    "datasets = DatasetDict(datasets)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, truncation=True)\n",
    "test_df.label.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d267f7-0d33-4494-accd-753ff5b21f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------- model ------------\n",
    "quantization_config = BitsAndBytesConfig(load_in_8bit=True, bnb_4bit_compute_dtype=torch.float16)\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    r=64,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_CLS\",\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = \"[PAD]\"\n",
    "\n",
    "\n",
    "model = LlamaForSequenceClassification.from_pretrained(model_name,\n",
    "    num_labels=6,\n",
    "    quantization_config=quantization_config,\n",
    "    low_cpu_mem_usage = True\n",
    ")\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "tokenizer.pad_token_id = model.config.pad_token_id\n",
    "tokenized_datasets = datasets.map(tokenize_function, batched=True)\n",
    "model.add_adapter(peft_config, adapter_name=\"adapter_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb5d0c9-f6d1-46c0-8968-80461ff6aab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6f3984-31da-413f-b130-31c14f7c40ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "truncation = True\n",
    "max_length = 2000\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = \"out/\",\n",
    "    learning_rate=2e-4,\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size = 1,\n",
    "    per_device_eval_batch_size = 1,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy = \"epoch\",\n",
    "    push_to_hub=False,\n",
    "    save_strategy='epoch',\n",
    "    #save_safetensors=True,\n",
    "    #load_best_model_at_end = True,\n",
    "    #report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,  #\n",
    "    args=training_args,  \n",
    "    train_dataset=tokenized_datasets[\"train\"],  \n",
    "    eval_dataset=tokenized_datasets[\"val\"],  \n",
    "    tokenizer=tokenizer,  \n",
    "    compute_metrics=compute_metrics,  \n",
    "   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31bb0a94-b90a-4c1a-b214-e915c95b0c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train and test\n",
    "\n",
    "try:\n",
    "    trainer.train()\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "predictions = trainer.predict(tokenized_datasets[\"test\"])\n",
    "predicted_class_ids = predictions.predictions.argmax(axis=1)\n",
    "actual_labels = tokenized_datasets[\"test\"][\"label\"]\n",
    "results = classification_report(actual_labels, predicted_class_ids, digits=5, output_dict=True)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1510a8e1-19c7-4103-b4e7-26fbdfc54c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results_file = '../data/OMC/F1_results/llama-3.2-1b-moral'\n",
    "experiment= \"baseline\"\n",
    "\n",
    "with open(results_file, \"a\") as f:\n",
    "    f.write(f\"\\nExperimento: {experiment}\\n\")\n",
    "    f.write(f\"\\nDataset OMC: \\n\")\n",
    "    f.write(json.dumps(results, indent=2))\n",
    "\n",
    "print(\"Training complete. Results saved in\", results_file)\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abeddb5d-6a53-4d36-988f-52d078a542aa",
   "metadata": {},
   "source": [
    "## DEEPSEEK BASELINE HUGGING FACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae4f7fb-8349-4745-8633-107d9dc6872e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import peft\n",
    "import pandas as pd\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    # https://github.com/huggingface/peft/issues/96#issuecomment-1460080427\n",
    "    TrainerCallback, TrainerState, TrainerControl, \n",
    "    LlamaForSequenceClassification,\n",
    "    LlamaForSequenceClassification,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "import torch\n",
    "from peft import LoraConfig, TaskType, PeftModel\n",
    "from transformers.trainer_utils import PREFIX_CHECKPOINT_DIR\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "assert torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dffd390-50fa-4648-b978-3915ee053602",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_pickle(\"../data/MIND/final_tweets_morality.pkl\")\n",
    "tweets['text']=tweets['text'].map(cleaner1)\n",
    "#tweets['label']=tweets['moral_label']\n",
    "tweets.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0388aefc-263f-48ad-95bc-4b7c12d5e998",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "\n",
    "#--------- data -----------\n",
    "#df, id2label, label2id= label_multiclass6(tweets)\n",
    "#label_5_rows = df[df['label'] == 5]\n",
    "#index_data = list(label_5_rows.index[0:2])\n",
    "#selected_rows = df.loc[index_data]\n",
    "#df = df.drop(index_data)\n",
    "#selected_rows\n",
    "\n",
    "#df, id2label, label2id= multi_labels(tweets)\n",
    "df, id2label, label2id = binary_labels(tweets)\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42)\n",
    "#test_df = pd.concat([test_df, selected_rows]).reset_index(drop=True)\n",
    "datasets = {'train': Dataset.from_pandas(train_df), 'val': Dataset.from_pandas(val_df), 'test': Dataset.from_pandas(test_df)}\n",
    "\n",
    "datasets = DatasetDict(datasets)\n",
    "test_df.label.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9a1639-08d6-4014-8ebb-8d1fa443c398",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Qwen2ForSequenceClassification\n",
    "quantization_config = BitsAndBytesConfig(load_in_8bit=True, bnb_4bit_compute_dtype=torch.float16)\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    r=64,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_CLS\",\n",
    ")\n",
    "\n",
    "#----model--------\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, truncation=True)\n",
    "tokenizer.pad_token = \"[PAD]\"\n",
    "\n",
    "\n",
    "model = Qwen2ForSequenceClassification.from_pretrained(\n",
    "    model_name, \n",
    "    num_labels=2, \n",
    "    id2label=id2label, \n",
    "    label2id=label2id,\n",
    "    quantization_config=quantization_config,\n",
    "    low_cpu_mem_usage = True\n",
    ")\n",
    "\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "tokenizer.pad_token_id = model.config.pad_token_id\n",
    "tokenized_datasets = datasets.map(tokenize_function, batched=True)\n",
    "model.add_adapter(peft_config, adapter_name=\"adapter_1\")\n",
    "\n",
    "truncation = True\n",
    "max_length = 2000\n",
    "\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = \"out/\",\n",
    "    learning_rate=2e-4,\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size = 8,\n",
    "    per_device_eval_batch_size = 8,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy = \"epoch\",\n",
    "    push_to_hub=False,\n",
    "    save_strategy='epoch',\n",
    "    #save_safetensors=True,\n",
    "    #load_best_model_at_end = True,\n",
    "    #report_to=\"none\",\n",
    ")\n",
    "\n",
    "# trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"val\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c41ddec-4eae-4750-843e-0ce86a1ae409",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf0afb6-56d2-4f2b-a9c9-c9184aba859a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train and test\n",
    "\n",
    "#try:\n",
    "#    trainer.train()\n",
    "#except Exception as e:\n",
    "#    print(e)\n",
    "\n",
    "predictions = trainer.predict(tokenized_datasets[\"test\"])\n",
    "predicted_class_ids = predictions.predictions.argmax(axis=1)\n",
    "actual_labels = tokenized_datasets[\"test\"][\"label\"]\n",
    "results = classification_report(actual_labels, predicted_class_ids, digits=5, output_dict=True)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c96f61-03d8-4113-bf0f-a026db7ad27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save results\n",
    "import json\n",
    "results_file = '../data/MIND/F1_results/DeepSeek-Qwen'\n",
    "experiment= \"baseline\"\n",
    "\n",
    "with open(results_file, \"a\") as f:\n",
    "    f.write(f\"\\nExperimento: {experiment}\\n\")\n",
    "    f.write(f\"\\nDataset POZZI: \\n\")\n",
    "    f.write(json.dumps(results, indent=2))\n",
    "\n",
    "print(\"Training complete. Results saved in\", results_file)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
