{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4aa9938a-2b8a-49a6-919c-3937281dfef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import liwc\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from itertools import combinations\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ce739e-63f8-4645-ba97-89e0421aa3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#READ \n",
    "\n",
    "results_file = '../data/MIND/F1_results/llama-3.2-1b-moral'\n",
    "\n",
    "# Leer JSON\n",
    "with open(results_file, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "rows = []\n",
    "\n",
    "for exp in data['experiments']:\n",
    "    experiment_name = exp['experiment']\n",
    "    dataset_name = exp['dataset']\n",
    "    results = exp['results']\n",
    "    \n",
    "    row = {\n",
    "        'Dataset': dataset_name,\n",
    "        'Experiment': experiment_name\n",
    "    }\n",
    "    \n",
    "    for i in range(6):\n",
    "        i_str = str(i)\n",
    "        if i_str in results:\n",
    "            row[i_str] = round(results[i_str]['f1-score'] * 100, 2)  # Multiplicar *100 y redondear\n",
    "        else:\n",
    "            row[i_str] = 0.00\n",
    "\n",
    "    if 'macro avg' in results:\n",
    "        row['MacroF1'] = round(results['macro avg']['f1-score'] * 100, 2)\n",
    "    else:\n",
    "        row['MacroF1'] = 0.00\n",
    "\n",
    "    rows.append(row)\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "df = df[['Dataset', 'Experiment', '0', '1', '2', '3', '4', '5', 'MacroF1']]\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1ebc9d-6384-4896-9f24-02adb8bcfb5b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# MoralBert\n",
    "\n",
    "- Usar aplicación MoralBert y seleccionar el detalle de las anotaciones que coincida en los modelos.\n",
    "- Este modelo devuelve las probabilidades de 10 valores morales (polaridad)\n",
    "- Link: https://huggingface.co/spaces/vjosap/MoralBERTApp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc792893-12ce-419d-9ee4-0f0a5bea8133",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('../data/OMC/omc_morality.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2d76d1-2023-4cc6-8782-e27cb0434b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def moralbert_predictions(df,key=False):\n",
    "    moral_columns = df.columns[5:] \n",
    "    #obtener la moral con mayor probabilidad\n",
    "    df['moralbert'] = df[moral_columns].idxmax(axis=1)\n",
    "    df['moral_value'] = df[moral_columns].max(axis=1)  # Valor máximo de esa columna\n",
    "    df.loc[df['moral_value'] < 0.5, 'moralbert'] = 'nonmoral'\n",
    "    df.drop(columns=moral_columns, inplace=True)\n",
    "    df.drop('moral_value', axis=1 ,inplace=True)\n",
    "    return df\n",
    "\n",
    "df = moralbert_predictions(df,key=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037fe58a-d64d-42d5-aadf-98071c639f9a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Modelos GSI\n",
    "Se cargan los modelos de HF del GSI, hay dos opciones de modelo (con y sin polaridad)\n",
    "- Usar API: https://moral-values-api.gsi.upm.es/predict\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b766ed2a-c98c-48a7-b170-eb0244d9bcbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import unicodedata\n",
    "def to_title_case(name):\n",
    "    \"\"\"Convierte una anotación moral en mayúsculas a minúsculas, eliminando caracteres especiales\"\"\"\n",
    "    try:\n",
    "        normalized_name = unicodedata.normalize('NFD', name)\n",
    "    except Exception as e:\n",
    "        print(name)\n",
    "        raise e\n",
    "    \n",
    "    # Eliminar acentos y caracteres especiales\n",
    "    ascii_name = ''.join(c for c in normalized_name if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "    \n",
    "    return re.sub(r'[^a-zA-Z0-9]', '', ascii_name).lower()\n",
    "\n",
    "\n",
    "\n",
    "MORAL_API_URL = \"https://moral-values-api.gsi.upm.es/predict\"\n",
    "\n",
    "def analyze_moral_polarity(text):\n",
    "    request_payload = {\n",
    "        \"text\": text,\n",
    "        \"model_name\": \"multimoralpolarity_model\"}\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(MORAL_API_URL, json=request_payload)\n",
    "        response.raise_for_status()\n",
    "        prediction = response.json()\n",
    "\n",
    "        if \"Probabilities\" in prediction:\n",
    "            highest_moral = max(prediction[\"Probabilities\"], key=prediction[\"Probabilities\"].get)\n",
    "            #highest_confidence = prediction[\"Probabilities\"][highest_moral]\n",
    "            return highest_moral\n",
    "        else:\n",
    "            return \"No moral values detected.\", 0.0\n",
    "            \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(\"Error analyzing moral values:\", e)\n",
    "        return \"Error\", 0.0\n",
    "\n",
    "\n",
    "def analyze_moral_trait(text):\n",
    "    request_payload = {\n",
    "        \"text\": text,\n",
    "        \"model_name\": \"multimoral_model\"}\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(MORAL_API_URL, json=request_payload)\n",
    "        response.raise_for_status()\n",
    "        prediction = response.json()\n",
    "\n",
    "        if \"Probabilities\" in prediction:\n",
    "            highest_moral = max(prediction[\"Probabilities\"], key=prediction[\"Probabilities\"].get)\n",
    "            #highest_confidence = prediction[\"Probabilities\"][highest_moral]\n",
    "            return highest_moral\n",
    "        else:\n",
    "            return \"No moral values detected.\", 0.0\n",
    "            \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(\"Error analyzing moral values:\", e)\n",
    "        return \"Error\", 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ac08c3-150d-4209-bd4e-0939aaee17fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['roberta_mmp'] = df['text'].apply(lambda x: pd.Series(analyze_moral_polarity(x)))\n",
    "df['roberta_mmp'] = df['roberta_mmp'].apply(lambda x: pd.Series(to_title_case(x)))\n",
    "\n",
    "\n",
    "df['roberta_mm'] = df['text'].apply(lambda x: pd.Series(analyze_moral_trait(x)))\n",
    "df['roberta_mm']=df['roberta_mm'].replace({'NO-MORAL':'no moral','AUTHORITY/SUBVERSION':'authority','LOYALTY/BETRAYAL':'loyalty','CARE/HARM':'care','FAIRNESS/CHEATING':'fairness','PURITY/DEGRADATION':'purity'})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7684925c-189c-4717-9420-39dda7823bbb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# LIWC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1687953d-66f3-4198-b65f-e88d014baef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('../data/OMC/final_omc_morality.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8032de18-c4ad-44c0-bcea-d24eadf2cf06",
   "metadata": {},
   "outputs": [],
   "source": [
    "parse, category_names = liwc.load_token_parser('../data/mfd2.0.dic')\n",
    "\n",
    "def tokenize(text):\n",
    "    '''tokenizar texto'''\n",
    "    for match in re.finditer(r'\\w+', text, re.UNICODE):\n",
    "        yield match.group(0)\n",
    "        \n",
    "def lexicon_analysis(text):\n",
    "    '''aplicar liwic, si no se detecta categoría asignar valor no moral'''\n",
    "    tokens = tokenize(text)\n",
    "    category_counts = Counter()\n",
    "    for token in tokens:\n",
    "        categories = parse(token)\n",
    "        if categories:\n",
    "            category_counts.update(categories)    \n",
    "    return category_counts\n",
    "\n",
    "\n",
    "def get_max_category(liwc_dict):\n",
    "    '''obtener categoría de mayor valor'''\n",
    "    if not liwc_dict:  \n",
    "        return 'no moral'\n",
    "    return max(liwc_dict, key=liwc_dict.get)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9651f4-decb-4c83-8bb8-d4fbd382a23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['liwc_mfd']=df['text'].apply(lexicon_analysis)\n",
    "df['liwc_mfd'] = df['liwc_mfd'].apply(get_max_category)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf14cb83-d155-402e-a66c-1ee9967682df",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Perspectivism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a2bf48-bcdf-40e3-b1b3-8f7a5f14f6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"../models/perspectivist_bert_model\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb9f159-db82-4896-ad49-020e3f9b0fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_text(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    probabilities = logits.softmax(dim=1)\n",
    "    predicted_class = probabilities.argmax(dim=1).item()\n",
    "    return predicted_class\n",
    "\n",
    "df['persp_bert'] = df['text'].apply(lambda x: pd.Series(classify_text(x)))\n",
    "df= df.replace({'persp_bert': {1:'care', 2: 'fairness', 3: 'loyalty', 4: 'authority', 5: 'purity', 0: 'nonmoral'}})\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ed512f-9ee2-413d-b118-3f82ae0cae51",
   "metadata": {},
   "source": [
    "# Métricas de Anotación y Selección de Etiquetas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f9d79a-2737-4c7f-8db1-7f9109cf5682",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Moralidad sin polaridad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ded5ece-776d-45a4-ba20-cf4db636d8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv('../data/MIND/final_tweets_morality.csv')\n",
    "hcr=pd.read_csv('../data/HCR/final_hcr_morality.csv')\n",
    "omc=pd.read_csv('../data/OMC/final_omc_morality.csv')\n",
    "\n",
    "MORAL_MAPPING= {'harm':'care', 'cheating':'fairness','betrayal':'loyalty','subversion':'authority','degradation':'purity','sanctity':'purity','no moral':'nonmoral','nomoral':'nonmoral'}\n",
    "\n",
    "df['liwc_mfd'] = df['liwc_mfd'].apply(lambda x: x.split('.')[0] if isinstance(x, str) else x)\n",
    "df.replace({\"moralbert\": MORAL_MAPPING, \"roberta_mmp\":MORAL_MAPPING, \"roberta_mm\":MORAL_MAPPING, \"liwc_mfd\":MORAL_MAPPING, \"persp_bert\":MORAL_MAPPING}, inplace=True)\n",
    "\n",
    "hcr['liwc_mfd'] = hcr['liwc_mfd'].apply(lambda x: x.split('.')[0] if isinstance(x, str) else x)\n",
    "hcr.replace({\"moralbert\": MORAL_MAPPING, \"roberta_mmp\":MORAL_MAPPING, \"roberta_mm\":MORAL_MAPPING, \"liwc_mfd\":MORAL_MAPPING, \"persp_bert\":MORAL_MAPPING}, inplace=True)\n",
    "\n",
    "omc['liwc_mfd'] = omc['liwc_mfd'].apply(lambda x: x.split('.')[0] if isinstance(x, str) else x)\n",
    "omc.replace({\"moralbert\": MORAL_MAPPING, \"roberta_mmp\":MORAL_MAPPING, \"roberta_mm\":MORAL_MAPPING, \"liwc_mfd\":MORAL_MAPPING, \"persp_bert\":MORAL_MAPPING}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96928e44-540d-48d1-b2ad-042e1c47644f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SELECCIONAR ETIQUETA\n",
    "\n",
    "#model_cols = ['moralbert', 'roberta_mmp', 'roberta_mm', 'liwc_mfd']\n",
    "#model_cols=['moral_label2','human_annot']\n",
    "def count_moral_votes(row):\n",
    "    votes = {}\n",
    "    for col in model_cols:\n",
    "        label = row[col]\n",
    "        votes[label] = votes.get(label, 0) + 1\n",
    "    return votes\n",
    "\n",
    "def get_top_morals(vote_dict):\n",
    "    max_votes = max(vote_dict.values())\n",
    "    top_morals = [moral for moral, count in vote_dict.items() if count == max_votes]\n",
    "    \n",
    "    if len(top_morals) > 1 and 'nonmoral' in top_morals:\n",
    "        top_morals = [moral for moral in top_morals if moral != 'nonmoral']\n",
    "    \n",
    "    return top_morals[0]\n",
    "\n",
    "    \n",
    "df['top_morals'] = df.apply(count_moral_votes, axis=1)\n",
    "df['moral_label2'] = df['top_morals'].apply(get_top_morals)\n",
    "df.drop('top_morals',axis=1,inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f7a81d9-6703-4796-85f2-1777913732b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>moral_label2</th>\n",
       "      <th>human_annot</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>moral_label2</th>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.4174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>human_annot</th>\n",
       "      <td>0.4174</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              moral_label2  human_annot\n",
       "moral_label2        1.0000       0.4174\n",
       "human_annot         0.4174       1.0000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Métrica Cohen Kappa\n",
    "#label_cols = [\"moralbert\", \"roberta_mm\",\"roberta_mmp\", \"liwc_mfd\"]\n",
    "label_cols=['moral_label2','human_annot']\n",
    "\n",
    "def compute_cohen_kappa(df,label_cols=label_cols):\n",
    "    #label_cols = [\"moralbert\", \"roberta_mm\",\"roberta_mmp\", \"liwc_mfd\"]\n",
    "    label_cols=['moral_label2','human_annot']\n",
    "\n",
    "    kappa_matrix = pd.DataFrame(index=label_cols, columns=label_cols)\n",
    "    \n",
    "    # Cohen's Kappa values\n",
    "    for col1, col2 in combinations(label_cols, 2):\n",
    "        kappa = cohen_kappa_score(df[col1], df[col2])\n",
    "        kappa_matrix.loc[col1, col2] = kappa\n",
    "        kappa_matrix.loc[col2, col1] = kappa  \n",
    "    \n",
    "    # Diagonal v\n",
    "    for col in label_cols:\n",
    "        kappa_matrix.loc[col, col] = 1.0\n",
    "    \n",
    "    kappa_matrix = kappa_matrix.astype(float)\n",
    "    kappa_matrix = kappa_matrix.round(4)\n",
    "    \n",
    "    return kappa_matrix\n",
    "\n",
    "kappa_matrix_df1= compute_cohen_kappa(df)\n",
    "#kappa_matrix_omc= compute_cohen_kappa(omc)\n",
    "#kappa_matrix_hcr= compute_cohen_kappa(hcr)\n",
    "#df= pd.concat([kappa_matrix_df1,kappa_matrix_hcr,kappa_matrix_omc], keys= ['POZZI','HCR','OMC'])\n",
    "#print(df.to_latex())\n",
    "\n",
    "print(kappa_matrix_df1.to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab178c08-d9ed-4603-aca5-23f9dcfb745e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrr}\n",
      "\\toprule\n",
      " & moral_label2 & human_annot \\\\\n",
      "\\midrule\n",
      "moral_label2 & 1.000000 & 0.417400 \\\\\n",
      "human_annot & 0.417400 & 1.000000 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(kappa_matrix_df1.to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d6e537f-37a8-45cc-b8de-a269fdcd75a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4055)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Métrica Fleiss kappa\n",
    "\n",
    "from torchmetrics.nominal import FleissKappa\n",
    "import torch\n",
    "#https://lightning.ai/docs/torchmetrics/stable/nominal/fleiss_kappa.html\n",
    "\n",
    "categories = ['care', 'fairness', 'loyalty', 'authority', 'purity', 'nonmoral']\n",
    "\n",
    "# Create a function to count occurrences per category\n",
    "def count_category_occurrences(row, categories):\n",
    "    #annotators = [\"moralbert\", \"roberta_mm\",\"roberta_mmp\", \"liwc_mfd\"]\n",
    "    annotators=['moral_label2','human_annot']\n",
    "    counts = {category: 0 for category in categories}\n",
    "    \n",
    "    for annotator in annotators:\n",
    "        if row[annotator] in counts:\n",
    "            counts[row[annotator]] += 1\n",
    "            \n",
    "    return list(counts.values())\n",
    "\n",
    "\n",
    "ratingsdf1 = df.apply(lambda row: count_category_occurrences(row, categories), axis=1)\n",
    "#ratingsomc = omc.apply(lambda row: count_category_occurrences(row, categories), axis=1)\n",
    "#ratingshcr = hcr.apply(lambda row: count_category_occurrences(row, categories), axis=1)\n",
    "\n",
    "# Convertir los resultados a tensores\n",
    "ratings_tensor_df1 = torch.tensor(ratingsdf1.tolist(), dtype=torch.int)\n",
    "#ratings_tensor_omc = torch.tensor(ratingsomc.tolist(), dtype=torch.int)\n",
    "#ratings_tensor_hcr = torch.tensor(ratingshcr.tolist(), dtype=torch.int)\n",
    "\n",
    "fleiss_kappa = FleissKappa(mode='counts')\n",
    "\n",
    "# Calcular Fleiss' \n",
    "fleiss_k_df1 = fleiss_kappa(ratings_tensor_df1)\n",
    "#fleiss_k_omc = fleiss_kappa(ratings_tensor_omc)\n",
    "#fleiss_k_hcr = fleiss_kappa(ratings_tensor_hcr)\n",
    "\n",
    "#results = {'Dataset': ['df1', 'omc', 'hcr'],'Fleiss Kappa': [fleiss_k_df1.item(), fleiss_k_omc.item(), fleiss_k_hcr.item()]}\n",
    "\n",
    "#fleiss_kappa_df = pd.DataFrame(results)\n",
    "\n",
    "#print(fleiss_kappa_df.to_latex())\n",
    "fleiss_k_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "17ef3c7d-da23-44ee-8106-b711ac83e711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "textos: 159 , anotadores:  2\n",
      "PABAK: 0.320754716981132\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(0.320754716981132)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##PABAK\n",
    "from calcular_pabak import *\n",
    "def run_pabak_multiclass(df, annotator_columns):\n",
    "    #map values into int\n",
    "    df_filtered = df[annotator_columns].replace({\n",
    "        'care': 1, 'harm': 1,\n",
    "        'fairness': 2, 'cheating': 2,\n",
    "        'loyalty': 3, 'betrayal': 3,\n",
    "        'authority': 4, 'subversion': 4,\n",
    "        'purity': 5, 'degradation': 5,\n",
    "        'nonmoral': 6, 'nomoral': 6,  'no moral': 6\n",
    "    })\n",
    "\n",
    "\n",
    "    data_matrix = df_filtered.values\n",
    "    #print(data_matrix)\n",
    "    observed_agreement_full = calculate_observed_agreement_full(data_matrix)\n",
    "    PABAK_full = 2 * observed_agreement_full - 1\n",
    "\n",
    "    print(\"PABAK:\", PABAK_full)\n",
    "    return PABAK_full\n",
    "\n",
    "annotator_columns = [\"moralbert\", \"roberta_mm\", \"roberta_mmp\", \"liwc_mfd\"]\n",
    "annotator_columns = ['moral_label2','human_annot']\n",
    "\n",
    "df_pabak = run_pabak_multiclass(df, annotator_columns)\n",
    "#hcr_pabak = run_pabak_multiclass(hcr, annotator_columns)\n",
    "#omc_pabak = run_pabak_multiclass(omc, annotator_columns)\n",
    "df_pabak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2bee85-9edf-4439-bcdd-a2bb79fc02ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "869e8c5c-a61e-4ff0-ac42-66ca2c52618d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Moralidad con polaridad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fe9aaf-7611-4831-9d4f-490cdb02ac32",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('../data/MIND/final_tweets_morality.csv')\n",
    "hcr=pd.read_csv('../data/HCR/final_hcr_morality.csv')\n",
    "omc=pd.read_csv('../data/OMC/final_omc_morality.csv')\n",
    "\n",
    "LW_MORAL_MAPPING= {'care.virtue':'care', 'fairness.virtue':'fairness','loyalty.virtue':'loyalty','authority.virtue':'authority','sanctity.virtue':'purity','care.vice':'harm','fairness.vice':'cheating','loyalty.vice':'betrayal','authority.vice':'subversion','sanctity.vice':'degradation','no moral':'nonmoral','nomoral':'nonmoral'}\n",
    "\n",
    "df.replace({\"liwc_mfd\": LW_MORAL_MAPPING,\"moralbert\":LW_MORAL_MAPPING,\"roberta_mmp\":LW_MORAL_MAPPING}, inplace=True)\n",
    "hcr.replace({\"liwc_mfd\": LW_MORAL_MAPPING,\"moralbert\":LW_MORAL_MAPPING,\"roberta_mmp\":LW_MORAL_MAPPING}, inplace=True)\n",
    "omc.replace({\"liwc_mfd\": LW_MORAL_MAPPING,\"moralbert\":LW_MORAL_MAPPING,\"roberta_mmp\":LW_MORAL_MAPPING}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c04613-3b94-4f43-8704-2e8945e67132",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SELECCIÓN ETIQUETA CON POLARIDAD\n",
    "'''\n",
    "model_cols = ['moralbert', 'roberta_mmp', 'liwc_mfd']\n",
    "\n",
    "def count_moral_votes(row):\n",
    "    votes = {}\n",
    "    for col in model_cols:\n",
    "        label = row[col]\n",
    "        votes[label] = votes.get(label, 0) + 1\n",
    "    return votes\n",
    "\n",
    "def get_top_morals(vote_dict):\n",
    "    max_votes = max(vote_dict.values())\n",
    "    top_morals = [moral for moral, count in vote_dict.items() if count == max_votes]\n",
    "    \n",
    "    if len(top_morals) > 1 and 'nonmoral' in top_morals:\n",
    "        top_morals = [moral for moral in top_morals if moral != 'nonmoral']\n",
    "    \n",
    "    return top_morals[0]\n",
    "\n",
    "    \n",
    "df['top_morals'] = df.apply(count_moral_votes, axis=1)\n",
    "df['moral_label_polarity'] = df['top_morals'].apply(get_top_morals)\n",
    "df.drop('top_morals',axis=1,inplace=True)\n",
    "df\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1fa1d6-cd85-4707-a788-ed7094a7147c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cohen_kappa(df,label_cols=label_cols):\n",
    "    label_cols = [\"moralbert\",\"roberta_mmp\", \"liwc_mfd\"]\n",
    "    kappa_matrix = pd.DataFrame(index=label_cols, columns=label_cols)\n",
    "    \n",
    "    # Fill in pairwise Cohen's Kappa values\n",
    "    for col1, col2 in combinations(label_cols, 2):\n",
    "        kappa = cohen_kappa_score(df[col1], df[col2])\n",
    "        kappa_matrix.loc[col1, col2] = kappa\n",
    "        kappa_matrix.loc[col2, col1] = kappa  # symmetric\n",
    "    \n",
    "    # Diagonal values (agreement with self = 1.0)\n",
    "    for col in label_cols:\n",
    "        kappa_matrix.loc[col, col] = 1.0\n",
    "    \n",
    "    kappa_matrix = kappa_matrix.astype(float)\n",
    "    kappa_matrix = kappa_matrix.round(4)\n",
    "    \n",
    "    return kappa_matrix\n",
    "\n",
    "\n",
    "cols = [\"moralbert\", \"roberta_mmp\", \"liwc_mfd\"]\n",
    "\n",
    "kappa_matrix_df1= compute_cohen_kappa(df,label_cols=cols)\n",
    "kappa_matrix_omc= compute_cohen_kappa(omc, label_cols=cols)\n",
    "kappa_matrix_hcr= compute_cohen_kappa(hcr, label_cols=cols)\n",
    "\n",
    "df2= pd.concat([kappa_matrix_df1,kappa_matrix_hcr,kappa_matrix_omc], keys= ['POZZI','HCR','OMC'])\n",
    "print(df2.to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a89a401-a874-46c0-bcf4-5783ca6dde77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Métrica Fleiss kappa\n",
    "from torchmetrics.nominal import FleissKappa\n",
    "import torch\n",
    "#https://lightning.ai/docs/torchmetrics/stable/nominal/fleiss_kappa.html\n",
    "\n",
    "def count_category_occurrences(row, categories):\n",
    "    annotators = ['moralbert','roberta_mmp', 'liwc_mfd']\n",
    "    counts = {category: 0 for category in categories}\n",
    "    \n",
    "    for annotator in annotators:\n",
    "        if row[annotator] in counts:\n",
    "            counts[row[annotator]] += 1\n",
    "            \n",
    "    return list(counts.values())\n",
    "\n",
    "\n",
    "ratingsdf1 = df.apply(lambda row: count_category_occurrences(row, categories), axis=1)\n",
    "ratingsomc = omc.apply(lambda row: count_category_occurrences(row, categories), axis=1)\n",
    "ratingshcr = hcr.apply(lambda row: count_category_occurrences(row, categories), axis=1)\n",
    "\n",
    "# Convertir los resultados a tensores\n",
    "ratings_tensor_df1 = torch.tensor(ratingsdf1.tolist(), dtype=torch.int)\n",
    "ratings_tensor_omc = torch.tensor(ratingsomc.tolist(), dtype=torch.int)\n",
    "ratings_tensor_hcr = torch.tensor(ratingshcr.tolist(), dtype=torch.int)\n",
    "\n",
    "fleiss_kappa = FleissKappa(mode='counts')\n",
    "\n",
    "# Calcular Fleiss' \n",
    "fleiss_k_df1 = fleiss_kappa(ratings_tensor_df1)\n",
    "fleiss_k_omc = fleiss_kappa(ratings_tensor_omc)\n",
    "fleiss_k_hcr = fleiss_kappa(ratings_tensor_hcr)\n",
    "\n",
    "results = {\n",
    "    'Dataset': ['df1', 'omc', 'hcr'],\n",
    "    'Fleiss Kappa': [fleiss_k_df1.item(), fleiss_k_omc.item(), fleiss_k_hcr.item()]}\n",
    "\n",
    "fleiss_kappa_df = pd.DataFrame(results)\n",
    "\n",
    "print(fleiss_kappa_df.to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff381c49-27d6-4600-b71b-3e9cb3ea944c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## PABAK\n",
    "\n",
    "##PABAK\n",
    "from calcular_pabak import *\n",
    "def run_pabak_multiclass(df, annotator_columns):\n",
    "    df_filtered = df[annotator_columns].replace({\n",
    "        'care': 1, 'harm': 2,\n",
    "        'fairness': 3, 'cheating': 4,\n",
    "        'loyalty': 5, 'betrayal': 6,\n",
    "        'authority': 7, 'subversion': 8,\n",
    "        'purity': 9, 'degradation': 10,\n",
    "        'nonmoral': 11, 'no moral': 11, 'nomoral': 11\n",
    "    })\n",
    "\n",
    "    data_matrix = df_filtered.values\n",
    "\n",
    "    observed_agreement_full = calculate_observed_agreement_full(data_matrix)\n",
    "    PABAK_full = 2 * observed_agreement_full - 1\n",
    "\n",
    "    print(\"PABAK:\", PABAK_full)\n",
    "    return PABAK_full\n",
    "\n",
    "annotator_columns = [\"moralbert\", \"roberta_mmp\", \"liwc_mfd\"]\n",
    "pabak_df = run_pabak_multiclass(df, annotator_columns)\n",
    "pabak_omc = run_pabak_multiclass(omc, annotator_columns)\n",
    "pabak_hcr = run_pabak_multiclass(hcr, annotator_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696d4400-b902-4af8-9ac0-ab932fe965e0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Unión Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccad203-cdb8-4438-9412-6ac3408a95c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings= pd.read_pickle('../models/omc_tadw_df.pkl')\n",
    "embeddings\n",
    "\n",
    "df_merged = pd.merge(df_merged, embeddings[['tweet.id', 'extra_data']], on='tweet.id', how='inner')\n",
    "df_merged.rename(columns={'extra_data':'tadw'},inplace=True)\n",
    "df_merged\n",
    "\n",
    "#df_merged.to_pickle('../data/OMC/final_omc_morality.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b5befc-c51b-4995-8078-bebd7a6da3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"../data/OMC/final_omc_morality.pkl\")\n",
    "df\n",
    "\n",
    "embedding_columns = ['svd', 'deepwalk', 'node2vec', 'tadw']\n",
    "\n",
    "combined_embeddings = np.array([\n",
    "    np.concatenate([\n",
    "        row['svd'],\n",
    "        row['deepwalk'],\n",
    "        row['node2vec'],\n",
    "        row['tadw']\n",
    "    ]) for _, row in df.iterrows()\n",
    "])\n",
    "\n",
    "#np.save(\"all_omc_embeddings.npy\", combined_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f5722e-61e7-4c81-a074-b04d95734ea8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "87e1d7ce-e285-4025-ad1c-8f4a9a292151",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Anotador Humano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d6673d-fd23-41cc-bb3f-3f14e952f755",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_pickle(\"../data/MIND/final_tweets_morality.pkl\")\n",
    "\n",
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78dd586-b243-4c10-9006-fe6561735bca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06dd4ce7-5d68-4806-bde5-583ec9c5dc58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
